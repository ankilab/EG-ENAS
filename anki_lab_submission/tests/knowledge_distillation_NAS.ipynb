{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a3feaae-35d2-4405-9391-30cd29c287c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpc/iwb3/iwb3021h/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from torchsummary import summary\n",
    "import ast\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import icecream as ic\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"../../\")\n",
    "sys.path.append(\"../\")\n",
    "####### Dataset ############\n",
    "from data_processor import DataProcessor\n",
    "######## Search space #########\n",
    "from search_space.RegNet import RegNet\n",
    "from search_space.utils import create_widths_plot, scatter_results, get_generation_dfs\n",
    "######## Training ###############\n",
    "from trainer import Trainer, TrainerDistillation\n",
    "from utils.train_cfg import get_cfg, show_cfg\n",
    "###################################################\n",
    "random_seed = 1\n",
    "random.seed(random_seed)\n",
    "# Set seed for NumPy\n",
    "np.random.seed(random_seed)\n",
    "# Set seed for PyTorch\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "# Additional steps if using CuDNN (optional, for GPU acceleration)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import os\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import json\n",
    "from io import StringIO\n",
    "from coolname import generate_slug\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e44ae1f-9fcc-42b6-8a3b-8a502c9ccf15",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df08bad4-9d31-4a3c-80ac-af16603bc57b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dataset_metadata(dataset_path):\n",
    "    with open(os.path.join(dataset_path, 'metadata'), \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "    return metadata\n",
    "# load dataset from file\n",
    "def load_datasets(data_path, truncate):\n",
    "    data_path = '../../datasets/'+data_path\n",
    "    train_x = np.load(os.path.join(data_path,'train_x.npy'))\n",
    "    train_y = np.load(os.path.join(data_path,'train_y.npy'))\n",
    "    valid_x = np.load(os.path.join(data_path,'valid_x.npy'))\n",
    "    valid_y = np.load(os.path.join(data_path,'valid_y.npy'))\n",
    "    test_x = np.load(os.path.join(data_path,'test_x.npy'))\n",
    "    metadata = load_dataset_metadata(data_path)\n",
    "\n",
    "    if truncate:\n",
    "        train_x = train_x[:64]\n",
    "        train_y = train_y[:64]\n",
    "        valid_x = valid_x[:64]\n",
    "        valid_y = valid_y[:64]\n",
    "        test_x = test_x[:64]\n",
    "\n",
    "    return (train_x, train_y), \\\n",
    "           (valid_x, valid_y), \\\n",
    "           (test_x), metadata\n",
    "\n",
    "def validation(model, val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # 4. Calculate Accuracy\n",
    "    accuracy = correct / total\n",
    "    print('Accuracy on the test set: {:.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d881af8-a907-46d8-83f7-d2978e2b45a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/jupyterhub/jh3.1.1-py3.11/envs/pytorch-2.2.0/lib/python3.12/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'num_classes': 20,\n",
       " 'input_shape': [50000, 3, 28, 28],\n",
       " 'codename': 'Adaline',\n",
       " 'benchmark': 89.85,\n",
       " 'select_augment': False,\n",
       " 'train_config_path': 'anki_lab_submission/configs/train/augmentations_adam.yaml'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset=\"AddNIST\"\n",
    "(train_x, train_y), (valid_x, valid_y), (test_x), metadata = load_datasets(Dataset, truncate=False)\n",
    "test_y = np.load(os.path.join('../../datasets/'+Dataset,'test_y.npy'))\n",
    "metadata[\"select_augment\"]=False\n",
    "data_processor = DataProcessor(train_x[:], train_y[:], valid_x, valid_y, test_x, metadata)\n",
    "train_loader, valid_loader, test_loader = data_processor.process()\n",
    "\n",
    "\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "941cedd0-a728-416f-9fd6-5b80e5b5ba86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9be5307500>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img=next(iter(train_loader))[0][0]\n",
    "img=img.numpy().transpose(1, 2, 0)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394894d6-8ced-423d-9322-234d052133ef",
   "metadata": {},
   "source": [
    "# Load search space and population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26a09d25-921c-4017-a275-455c985eced1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rg=regnet_space=RegNet(metadata,\n",
    "                    W0=[16, 120, 8],\n",
    "                    WA=[16, 64, 8],\n",
    "                    WM=[2.05,2.9,0.05],\n",
    "                    D=[8,22,1], \n",
    "                    G=[8,8,8], \n",
    "                    base_config=f\"../configs/search_space/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b03ee60-d9ea-44ce-9910-229d9065e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_best, info_best, _= regnet.load_model(\"output/test_regnet_cifar100/RegNetY800MF\", weights_file=\"output/test_regnet_cifar100/RegNetY800MF/student_best\")\n",
    "#summary(model_best, (3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8cc84b9-b1ff-45bf-abe8-29b13785892e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd63fecc-7ad7-443b-93fa-5f194e5da39d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/emerald_aardwark/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/onyx_gaur/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/rampant_myna/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/succinct_chimpanzee/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/red_caterpillar/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/thistle_owl/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/mahogany_collie/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/quizzical_caiman/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/cinnamon_dove/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/mature_hare/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/silky_dachshund/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/crystal_fennec/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/chubby_sawfish/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/amaranth_ringtail/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/mahogany_raven/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/cerulean_prawn/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/viridian_muskrat/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/original_reindeer/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/kind_armadillo/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/gifted_goldfish/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/roaring_snake/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/optimal_wildebeest/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/portable_griffin/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/outrageous_flounder/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/vehement_lizard/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/shrewd_muskrat/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/encouraging_whippet/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/eggplant_partridge/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/lavender_agama/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/sexy_skylark/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/brainy_cobra/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/delectable_crab/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/raspberry_bullfinch/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/red_pudu/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/pristine_woodpecker/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/awesome_wombat/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/red_bullmastiff/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/sly_saluki/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/denim_butterfly/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/white_cow/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/bulky_coati/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/helpful_ermine/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/monumental_trout/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/hypersonic_porcupine/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/carrot_bison/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/vivacious_beetle/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/tangerine_beaver/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/nostalgic_bison/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/vigilant_ibex/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/massive_jerboa/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/prompt_heron/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/powerful_chupacabra/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/mighty_crane/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/tourmaline_jacamar/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/chirpy_swallow/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/frisky_clam/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/loutish_snail/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/remarkable_seahorse/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/visionary_adder/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/interesting_dormouse/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/quartz_dove/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/miniature_sawfly/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/cerulean_emu/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/pygmy_waxbill/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/crystal_swallow/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/overjoyed_caracal/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/dark_goat/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/glittering_ostrich/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/proficient_duck/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/beneficial_axolotl/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/macho_oxpecker/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/carmine_chinchilla/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/pistachio_mouflon/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/unnatural_rook/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/discerning_urchin/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/hulking_wombat/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/hulking_bee/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/abiding_narwhal/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/courageous_puffin/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/overjoyed_guillemot/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/woodoo_herring/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/mustard_marten/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/abiding_markhor/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/elegant_vicugna/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/rainbow_smilodon/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/hot_sunfish/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/micro_lion/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/industrious_whippet/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/tidy_seal/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/rampant_sunfish/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/magenta_mink/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/hot_pogona/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/gorgeous_goose/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/tentacled_squid/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/refined_panda/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/impartial_bullfinch/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/illustrious_taipan/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/fearless_vulture/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/fanatic_magpie/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/logical_salmon/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/onyx_groundhog/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/merciful_elephant/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/lurking_okapi/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/athletic_dogfish/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/zippy_curassow/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/therapeutic_spoonbill/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/tungsten_nautilus/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/athletic_manatee/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/talented_cuscus/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/denim_hippo/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/lurking_chamois/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/primitive_harrier/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/dainty_rattlesnake/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/debonair_bloodhound/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/tall_koel/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/cyan_tench/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/finicky_armadillo/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/nocturnal_scorpion/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/uptight_dugong/config.yaml\n",
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Adaline/striped_dalmatian/config.yaml\n"
     ]
    }
   ],
   "source": [
    "    current_time=datetime.now().strftime(\"%d_%m_%Y_%H_%M\")\n",
    "    test_folder=f\"{os.getenv('WORK')}/NAS_COMPETITION_RESULTS/kwnowledge_distillation/kd/{current_time}/{metadata['codename']}\"\n",
    "    \n",
    "    folder=f\"/home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/{metadata['codename']}\"\n",
    "    models, chromosomes=rg.load_generation(folder)\n",
    "    #models, chromosomes=rg.create_random_generation(save_folder=test_folder,gen=None, size=1, config_updates=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425ed337-712d-4998-b932-bcd3022b91f5",
   "metadata": {},
   "source": [
    "# Knowledge inheritance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27d22921-1b75-4d14-93dd-46fe173a3e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0595bdf9-3252-4207-af53-b25eb9b6ecce",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "RegNet                                             [128, 20]                 --\n",
       "├─ResStemCifar: 1-1                                [128, 28, 28, 28]         --\n",
       "│    └─Conv2d: 2-1                                 [128, 28, 28, 28]         756\n",
       "│    └─BatchNorm2d: 2-2                            [128, 28, 28, 28]         56\n",
       "│    └─ReLU: 2-3                                   [128, 28, 28, 28]         --\n",
       "├─AnyStage: 1-2                                    [128, 64, 14, 14]         --\n",
       "│    └─ResBottleneckBlock: 2-4                     [128, 64, 14, 14]         --\n",
       "│    │    └─BottleneckTransform: 3-1               [128, 64, 14, 14]         11,847\n",
       "│    │    └─DropPath: 3-2                          [128, 64, 14, 14]         --\n",
       "│    │    └─Sequential: 3-3                        [128, 64, 14, 14]         1,920\n",
       "│    │    └─ReLU: 3-4                              [128, 64, 14, 14]         --\n",
       "├─AnyStage: 1-3                                    [128, 144, 7, 7]          --\n",
       "│    └─ResBottleneckBlock: 2-5                     [128, 144, 7, 7]          --\n",
       "│    │    └─BottleneckTransform: 3-5               [128, 144, 7, 7]          45,952\n",
       "│    │    └─DropPath: 3-6                          [128, 144, 7, 7]          --\n",
       "│    │    └─Sequential: 3-7                        [128, 144, 7, 7]          9,504\n",
       "│    │    └─ReLU: 3-8                              [128, 144, 7, 7]          --\n",
       "│    └─ResBottleneckBlock: 2-6                     [128, 144, 7, 7]          --\n",
       "│    │    └─BottleneckTransform: 3-9               [128, 144, 7, 7]          63,252\n",
       "│    │    └─DropPath: 3-10                         [128, 144, 7, 7]          --\n",
       "│    │    └─Identity: 3-11                         [128, 144, 7, 7]          --\n",
       "│    │    └─ReLU: 3-12                             [128, 144, 7, 7]          --\n",
       "│    └─ResBottleneckBlock: 2-7                     [128, 144, 7, 7]          --\n",
       "│    │    └─BottleneckTransform: 3-13              [128, 144, 7, 7]          63,252\n",
       "│    │    └─DropPath: 3-14                         [128, 144, 7, 7]          --\n",
       "│    │    └─Identity: 3-15                         [128, 144, 7, 7]          --\n",
       "│    │    └─ReLU: 3-16                             [128, 144, 7, 7]          --\n",
       "├─AnyStage: 1-4                                    [128, 336, 4, 4]          --\n",
       "│    └─ResBottleneckBlock: 2-8                     [128, 336, 4, 4]          --\n",
       "│    │    └─BottleneckTransform: 3-17              [128, 336, 4, 4]          212,052\n",
       "│    │    └─DropPath: 3-18                         [128, 336, 4, 4]          --\n",
       "│    │    └─Sequential: 3-19                       [128, 336, 4, 4]          49,056\n",
       "│    │    └─ReLU: 3-20                             [128, 336, 4, 4]          --\n",
       "│    └─ResBottleneckBlock: 2-9                     [128, 336, 4, 4]          --\n",
       "│    │    └─BottleneckTransform: 3-21              [128, 336, 4, 4]          308,868\n",
       "│    │    └─DropPath: 3-22                         [128, 336, 4, 4]          --\n",
       "│    │    └─Identity: 3-23                         [128, 336, 4, 4]          --\n",
       "│    │    └─ReLU: 3-24                             [128, 336, 4, 4]          --\n",
       "│    └─ResBottleneckBlock: 2-10                    [128, 336, 4, 4]          --\n",
       "│    │    └─BottleneckTransform: 3-25              [128, 336, 4, 4]          308,868\n",
       "│    │    └─DropPath: 3-26                         [128, 336, 4, 4]          --\n",
       "│    │    └─Identity: 3-27                         [128, 336, 4, 4]          --\n",
       "│    │    └─ReLU: 3-28                             [128, 336, 4, 4]          --\n",
       "│    └─ResBottleneckBlock: 2-11                    [128, 336, 4, 4]          --\n",
       "│    │    └─BottleneckTransform: 3-29              [128, 336, 4, 4]          308,868\n",
       "│    │    └─DropPath: 3-30                         [128, 336, 4, 4]          --\n",
       "│    │    └─Identity: 3-31                         [128, 336, 4, 4]          --\n",
       "│    │    └─ReLU: 3-32                             [128, 336, 4, 4]          --\n",
       "│    └─ResBottleneckBlock: 2-12                    [128, 336, 4, 4]          --\n",
       "│    │    └─BottleneckTransform: 3-33              [128, 336, 4, 4]          308,868\n",
       "│    │    └─DropPath: 3-34                         [128, 336, 4, 4]          --\n",
       "│    │    └─Identity: 3-35                         [128, 336, 4, 4]          --\n",
       "│    │    └─ReLU: 3-36                             [128, 336, 4, 4]          --\n",
       "│    └─ResBottleneckBlock: 2-13                    [128, 336, 4, 4]          --\n",
       "│    │    └─BottleneckTransform: 3-37              [128, 336, 4, 4]          308,868\n",
       "│    │    └─DropPath: 3-38                         [128, 336, 4, 4]          --\n",
       "│    │    └─Identity: 3-39                         [128, 336, 4, 4]          --\n",
       "│    │    └─ReLU: 3-40                             [128, 336, 4, 4]          --\n",
       "│    └─ResBottleneckBlock: 2-14                    [128, 336, 4, 4]          --\n",
       "│    │    └─BottleneckTransform: 3-41              [128, 336, 4, 4]          308,868\n",
       "│    │    └─DropPath: 3-42                         [128, 336, 4, 4]          --\n",
       "│    │    └─Identity: 3-43                         [128, 336, 4, 4]          --\n",
       "│    │    └─ReLU: 3-44                             [128, 336, 4, 4]          --\n",
       "│    └─ResBottleneckBlock: 2-15                    [128, 336, 4, 4]          --\n",
       "│    │    └─BottleneckTransform: 3-45              [128, 336, 4, 4]          308,868\n",
       "│    │    └─DropPath: 3-46                         [128, 336, 4, 4]          --\n",
       "│    │    └─Identity: 3-47                         [128, 336, 4, 4]          --\n",
       "│    │    └─ReLU: 3-48                             [128, 336, 4, 4]          --\n",
       "├─AnyHead: 1-5                                     [128, 20]                 --\n",
       "│    └─AdaptiveAvgPool2d: 2-16                     [128, 336, 1, 1]          --\n",
       "│    └─Dropout: 2-17                               [128, 336]                --\n",
       "│    └─Linear: 2-18                                [128, 20]                 6,740\n",
       "====================================================================================================\n",
       "Total params: 2,626,463\n",
       "Trainable params: 2,626,463\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 5.98\n",
       "====================================================================================================\n",
       "Input size (MB): 1.20\n",
       "Forward/backward pass size (MB): 689.05\n",
       "Params size (MB): 10.50\n",
       "Estimated Total Size (MB): 700.75\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(models[\"abiding_markhor\"], input_size=next(iter(train_loader))[0].shape[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e40e2379-e68e-4d44-9839-78637bc16bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ws': [96, 200, 400, 824],\n",
       " 'bs': [1.0, 1.0, 1.0, 1.0],\n",
       " 'gs': [8, 8, 8, 8],\n",
       " 'ds': [2, 4, 10, 3],\n",
       " 'num_stages': 4,\n",
       " 'total_size_mb': 37.75217819213867,\n",
       " 'h': 1,\n",
       " 'w': 1,\n",
       " 'flops': 1704959,\n",
       " 'params': 9896507,\n",
       " 'acts': 9151,\n",
       " 'WA': 32.0,\n",
       " 'W0': 96,\n",
       " 'WM': 2.05,\n",
       " 'DEPTH': 19,\n",
       " 'GROUP_W': 8}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chromosomes[list(chromosomes.keys())[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97e3bd9e-ab24-4afd-836a-a0016717ca5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.train_cfg import load_checkpoint\n",
    "keys_to_delete=[]\n",
    "super_model={}\n",
    "for model_name, model in models.items():\n",
    "    try:\n",
    "        weights_file=f\"/home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/{metadata[\"codename\"]}/{model_name}/student_best\"\n",
    "        state = load_checkpoint(weights_file)\n",
    "        model.load_state_dict(state[\"model\"])\n",
    "    except:\n",
    "        # Collect the keys that need to be deleted\n",
    "        keys_to_delete.append(model_name)\n",
    "# Remove the collected keys from both dictionaries\n",
    "for key in keys_to_delete:\n",
    "    del models[key]\n",
    "    del chromosomes[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "891a2522-6f98-48cd-86cb-c6e8fe7a7906",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(models.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c53b7a4e-3e5b-4e6d-8040-7b554e0fc96b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=models[model_name]\n",
    "stage=2\n",
    "attribute_chain = f\"model.s2.b1.state_dict\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29ff17e3-62ec-4c22-819d-97af52cc12f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_state_dict(attribute_chain):\n",
    "        # Split the attribute chain into parts\n",
    "        attrs = attribute_chain.split('.')\n",
    "        # Start with the model object\n",
    "        current_obj = eval(attrs.pop(0))  # eval is used only to get 'model', this part is safe\n",
    "        # Iterate through the attribute chain\n",
    "        for attr in attrs:\n",
    "            current_obj = getattr(current_obj, attr)\n",
    "        # If the final attribute is callable (e.g., a method), call it\n",
    "        if callable(current_obj):\n",
    "            result = current_obj()\n",
    "        else:\n",
    "            result = current_obj\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "56011b9a-1161-4ead-b1f8-d9b5b6fd78f7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('f.a.weight',\n",
       "              tensor([[[[-0.0670]],\n",
       "              \n",
       "                       [[-0.0040]],\n",
       "              \n",
       "                       [[ 0.0527]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0299]],\n",
       "              \n",
       "                       [[-0.0517]],\n",
       "              \n",
       "                       [[ 0.0660]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0058]],\n",
       "              \n",
       "                       [[ 0.0004]],\n",
       "              \n",
       "                       [[-0.0177]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.1535]],\n",
       "              \n",
       "                       [[ 0.0154]],\n",
       "              \n",
       "                       [[ 0.0358]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0073]],\n",
       "              \n",
       "                       [[ 0.0054]],\n",
       "              \n",
       "                       [[ 0.0563]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0336]],\n",
       "              \n",
       "                       [[ 0.2118]],\n",
       "              \n",
       "                       [[-0.0352]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0560]],\n",
       "              \n",
       "                       [[-0.1056]],\n",
       "              \n",
       "                       [[-0.0503]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.1189]],\n",
       "              \n",
       "                       [[-0.0795]],\n",
       "              \n",
       "                       [[ 0.0678]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0376]],\n",
       "              \n",
       "                       [[-0.0731]],\n",
       "              \n",
       "                       [[-0.0714]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0129]],\n",
       "              \n",
       "                       [[-0.0512]],\n",
       "              \n",
       "                       [[ 0.1745]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1411]],\n",
       "              \n",
       "                       [[-0.0206]],\n",
       "              \n",
       "                       [[ 0.2064]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0045]],\n",
       "              \n",
       "                       [[ 0.0180]],\n",
       "              \n",
       "                       [[ 0.0085]]]], device='cuda:0')),\n",
       "             ('f.a_bn.weight',\n",
       "              tensor([0.9667, 0.9419, 0.7693, 0.9365, 0.8324, 0.9543, 0.9089, 0.9304, 0.9466,\n",
       "                      0.8674, 0.8818, 0.8855, 0.8518, 1.0737, 0.8738, 0.7998, 0.7813, 0.8516,\n",
       "                      1.0300, 0.7925, 0.9720, 0.9152, 1.0123, 0.8468, 0.9848, 0.8834, 0.9205,\n",
       "                      0.7687, 0.8480, 0.6855, 0.9706, 1.0471, 0.9139, 0.8278, 0.9179, 0.9253,\n",
       "                      0.7581, 0.7429, 0.9208, 1.1020, 0.9899, 0.9589, 0.7702, 1.0615, 0.9143,\n",
       "                      0.8075, 0.8772, 0.8488, 0.9612, 1.0570, 0.8106, 0.8144, 0.9447, 0.8576,\n",
       "                      0.8061, 0.9616, 0.8431, 1.0779, 0.8680, 0.8357, 0.8963, 0.8672, 0.9279,\n",
       "                      0.8251, 0.9013, 0.9638, 0.9095, 0.8015, 0.9663, 0.8706, 0.9936, 0.9215,\n",
       "                      0.8522, 0.9130, 0.8253, 0.9648, 0.7735, 0.8497, 1.0670, 0.9810, 0.8044,\n",
       "                      0.7580, 1.0658, 0.8748, 0.9273, 1.0134, 0.8376, 0.8406, 0.8830, 0.8195,\n",
       "                      0.8165, 0.7849, 1.0329, 0.9655, 1.0717, 0.8722, 0.8993, 0.8053, 0.9712,\n",
       "                      1.0149, 0.9242, 0.8828, 0.9970, 0.8595, 0.8381, 0.9558, 1.0017, 0.8415,\n",
       "                      0.8451, 0.8907, 0.9905, 0.8811, 0.8734, 0.8066, 0.7747, 0.8463, 1.1542,\n",
       "                      0.7492, 1.0201, 0.8862, 0.8415, 0.9955, 0.9927, 0.8604, 0.7863, 0.8718,\n",
       "                      0.8056, 0.9609, 0.8314, 0.8181, 1.0160, 0.9715, 1.0148, 0.8493, 0.9185,\n",
       "                      0.8910, 0.9263, 0.8900, 0.7950, 0.8600, 0.9792, 1.0048, 0.9527, 0.8424,\n",
       "                      0.9964, 0.8788, 0.8170, 0.7229, 0.9628, 0.9165, 0.9487, 0.9522, 0.8088,\n",
       "                      0.9188, 0.6571, 0.8308, 1.1398, 0.9925, 0.8487, 0.8259, 0.9190, 0.9469,\n",
       "                      0.8089, 0.8663, 0.9877, 0.9396, 0.8596, 0.9793, 0.9278, 0.8057, 0.8465,\n",
       "                      1.0624, 0.8460, 0.7362, 1.0653, 0.8227, 0.9394, 0.8742, 0.8609, 0.8578,\n",
       "                      0.9087, 1.0506, 0.8973, 0.9523, 0.8801, 1.0003, 0.8324, 0.8938, 0.9779,\n",
       "                      0.8723, 0.9547, 0.9065, 0.8836, 0.7765, 0.9907, 0.8410, 0.9580, 0.8589,\n",
       "                      0.8700, 1.0643, 0.9275, 0.8847, 0.9078, 0.9128, 0.9588, 0.8916, 0.9363,\n",
       "                      0.9493, 0.7628, 0.7474, 0.9702, 0.9213, 1.0349, 0.9108, 0.7517, 0.8433,\n",
       "                      0.9008, 0.8409, 0.9366, 0.8903, 0.9641, 0.9055, 0.9477, 0.9179, 0.9234,\n",
       "                      0.9162, 0.9108, 0.9560, 0.9469, 0.8414, 0.8738, 0.9238, 0.9016, 0.9004,\n",
       "                      0.9751, 0.9787, 0.8854, 0.9382, 0.8540, 0.9793, 0.9495, 0.8400, 0.9498,\n",
       "                      0.9867, 0.9163, 0.7445, 0.9745, 0.9184, 1.0172, 1.0046, 0.8790, 0.9579,\n",
       "                      0.8225, 0.7949, 0.8733, 0.9137, 0.9114, 0.9347, 0.8937, 0.9959, 0.7903,\n",
       "                      0.9562, 0.9013, 0.8810, 0.9854, 0.8441, 1.0244, 1.1655, 0.7621, 0.8456,\n",
       "                      0.8135, 0.7189, 0.8241, 0.9939, 0.6907, 0.9688, 0.8651, 0.9714, 0.9071,\n",
       "                      0.8883, 0.7507, 0.9061, 1.0341, 1.0299, 0.8850, 0.8352, 0.8991, 0.7510],\n",
       "                     device='cuda:0')),\n",
       "             ('f.a_bn.bias',\n",
       "              tensor([-0.0349, -0.0111, -0.0247,  0.0820, -0.1306,  0.0007, -0.0035,  0.0710,\n",
       "                       0.0505,  0.0151, -0.0107, -0.0192,  0.0202, -0.1156, -0.0385, -0.1522,\n",
       "                      -0.0290, -0.0761, -0.0269, -0.0026, -0.0343, -0.0922,  0.0082,  0.0440,\n",
       "                       0.0181, -0.0080, -0.0874, -0.1244, -0.1063, -0.1665, -0.1005,  0.0972,\n",
       "                      -0.0497, -0.0641, -0.0105, -0.0691, -0.1005, -0.1087, -0.1010,  0.1750,\n",
       "                       0.0614,  0.0573, -0.0263, -0.0033, -0.0070, -0.1770, -0.1092,  0.0356,\n",
       "                      -0.0318, -0.0477, -0.0749,  0.0667, -0.1139, -0.0399, -0.1157,  0.0017,\n",
       "                      -0.0727,  0.1305, -0.0597, -0.0352, -0.0453, -0.0653, -0.0024, -0.0980,\n",
       "                       0.0018, -0.0109,  0.0559, -0.0691,  0.1345,  0.0578,  0.1436, -0.0671,\n",
       "                      -0.0036,  0.0092, -0.1388,  0.0200, -0.0655,  0.0193, -0.0798, -0.0826,\n",
       "                      -0.0579, -0.0898,  0.1418,  0.0608, -0.0907,  0.1089,  0.0434,  0.0213,\n",
       "                      -0.0356,  0.0343, -0.0264, -0.0367,  0.0299,  0.0661,  0.1102, -0.1040,\n",
       "                      -0.0470, -0.0527, -0.0052,  0.0165, -0.1658, -0.0531, -0.0526, -0.0382,\n",
       "                       0.0251,  0.1033, -0.0435,  0.0173, -0.0492, -0.0568,  0.0898, -0.0142,\n",
       "                       0.0672, -0.1332, -0.1112, -0.0363,  0.0857,  0.0076, -0.0397,  0.0902,\n",
       "                       0.0145,  0.0526,  0.0899, -0.0280, -0.0703, -0.1537, -0.1733, -0.0976,\n",
       "                      -0.0247, -0.0761,  0.0526, -0.0422,  0.0176,  0.0867, -0.0097, -0.0162,\n",
       "                      -0.0420, -0.1221, -0.0946, -0.0239,  0.1292, -0.0855,  0.0281, -0.1103,\n",
       "                       0.0754, -0.1018, -0.0069, -0.1079, -0.0235,  0.0233,  0.0662,  0.1419,\n",
       "                       0.0667,  0.0111, -0.1577,  0.0340, -0.1058,  0.0726, -0.0905, -0.0840,\n",
       "                      -0.0876, -0.0118, -0.1473,  0.0380,  0.0534, -0.0721, -0.0554, -0.0462,\n",
       "                       0.0763, -0.0130, -0.0840,  0.0605, -0.0567, -0.1372,  0.1004, -0.1158,\n",
       "                      -0.0701,  0.0872, -0.0809,  0.1046,  0.1767,  0.1280, -0.0379,  0.0194,\n",
       "                      -0.0509,  0.0117, -0.0313, -0.0488,  0.0224,  0.0806,  0.0166,  0.0701,\n",
       "                      -0.0226, -0.1302, -0.0164, -0.0107,  0.0900, -0.0069, -0.0916,  0.1267,\n",
       "                      -0.0736, -0.0499,  0.0485,  0.0373,  0.0099, -0.0113, -0.0708,  0.0116,\n",
       "                      -0.0507, -0.1471,  0.0091,  0.1469,  0.1263, -0.1220, -0.1177, -0.0553,\n",
       "                      -0.0791, -0.0790,  0.0073,  0.0532,  0.1076,  0.0500,  0.0291, -0.0311,\n",
       "                      -0.0755, -0.0155,  0.0875,  0.0026,  0.1685,  0.0383, -0.0170,  0.0621,\n",
       "                      -0.0310,  0.1030,  0.0233,  0.1411, -0.0709,  0.1145,  0.0133,  0.0306,\n",
       "                      -0.0181,  0.0184,  0.0204,  0.1276,  0.0147, -0.1033, -0.0125,  0.0408,\n",
       "                       0.0953,  0.1702,  0.0134, -0.0438, -0.0142, -0.0899,  0.0719, -0.0313,\n",
       "                      -0.0667, -0.0878, -0.0692, -0.0640,  0.0026,  0.1491,  0.0323,  0.0527,\n",
       "                      -0.1042, -0.0424,  0.1233,  0.2715, -0.1572, -0.0238, -0.0380, -0.1296,\n",
       "                      -0.1073, -0.0839, -0.2232, -0.0363,  0.0478,  0.1374, -0.0616, -0.0281,\n",
       "                      -0.0749, -0.0672,  0.1535,  0.1676,  0.0370, -0.0049, -0.0486, -0.0692],\n",
       "                     device='cuda:0')),\n",
       "             ('f.a_bn.running_mean',\n",
       "              tensor([-2.5448, -0.5190,  2.5256,  1.7079, -0.1399, -2.7508, -0.2052,  0.4818,\n",
       "                      -3.2428,  2.3681, -1.6702, -3.5227, -0.0678, -2.8952, -0.6515, -2.4123,\n",
       "                       0.3819, -0.8528, -0.3028, -3.1428, -3.3781, -2.0740, -1.3155, -2.3516,\n",
       "                      -0.4626, -0.8320, -0.3917,  2.6672,  0.6751, -3.7102, -3.6675, -1.5774,\n",
       "                      -0.4734,  0.9609, -3.6405,  0.6166,  1.8242, -2.1917,  1.0514,  1.8903,\n",
       "                      -1.8289,  0.0407, -0.5570, -1.3194,  0.8105, -0.9840,  0.5105,  2.5969,\n",
       "                      -0.5437, -0.9006, -0.5663, -0.3600, -3.7737, -2.9820, -2.2086,  0.4791,\n",
       "                       1.2794, -0.0171, -1.9462, -1.3490,  1.1737, -1.3750, -1.2641, -1.4310,\n",
       "                      -0.4324, -0.2089,  1.3692,  1.4783, -1.0198, -3.5462, -1.4045,  0.5242,\n",
       "                       0.0931,  0.0286, -0.8957,  0.7734,  1.0148,  1.7348, -1.4206,  0.2246,\n",
       "                      -1.7229, -1.9740, -0.4769,  1.9090, -1.6336, -0.8202, -0.4678,  0.7986,\n",
       "                      -1.5927, -1.5998, -1.4216,  2.4279, -0.6439,  0.0721, -1.4618,  0.9494,\n",
       "                      -2.7463, -0.1982,  0.5355, -0.7686, -0.4794, -0.6378,  0.1428, -2.1895,\n",
       "                      -1.7420,  1.9119, -0.0934,  1.7597, -0.4999,  0.9244, -0.6118, -0.1609,\n",
       "                      -1.9007, -2.7127, -2.2671,  0.3958, -1.4771,  1.3999, -0.4035,  0.9937,\n",
       "                      -3.0332, -0.3439, -0.6397, -0.8090, -1.3905, -1.0871, -3.3263, -1.4100,\n",
       "                       0.1037, -2.6440, -1.5682, -2.0806, -1.7603,  0.7524, -1.4640, -2.2647,\n",
       "                      -1.4708, -0.2210,  3.1105, -0.9844, -0.2670, -1.8245,  3.4169, -3.1268,\n",
       "                      -1.7106, -1.8138, -0.0721, -0.8264, -0.7644,  1.2756,  0.0867,  1.2302,\n",
       "                      -1.6056, -2.1334,  1.9658, -2.4517, -1.1948, -3.0385, -2.3180, -2.0630,\n",
       "                      -2.9014,  2.9711,  0.4102,  0.2434, -0.2767,  2.8750, -2.6442,  2.8924,\n",
       "                       0.2086,  1.6768, -3.5059, -1.1583, -2.8893,  3.2710, -2.4230,  0.3047,\n",
       "                      -3.4113, -0.9719, -4.5289, -0.8481, -1.3784, -0.1923, -1.8063, -3.0386,\n",
       "                      -2.8451, -1.8087,  0.7184, -1.2683,  1.8974,  1.9911,  0.3552,  1.8440,\n",
       "                      -3.1288,  0.6180, -0.3037,  3.3399, -1.9041, -1.0364,  0.1508, -1.7080,\n",
       "                      -0.7930,  0.3328,  1.6007, -0.3415, -0.4015, -0.7635, -1.3969,  2.4286,\n",
       "                      -1.7905, -1.2960, -1.6791,  1.2820,  0.1131, -1.6693, -1.2174, -0.8940,\n",
       "                       3.2762,  1.5078,  0.1340,  1.4768,  1.7020,  0.7001, -0.0368,  0.1796,\n",
       "                      -0.8887, -0.2990,  0.2682, -0.4421,  0.5950, -0.7087, -1.1557, -1.3616,\n",
       "                       1.2764, -0.9153, -0.9302, -0.5214, -1.3103, -1.6229, -0.7745, -1.5548,\n",
       "                      -0.5419, -1.2023,  4.0144, -1.7618, -0.2445, -1.6449, -3.8608, -2.5404,\n",
       "                      -0.2430, -0.3936,  0.3155, -4.1916, -0.2216, -2.4643,  2.1147, -2.7608,\n",
       "                      -1.9935, -2.6308, -1.4921, -0.8525, -0.2391,  0.7267, -0.3266, -1.3723,\n",
       "                      -3.3067,  0.3775, -0.5778, -2.0716, -3.4234,  1.5903, -0.9739,  2.7526,\n",
       "                      -1.3825,  1.0856, -0.8133, -1.4876, -0.3252, -0.6193, -0.9744, -1.6953,\n",
       "                      -0.6867, -1.2194,  0.4119,  0.8548, -2.4716, -1.9137, -1.4096, -0.7922],\n",
       "                     device='cuda:0')),\n",
       "             ('f.a_bn.running_var',\n",
       "              tensor([ 3.3215,  3.4294, 11.4048,  4.4079,  2.4048,  2.8796,  4.2458,  4.3047,\n",
       "                       6.2011,  5.6069,  7.1748,  7.5344,  4.8857,  2.9629,  3.6710,  6.0088,\n",
       "                       7.8033,  5.6528,  3.1832,  3.1355,  6.4771,  3.8749,  4.2605,  5.7252,\n",
       "                       4.1468,  5.2171,  2.3465,  8.6731,  5.8936, 10.2259,  4.3238,  7.4189,\n",
       "                       2.7152,  2.4393,  7.2440,  3.6612,  3.0213,  7.1747,  3.1040,  3.6687,\n",
       "                       3.8965,  6.9911,  3.2262,  5.5330,  3.6023,  3.6761,  3.4379, 13.4547,\n",
       "                       3.4788,  3.5660,  5.1998,  5.0831,  5.1819,  5.7231,  9.0488,  3.8147,\n",
       "                       5.2419,  5.0356,  5.1774, 11.4187,  3.3480,  6.0192,  8.4790,  4.8916,\n",
       "                       5.0139,  4.2306,  7.9928,  4.8914,  3.5743,  8.2127,  6.3977,  4.1878,\n",
       "                       3.1478,  5.4716,  4.8250,  5.2297,  4.0179,  4.9698,  2.6464,  7.4809,\n",
       "                       2.8188,  5.1067,  3.5823,  5.9622,  3.6848,  4.5112,  5.7519,  6.2719,\n",
       "                       3.9934,  5.1557,  3.2548,  5.8293,  4.7313,  5.1325,  5.0428,  2.9198,\n",
       "                       3.2892,  3.2829,  3.8179,  4.3246,  3.1984,  3.2468,  5.1736,  6.8671,\n",
       "                       3.0161,  7.2984,  3.6121, 11.3914,  4.1519,  3.3668,  4.2847,  7.8345,\n",
       "                       6.1353,  6.3113,  5.4134,  4.1466,  5.8781,  4.4851,  5.1515,  4.1321,\n",
       "                       5.7683,  4.3421,  5.3531,  4.0763,  6.9606,  4.5086, 11.5325,  4.5683,\n",
       "                       6.1421,  8.4650,  3.3266,  6.2766,  4.8159,  5.4076,  3.5259,  2.7490,\n",
       "                       4.5544,  2.9256, 11.3359,  3.6319,  6.1342,  3.0369, 10.3297,  2.3097,\n",
       "                       3.5638,  4.4062,  3.0490,  3.2131,  4.2114, 10.2208,  6.8457,  9.0467,\n",
       "                       6.6582,  4.6910,  5.8555,  5.6132,  3.6347,  4.1096,  4.7027,  7.3013,\n",
       "                       4.2244,  6.4463,  4.6584,  7.3870,  5.1840,  6.9393,  4.1265,  7.2898,\n",
       "                       3.0248,  4.9691, 13.6207,  2.9536,  4.8237,  8.6168,  4.7402,  2.4109,\n",
       "                       3.0421,  3.4931,  3.6701,  9.3780,  5.5273,  4.4440, 10.3024,  7.0430,\n",
       "                       7.1610,  4.0589,  2.8056,  5.8994,  9.7815,  8.0594,  8.4189, 13.4795,\n",
       "                       5.9591,  5.0059,  6.5117, 11.4654,  3.9149,  3.0395,  4.0362,  7.8905,\n",
       "                       2.6938,  3.3751,  4.8997,  4.1270,  5.0872,  5.4733,  4.5873,  7.2106,\n",
       "                       4.4660,  4.5425,  3.3247,  8.2174,  8.8760,  7.6423,  5.6128,  4.5648,\n",
       "                       5.2197,  4.3976,  2.8651,  3.3391,  6.1582,  5.5546,  3.9704,  4.3377,\n",
       "                       5.6722,  5.9510,  5.0886,  4.2337,  9.2000,  4.3647,  5.9124,  3.9098,\n",
       "                       4.7353,  5.7194,  2.9629,  3.9648,  3.8780,  2.6625,  2.8948,  6.9508,\n",
       "                       9.1341,  5.8673,  9.0027,  7.3631,  3.9958,  7.3872,  2.7254,  5.5991,\n",
       "                       5.0247,  5.5745,  5.5972, 11.5805,  3.8740,  3.1559,  4.1529,  7.1396,\n",
       "                       2.7908,  3.0679,  5.3349,  6.9932,  3.1543,  5.5243,  5.8183,  3.4391,\n",
       "                       4.0018,  3.9049,  4.8922,  5.0849,  2.3612, 10.0290,  4.7144,  9.2820,\n",
       "                       3.1560,  3.9637,  3.7326,  4.1374,  4.6223,  4.9430,  2.8849,  7.3378,\n",
       "                       2.5234,  6.0552,  7.1635,  7.8492,  4.1332,  3.5463,  5.6811,  3.0243],\n",
       "                     device='cuda:0')),\n",
       "             ('f.a_bn.num_batches_tracked', tensor(16497, device='cuda:0')),\n",
       "             ('f.b.weight',\n",
       "              tensor([[[[-0.0745,  0.0708,  0.1366],\n",
       "                        [-0.1130, -0.0943, -0.0107],\n",
       "                        [-0.0416,  0.1471,  0.1470]],\n",
       "              \n",
       "                       [[ 0.1114,  0.0783,  0.0518],\n",
       "                        [-0.0287,  0.0801,  0.1098],\n",
       "                        [-0.0994, -0.0578,  0.1458]],\n",
       "              \n",
       "                       [[-0.0519, -0.0450,  0.0149],\n",
       "                        [ 0.0064, -0.0168,  0.0995],\n",
       "                        [ 0.0479, -0.0306, -0.0348]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0024,  0.0016, -0.0439],\n",
       "                        [ 0.0868,  0.0929, -0.0234],\n",
       "                        [ 0.0446,  0.0596, -0.0220]],\n",
       "              \n",
       "                       [[ 0.0376,  0.0548,  0.1380],\n",
       "                        [-0.0906, -0.0004,  0.1665],\n",
       "                        [-0.0461, -0.0780,  0.0414]],\n",
       "              \n",
       "                       [[ 0.0578, -0.0337, -0.0795],\n",
       "                        [ 0.1139,  0.0662, -0.0426],\n",
       "                        [ 0.0973,  0.0920, -0.0136]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0025, -0.0029, -0.0423],\n",
       "                        [-0.0174,  0.0329, -0.0489],\n",
       "                        [ 0.0393,  0.0255,  0.0453]],\n",
       "              \n",
       "                       [[-0.0437, -0.0324,  0.0009],\n",
       "                        [ 0.0240, -0.0697, -0.0447],\n",
       "                        [-0.0372, -0.0900, -0.0697]],\n",
       "              \n",
       "                       [[-0.0494, -0.1280, -0.1459],\n",
       "                        [ 0.0040, -0.1236, -0.0836],\n",
       "                        [ 0.0279, -0.0442, -0.0509]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0836,  0.0984,  0.0926],\n",
       "                        [ 0.0518,  0.0936,  0.0226],\n",
       "                        [ 0.0831,  0.0416,  0.0955]],\n",
       "              \n",
       "                       [[-0.1160, -0.1060, -0.0505],\n",
       "                        [-0.1028, -0.0146, -0.0055],\n",
       "                        [-0.0678, -0.0729, -0.0437]],\n",
       "              \n",
       "                       [[-0.0584, -0.0077, -0.0064],\n",
       "                        [ 0.0044, -0.0101,  0.0950],\n",
       "                        [-0.0018, -0.0116,  0.0131]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1337, -0.0240,  0.0116],\n",
       "                        [-0.0171,  0.0094,  0.0446],\n",
       "                        [ 0.0730,  0.1010,  0.0703]],\n",
       "              \n",
       "                       [[-0.0949, -0.0761, -0.0808],\n",
       "                        [-0.0735, -0.0668, -0.0481],\n",
       "                        [-0.0446, -0.1294, -0.0996]],\n",
       "              \n",
       "                       [[ 0.0958, -0.0346, -0.0710],\n",
       "                        [ 0.0636,  0.0383, -0.0753],\n",
       "                        [-0.0281, -0.0587, -0.0408]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.1225, -0.0707, -0.0265],\n",
       "                        [-0.0933, -0.0780, -0.0610],\n",
       "                        [ 0.0474,  0.0133,  0.0700]],\n",
       "              \n",
       "                       [[-0.0087,  0.0146, -0.0107],\n",
       "                        [ 0.0982, -0.0236, -0.0485],\n",
       "                        [ 0.0736, -0.0085, -0.1910]],\n",
       "              \n",
       "                       [[ 0.0203,  0.0139,  0.0432],\n",
       "                        [ 0.0411,  0.1220,  0.0666],\n",
       "                        [-0.1215, -0.1112, -0.1310]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0106, -0.0504, -0.1831],\n",
       "                        [ 0.1221,  0.0315, -0.0480],\n",
       "                        [ 0.0419, -0.0223,  0.0288]],\n",
       "              \n",
       "                       [[-0.0218,  0.0420,  0.0705],\n",
       "                        [-0.1081,  0.0503,  0.0267],\n",
       "                        [-0.1509, -0.0638, -0.0376]],\n",
       "              \n",
       "                       [[-0.0122, -0.0584,  0.0637],\n",
       "                        [ 0.1685, -0.0026, -0.0370],\n",
       "                        [ 0.1177,  0.0881, -0.0211]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0127,  0.0430,  0.0588],\n",
       "                        [-0.0068,  0.0607,  0.0410],\n",
       "                        [-0.0608, -0.0142,  0.1372]],\n",
       "              \n",
       "                       [[-0.0041, -0.1026, -0.0867],\n",
       "                        [-0.0687, -0.0011, -0.0024],\n",
       "                        [-0.1353,  0.0314, -0.0423]],\n",
       "              \n",
       "                       [[-0.0476, -0.0256, -0.1195],\n",
       "                        [ 0.0476, -0.0084, -0.0029],\n",
       "                        [ 0.0465, -0.0115, -0.0861]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0260, -0.0080, -0.0549],\n",
       "                        [ 0.0012, -0.1639,  0.0260],\n",
       "                        [ 0.0709, -0.0491, -0.0149]],\n",
       "              \n",
       "                       [[ 0.0667, -0.0344,  0.0036],\n",
       "                        [ 0.0738, -0.0146,  0.0151],\n",
       "                        [-0.0054,  0.0829,  0.0670]],\n",
       "              \n",
       "                       [[-0.0472, -0.0060,  0.0754],\n",
       "                        [ 0.0798,  0.0723,  0.0800],\n",
       "                        [ 0.0102, -0.0069, -0.1302]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0678,  0.0185,  0.0070],\n",
       "                        [ 0.0450, -0.0014, -0.0151],\n",
       "                        [-0.0869, -0.0005, -0.1022]],\n",
       "              \n",
       "                       [[-0.1215, -0.0539,  0.0698],\n",
       "                        [-0.0311,  0.0375, -0.0145],\n",
       "                        [-0.0617, -0.0909, -0.0820]],\n",
       "              \n",
       "                       [[-0.0977, -0.0827, -0.0348],\n",
       "                        [-0.0597, -0.0537, -0.0035],\n",
       "                        [ 0.0369,  0.0633,  0.0450]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0491,  0.0481,  0.1296],\n",
       "                        [ 0.0235, -0.0655,  0.0739],\n",
       "                        [ 0.0051, -0.0679, -0.0069]],\n",
       "              \n",
       "                       [[ 0.0865, -0.0518, -0.0264],\n",
       "                        [ 0.1347, -0.0602, -0.0366],\n",
       "                        [-0.0620,  0.0065, -0.0507]],\n",
       "              \n",
       "                       [[-0.0273,  0.0598,  0.1342],\n",
       "                        [-0.0665, -0.0403,  0.0484],\n",
       "                        [-0.1096, -0.2054, -0.0616]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0738, -0.0803, -0.0928],\n",
       "                        [-0.0045,  0.0646,  0.0753],\n",
       "                        [ 0.0530,  0.0446,  0.0820]],\n",
       "              \n",
       "                       [[-0.1515, -0.0911, -0.0858],\n",
       "                        [-0.0635, -0.0858, -0.0555],\n",
       "                        [ 0.0850,  0.0698,  0.0666]],\n",
       "              \n",
       "                       [[-0.0453,  0.0277, -0.0334],\n",
       "                        [-0.0643, -0.0400,  0.0133],\n",
       "                        [-0.0014, -0.0514, -0.0279]]]], device='cuda:0')),\n",
       "             ('f.b_bn.weight',\n",
       "              tensor([0.9384, 0.8401, 1.0394, 0.9050, 0.8600, 0.7950, 0.9626, 0.9492, 0.8787,\n",
       "                      0.9311, 1.0143, 0.9184, 1.0165, 0.8327, 0.8667, 0.9312, 0.9398, 0.9495,\n",
       "                      0.9460, 0.9568, 0.9873, 0.9269, 1.0122, 0.8640, 1.1239, 0.9128, 0.9389,\n",
       "                      0.9157, 0.8747, 0.8352, 0.9202, 0.9453, 0.7985, 0.8640, 0.7855, 0.8982,\n",
       "                      0.8586, 0.9815, 0.8290, 0.8750, 0.8946, 0.8374, 0.8186, 0.9798, 0.9179,\n",
       "                      0.9064, 0.9327, 0.8704, 0.9683, 0.9994, 0.9425, 0.8512, 0.8550, 0.8936,\n",
       "                      0.9671, 0.8832, 0.9506, 0.9712, 0.9159, 0.8999, 0.8855, 0.9402, 0.9022,\n",
       "                      0.9927, 0.9163, 0.9501, 0.9741, 0.9406, 0.8639, 1.0145, 0.8822, 0.9658,\n",
       "                      0.8398, 0.9121, 0.9176, 0.9100, 0.9818, 0.9792, 0.9460, 1.0273, 0.9275,\n",
       "                      1.0495, 0.8949, 0.8904, 0.8509, 0.8541, 0.8442, 0.9139, 0.9538, 0.9494,\n",
       "                      0.9223, 0.9114, 0.7590, 0.9436, 0.9194, 0.9453, 0.7891, 0.9819, 0.8770,\n",
       "                      0.8684, 0.9017, 0.9887, 0.8266, 0.9493, 0.9017, 0.9548, 0.9408, 1.0573,\n",
       "                      0.8134, 0.9207, 0.8596, 0.8668, 0.8426, 1.0055, 0.8936, 0.9887, 0.8959,\n",
       "                      0.8933, 0.9565, 0.8773, 0.9267, 0.9591, 0.9199, 0.8696, 0.9156, 0.9192,\n",
       "                      0.8655, 0.9945, 0.8346, 0.8221, 0.9547, 0.8901, 0.8117, 0.9147, 0.9592,\n",
       "                      0.9296, 0.9014, 0.9668, 0.8682, 0.9284, 0.9078, 0.8528, 0.8645, 0.9157,\n",
       "                      0.8494, 0.9032, 1.0068, 0.9185, 0.9576, 1.0148, 0.9548, 0.9334, 0.9710,\n",
       "                      0.9558, 0.9742, 1.0012, 1.0027, 0.9304, 0.9512, 0.9550, 0.9546, 0.9183,\n",
       "                      1.1264, 0.9761, 0.8917, 0.9006, 0.9780, 1.0044, 0.8947, 0.9372, 0.8921,\n",
       "                      0.8335, 0.8912, 0.9206, 0.9137, 0.9413, 0.8389, 0.9413, 0.9019, 0.8548,\n",
       "                      1.0204, 0.7778, 0.8692, 0.8784, 0.9714, 0.9361, 0.8803, 0.9357, 0.9139,\n",
       "                      0.8843, 0.9236, 0.8395, 0.9868, 1.0515, 0.9310, 0.9227, 0.8414, 0.8468,\n",
       "                      0.9237, 0.8943, 0.9040, 0.8405, 0.9909, 0.8069, 0.8812, 0.8926, 0.9038,\n",
       "                      0.7779, 0.9010, 0.9894, 0.9183, 0.9201, 0.8667, 0.9259, 0.9545, 1.0533,\n",
       "                      0.8127, 0.9380, 0.8885, 0.9244, 0.8136, 0.8332, 0.8687, 0.9252, 0.8587,\n",
       "                      0.8796, 0.8587, 0.9196, 0.9453, 0.8853, 1.0391, 0.8915, 0.8584, 0.8650,\n",
       "                      0.8724, 0.8451, 0.8524, 0.9338, 0.8118, 0.8112, 0.8947, 0.8853, 0.9937,\n",
       "                      0.9674, 0.9504, 0.8822, 0.8624, 0.8492, 0.8851, 0.8965, 0.8809, 0.9409,\n",
       "                      0.7550, 0.9712, 0.8465, 0.9647, 0.9303, 0.9127, 0.8772, 0.9912, 0.8812,\n",
       "                      0.8596, 0.8712, 0.8918, 0.9766, 0.9109, 0.8721, 1.0158, 1.0147, 0.8060,\n",
       "                      0.9088, 0.9686, 0.9180, 0.8556, 1.0441, 0.9891, 1.0230, 0.9142, 0.9063,\n",
       "                      0.9252, 0.8442, 0.9553, 0.8708, 0.9518, 0.8311, 0.8958, 0.9138, 0.8930],\n",
       "                     device='cuda:0')),\n",
       "             ('f.b_bn.bias',\n",
       "              tensor([ 0.0201, -0.0991, -0.0512,  0.0207,  0.0595, -0.1681,  0.0707,  0.0253,\n",
       "                      -0.1207,  0.0684,  0.0067, -0.0445,  0.0500, -0.0854, -0.1020, -0.0562,\n",
       "                      -0.1015, -0.0262, -0.0557,  0.1264, -0.0073,  0.0297,  0.1904, -0.0246,\n",
       "                      -0.1503, -0.0364, -0.0509,  0.0712, -0.0796, -0.0429, -0.0852, -0.0256,\n",
       "                      -0.1124, -0.0701, -0.0909, -0.0848,  0.0494, -0.0633, -0.0998,  0.0291,\n",
       "                      -0.0906, -0.1124, -0.1101, -0.0585, -0.0356,  0.0335, -0.0078,  0.0209,\n",
       "                       0.0196, -0.0634, -0.0489, -0.1065, -0.0475,  0.0491,  0.0299, -0.0415,\n",
       "                       0.0252,  0.0383,  0.0288, -0.0088, -0.0188,  0.0322, -0.0305,  0.0102,\n",
       "                       0.0942,  0.0124, -0.0173,  0.0814, -0.0989,  0.1407,  0.0022,  0.0174,\n",
       "                      -0.0114, -0.0166, -0.0440, -0.0475,  0.0289,  0.0074, -0.0945, -0.0314,\n",
       "                      -0.1332,  0.0691, -0.1159, -0.0105, -0.0239, -0.1509, -0.0633, -0.0388,\n",
       "                      -0.0078, -0.0263, -0.0683, -0.0349, -0.0789, -0.0328, -0.0373,  0.0300,\n",
       "                      -0.0984, -0.0522, -0.1201,  0.0353, -0.0240,  0.0347, -0.1821,  0.0416,\n",
       "                      -0.0147, -0.0553, -0.0099,  0.0718, -0.0072, -0.0896,  0.0138, -0.0735,\n",
       "                      -0.0129, -0.1138, -0.0751, -0.0046, -0.1101, -0.0564, -0.0706, -0.1765,\n",
       "                      -0.0879,  0.0114, -0.0577, -0.0535,  0.0512,  0.0048,  0.0223,  0.0472,\n",
       "                      -0.0552, -0.0444, -0.0322,  0.0556, -0.1032, -0.1005,  0.0506, -0.0289,\n",
       "                      -0.1137,  0.0515, -0.0467,  0.0933, -0.1373,  0.0463, -0.0339, -0.0116,\n",
       "                      -0.0167,  0.0029, -0.0237, -0.0991, -0.0556,  0.0205, -0.0078, -0.0580,\n",
       "                      -0.0872, -0.0020,  0.0709,  0.0996, -0.1664, -0.0613, -0.0689,  0.0616,\n",
       "                       0.0766, -0.0224, -0.0238, -0.0511,  0.0016, -0.0067,  0.1037,  0.1005,\n",
       "                      -0.0665,  0.0959, -0.1172, -0.0421, -0.1289, -0.0789, -0.1003, -0.0220,\n",
       "                      -0.0538,  0.0737,  0.0390,  0.0074,  0.0103, -0.0332, -0.0137, -0.0833,\n",
       "                      -0.0257, -0.0554,  0.0371, -0.0046,  0.0386, -0.0211,  0.0335, -0.0626,\n",
       "                      -0.0603,  0.0575, -0.0246, -0.0341, -0.1114, -0.0924, -0.0146, -0.0059,\n",
       "                       0.0065, -0.0126,  0.0005, -0.0749, -0.0063,  0.0512, -0.0467, -0.0753,\n",
       "                      -0.0850, -0.0051,  0.0142, -0.0282, -0.0548, -0.0565, -0.0357,  0.0554,\n",
       "                      -0.1117, -0.0106, -0.1007,  0.0028, -0.0558, -0.0303, -0.0350, -0.1342,\n",
       "                      -0.1126, -0.0863, -0.0536, -0.1229, -0.1579,  0.0050,  0.1498, -0.0145,\n",
       "                      -0.0209, -0.0197, -0.0198, -0.0559, -0.0594,  0.0522, -0.0616, -0.1544,\n",
       "                      -0.1473, -0.1547,  0.0798,  0.0223, -0.0174, -0.0207, -0.0937, -0.0157,\n",
       "                      -0.1407, -0.0321, -0.0701, -0.1123, -0.1797,  0.0233, -0.0042, -0.1133,\n",
       "                      -0.0027, -0.0175, -0.1014,  0.0100, -0.0826, -0.0932, -0.0405, -0.0696,\n",
       "                       0.0181, -0.1211,  0.0296,  0.0461,  0.0176, -0.0522, -0.1119,  0.0787,\n",
       "                       0.0045,  0.0324, -0.0268, -0.1064,  0.0631,  0.0214, -0.0311,  0.1033,\n",
       "                      -0.0236,  0.0521, -0.0316,  0.0572,  0.0086, -0.0015, -0.0998, -0.0027],\n",
       "                     device='cuda:0')),\n",
       "             ('f.b_bn.running_mean',\n",
       "              tensor([ 3.1872e-01, -3.0708e-01, -2.2208e-01,  2.1509e-01, -7.9734e-01,\n",
       "                       5.9012e-03, -4.3875e-02, -6.0110e-01,  2.3622e-01,  3.2032e-01,\n",
       "                       1.1686e-01, -1.1826e-01, -4.0021e-02, -5.9289e-02,  1.3963e-01,\n",
       "                      -4.7828e-02,  7.3280e-03,  3.5927e-01,  1.2007e-01, -8.8846e-02,\n",
       "                       1.7286e-01,  1.9898e-01,  4.2524e-02,  3.9281e-01, -1.5938e-01,\n",
       "                      -3.1005e-01, -2.5971e-01,  1.0847e-02, -1.1520e-01, -8.4227e-02,\n",
       "                       6.8441e-02, -3.7595e-02, -5.1119e-01, -4.0141e-01, -3.5204e-01,\n",
       "                      -1.4147e-01,  2.5489e-01,  3.3905e-01, -1.4253e-01, -2.7279e-01,\n",
       "                       2.6845e-02,  1.8600e-01, -1.2273e-01,  3.9284e-01,  1.6671e-01,\n",
       "                       2.8403e-01,  4.3460e-01, -5.0010e-01,  3.0595e-01,  1.0524e-01,\n",
       "                      -1.5733e-01, -2.2406e-01,  2.6894e-01,  1.9348e-02,  3.1385e-01,\n",
       "                      -1.6020e-01, -6.5080e-01, -5.0666e-02,  2.4654e-01,  4.4161e-01,\n",
       "                      -1.9061e-01, -2.6302e-01, -1.9757e-01, -3.5768e-02,  1.9796e-01,\n",
       "                      -9.6966e-01,  3.0635e-01, -6.4026e-01,  2.1804e-01,  2.1900e-01,\n",
       "                       1.0775e-01,  3.4433e-01, -3.7752e-01,  3.3869e-01,  1.0984e-01,\n",
       "                      -5.4206e-01,  5.5650e-01, -5.9376e-01, -9.0936e-03, -8.7070e-01,\n",
       "                      -4.0555e-02,  1.9582e-01,  9.2132e-03, -4.5845e-01,  3.8517e-01,\n",
       "                       8.7562e-04,  1.6988e-01,  1.3721e-01,  2.0693e-01, -1.7619e-02,\n",
       "                      -1.0740e-01,  3.8600e-01, -7.2153e-02, -9.3591e-02, -3.6960e-01,\n",
       "                      -2.9484e-01, -1.4776e-01,  2.3879e-02,  4.1732e-01, -4.0367e-01,\n",
       "                       9.2160e-02, -4.2642e-01,  1.9547e-01, -4.2216e-01, -9.2732e-01,\n",
       "                       1.0381e+00,  3.7569e-01,  1.5327e-01,  4.2289e-01, -2.9718e-02,\n",
       "                      -2.7390e-01,  1.0916e-01,  3.4107e-01, -9.7346e-02, -1.0207e-01,\n",
       "                      -1.2723e-01, -3.3188e-01, -2.9200e-01,  3.0146e-01, -4.8078e-02,\n",
       "                       2.5391e-01, -1.7022e-01, -1.8638e-01, -5.1091e-02, -4.4902e-01,\n",
       "                       1.7209e-01,  1.3330e-01,  1.2239e-01,  2.8096e-01, -2.7219e-01,\n",
       "                       4.4038e-01,  1.8241e-01, -7.6182e-01, -1.8612e-01,  8.4898e-02,\n",
       "                       1.1534e-01,  1.4649e-01,  2.4377e-01, -1.3085e-01, -5.7800e-01,\n",
       "                      -8.1819e-02,  3.0808e-01,  3.8541e-01,  6.9365e-01,  3.2926e-01,\n",
       "                       1.1632e-01, -7.3688e-01, -1.3196e-01, -1.5915e-01,  4.0207e-01,\n",
       "                      -3.2928e-01, -5.7363e-01,  2.2897e-01, -2.0039e-01, -4.8340e-01,\n",
       "                       1.4041e-01,  1.5803e-01,  1.3466e-01,  1.1593e-01, -2.3006e-02,\n",
       "                      -2.1636e-01, -6.1710e-02, -7.7072e-01, -7.8565e-01, -2.4983e-01,\n",
       "                      -2.0899e-01,  1.9429e-01,  4.4857e-01,  1.6573e-01,  1.2485e-01,\n",
       "                       3.1569e-01, -1.7266e-01, -3.9750e-01, -4.2628e-01,  2.1491e-03,\n",
       "                      -4.8411e-01, -3.8257e-01, -2.2685e-01, -1.2290e-01,  3.2365e-01,\n",
       "                       5.8035e-02,  3.8057e-01,  8.2154e-01, -4.7139e-01, -2.8643e-01,\n",
       "                       7.9742e-02, -1.0702e-01,  5.1540e-01,  3.8399e-01,  4.2024e-01,\n",
       "                      -2.9664e-02,  8.0364e-01, -2.0061e-01,  1.2792e-01,  2.2205e-02,\n",
       "                      -1.1285e+00,  7.5686e-02,  3.7227e-01, -5.4786e-01, -1.7357e-01,\n",
       "                      -2.8121e-01,  9.5693e-02,  6.1710e-01, -4.3079e-01, -6.0031e-01,\n",
       "                      -9.2624e-01,  3.2731e-02,  2.2458e-01, -4.7054e-01, -9.3012e-03,\n",
       "                      -4.8455e-01, -2.1975e-01,  2.7861e-01,  2.4160e-01,  3.0922e-02,\n",
       "                       7.7742e-02,  2.7700e-01, -6.2349e-01,  1.8014e-02, -8.2731e-01,\n",
       "                       1.3781e-01, -1.2603e-01, -5.1871e-01,  1.4962e-01, -4.7568e-01,\n",
       "                       1.6867e-01,  9.3404e-02, -1.6642e-01,  8.7112e-02,  7.3196e-02,\n",
       "                      -1.4736e-01, -5.7778e-02,  2.5920e-01, -2.1839e-03, -1.5009e-01,\n",
       "                       1.9810e-01, -1.9808e-01,  8.0269e-02,  1.6441e-01, -3.3254e-02,\n",
       "                       5.7187e-01, -2.8336e-01,  1.5932e-02, -8.8556e-02, -5.7455e-02,\n",
       "                      -6.2977e-01,  6.9357e-01,  3.5947e-01, -3.6718e-01, -4.9222e-01,\n",
       "                      -4.1981e-02,  2.2964e-02, -3.1827e-01, -2.3092e-02, -5.7752e-02,\n",
       "                      -6.3147e-02, -6.2597e-02,  4.5549e-01,  1.7879e-01,  2.0509e-02,\n",
       "                       4.2521e-01,  5.9372e-01, -1.9723e-01, -3.1070e-01, -3.3884e-02,\n",
       "                       5.1324e-01, -6.7447e-01, -4.0658e-02, -3.0778e-01, -3.1455e-01,\n",
       "                       3.0758e-01, -2.8199e-01, -2.2060e-01, -1.5954e-01, -2.9511e-02,\n",
       "                       1.0903e-01,  7.1858e-01,  2.4751e-01,  1.0894e-01, -4.0385e-01,\n",
       "                      -2.8433e-02, -4.0076e-02, -5.4646e-01,  4.1924e-01, -1.2969e-01,\n",
       "                       3.2373e-02, -1.0393e-01, -2.2531e-01], device='cuda:0')),\n",
       "             ('f.b_bn.running_var',\n",
       "              tensor([0.3507, 0.6807, 0.4877, 0.2267, 0.6421, 0.2405, 0.3899, 0.2636, 0.2965,\n",
       "                      0.3007, 0.2155, 0.1448, 0.1875, 0.2216, 0.1439, 0.2737, 0.2098, 0.2174,\n",
       "                      0.2066, 0.3099, 0.3557, 0.1869, 0.4939, 0.1715, 0.2170, 0.1521, 0.2839,\n",
       "                      0.1977, 0.1987, 0.1381, 0.2333, 0.2242, 0.5789, 0.4752, 0.3621, 0.2151,\n",
       "                      0.3295, 0.3042, 0.2683, 0.7129, 0.3195, 0.4561, 0.3565, 0.4630, 0.2021,\n",
       "                      0.3211, 0.3528, 0.5841, 0.2687, 0.2331, 0.3081, 0.1785, 0.1487, 0.1628,\n",
       "                      0.2092, 0.2282, 0.3189, 0.1979, 0.2491, 0.1883, 0.4409, 0.1483, 0.5469,\n",
       "                      0.2106, 0.4947, 0.5866, 0.2743, 0.4705, 0.2611, 0.3502, 0.8544, 0.2077,\n",
       "                      0.3708, 0.2359, 0.2560, 0.4352, 0.4062, 0.4814, 0.2473, 0.6821, 0.3516,\n",
       "                      0.3730, 0.4066, 0.3993, 0.4240, 0.3155, 0.3083, 0.2513, 0.6118, 0.2475,\n",
       "                      0.2637, 0.6325, 0.1590, 0.3242, 0.2991, 0.5308, 0.1707, 0.2688, 0.2290,\n",
       "                      0.2462, 0.1554, 0.2382, 0.2935, 0.2582, 0.8835, 1.7509, 0.4721, 0.3961,\n",
       "                      0.5313, 0.3087, 0.5552, 0.3378, 0.1534, 0.1712, 0.1754, 0.2407, 0.2433,\n",
       "                      0.2686, 0.3530, 0.2287, 0.1302, 0.1638, 0.1449, 0.1844, 0.2724, 0.1161,\n",
       "                      0.1478, 0.2969, 0.2062, 0.2206, 0.4413, 0.2505, 0.4544, 0.3407, 0.2556,\n",
       "                      0.1788, 0.1716, 0.2893, 0.2771, 0.3645, 0.1493, 0.3349, 0.2395, 0.8891,\n",
       "                      0.3528, 0.2042, 0.4375, 0.3392, 0.3069, 0.5654, 0.6977, 0.4424, 0.3368,\n",
       "                      0.2956, 0.5322, 0.3387, 0.2597, 0.2373, 0.3771, 0.1876, 0.3794, 0.2878,\n",
       "                      0.7697, 1.0719, 0.7621, 0.6070, 0.3883, 0.7455, 0.2596, 0.3555, 0.1437,\n",
       "                      0.2331, 0.6479, 0.5265, 0.2494, 0.3204, 0.2160, 0.5147, 0.3194, 0.3481,\n",
       "                      0.4299, 0.6432, 0.3109, 0.4616, 0.2534, 0.3341, 0.4512, 1.0757, 0.9024,\n",
       "                      0.1988, 0.3121, 0.7191, 0.3826, 0.4300, 0.1608, 0.6013, 0.3724, 0.4016,\n",
       "                      0.4825, 0.4610, 0.2666, 0.4085, 0.3648, 0.5749, 0.2461, 0.2526, 0.1266,\n",
       "                      0.1900, 0.1932, 0.2473, 0.1924, 0.1958, 0.2847, 0.1364, 0.3844, 0.3306,\n",
       "                      0.3922, 0.7992, 0.2399, 0.5791, 0.1455, 0.3835, 0.2976, 0.5531, 0.7189,\n",
       "                      0.2529, 0.1371, 0.9996, 0.4788, 0.4597, 0.3946, 0.2719, 0.3512, 0.3776,\n",
       "                      0.2779, 0.3674, 0.3429, 0.2846, 0.1776, 0.2842, 0.3001, 0.2140, 0.1614,\n",
       "                      0.3710, 0.4335, 0.2302, 0.2625, 0.4553, 0.2227, 0.1811, 0.2444, 0.1862,\n",
       "                      0.1730, 0.2807, 0.6794, 0.2137, 0.2291, 0.4321, 0.3982, 0.2409, 0.4390,\n",
       "                      0.2952, 0.3662, 0.3535, 0.2054, 0.3252, 0.6894, 0.3560, 0.2975, 0.2279,\n",
       "                      0.2145, 0.3857, 0.2712, 0.2682, 0.3525, 0.2477, 0.7957, 0.3142, 0.2193,\n",
       "                      0.1796, 0.2006, 0.1791, 0.5844, 0.4367, 0.2442, 0.2298, 0.1403, 0.2024],\n",
       "                     device='cuda:0')),\n",
       "             ('f.b_bn.num_batches_tracked', tensor(16497, device='cuda:0')),\n",
       "             ('f.se.f_ex.0.weight',\n",
       "              tensor([[[[-0.1149]],\n",
       "              \n",
       "                       [[ 0.2740]],\n",
       "              \n",
       "                       [[-0.0712]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0409]],\n",
       "              \n",
       "                       [[-0.0549]],\n",
       "              \n",
       "                       [[ 0.0276]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2485]],\n",
       "              \n",
       "                       [[ 0.1383]],\n",
       "              \n",
       "                       [[ 0.2554]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0961]],\n",
       "              \n",
       "                       [[-0.0612]],\n",
       "              \n",
       "                       [[ 0.0249]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1169]],\n",
       "              \n",
       "                       [[-0.0684]],\n",
       "              \n",
       "                       [[ 0.1871]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0390]],\n",
       "              \n",
       "                       [[ 0.3042]],\n",
       "              \n",
       "                       [[ 0.1261]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2745]],\n",
       "              \n",
       "                       [[-0.4760]],\n",
       "              \n",
       "                       [[-0.4598]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0015]],\n",
       "              \n",
       "                       [[ 0.4755]],\n",
       "              \n",
       "                       [[ 0.0518]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3310]],\n",
       "              \n",
       "                       [[ 0.2432]],\n",
       "              \n",
       "                       [[ 0.3230]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0973]],\n",
       "              \n",
       "                       [[ 0.3132]],\n",
       "              \n",
       "                       [[-0.5527]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1823]],\n",
       "              \n",
       "                       [[ 0.1618]],\n",
       "              \n",
       "                       [[ 0.0041]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0966]],\n",
       "              \n",
       "                       [[-0.1058]],\n",
       "              \n",
       "                       [[ 0.3021]]]], device='cuda:0')),\n",
       "             ('f.se.f_ex.0.bias',\n",
       "              tensor([-3.5955e-02, -2.0202e-02, -3.3354e-05, -3.2713e-02,  2.0213e-02,\n",
       "                       6.1909e-02,  4.1217e-02,  5.2737e-02, -1.4312e-02, -4.0524e-02,\n",
       "                      -4.8204e-02,  4.0876e-02, -6.4960e-02,  1.2623e-02,  2.1146e-02,\n",
       "                       6.7698e-03, -4.7065e-02,  1.3957e-02, -5.5531e-02,  8.7553e-03,\n",
       "                       2.4097e-02,  3.3009e-02, -5.2683e-02, -2.2155e-02, -9.6005e-03,\n",
       "                       9.8615e-03, -8.5297e-03,  9.0705e-03], device='cuda:0')),\n",
       "             ('f.se.f_ex.2.weight',\n",
       "              tensor([[[[ 0.0600]],\n",
       "              \n",
       "                       [[ 0.0656]],\n",
       "              \n",
       "                       [[ 0.1949]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.1010]],\n",
       "              \n",
       "                       [[ 0.0386]],\n",
       "              \n",
       "                       [[ 0.1070]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0724]],\n",
       "              \n",
       "                       [[-0.0235]],\n",
       "              \n",
       "                       [[-0.0335]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0696]],\n",
       "              \n",
       "                       [[ 0.0645]],\n",
       "              \n",
       "                       [[-0.0794]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1512]],\n",
       "              \n",
       "                       [[ 0.0602]],\n",
       "              \n",
       "                       [[-0.0077]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.1290]],\n",
       "              \n",
       "                       [[-0.1591]],\n",
       "              \n",
       "                       [[ 0.1166]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.1257]],\n",
       "              \n",
       "                       [[ 0.1210]],\n",
       "              \n",
       "                       [[ 0.0177]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0468]],\n",
       "              \n",
       "                       [[-0.0185]],\n",
       "              \n",
       "                       [[-0.0004]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0807]],\n",
       "              \n",
       "                       [[ 0.0263]],\n",
       "              \n",
       "                       [[-0.0487]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.1289]],\n",
       "              \n",
       "                       [[-0.0953]],\n",
       "              \n",
       "                       [[-0.0077]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1174]],\n",
       "              \n",
       "                       [[ 0.1095]],\n",
       "              \n",
       "                       [[-0.0096]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0241]],\n",
       "              \n",
       "                       [[-0.0218]],\n",
       "              \n",
       "                       [[ 0.0413]]]], device='cuda:0')),\n",
       "             ('f.se.f_ex.2.bias',\n",
       "              tensor([-0.1214, -0.1144,  0.0285, -0.0640, -0.0392, -0.2065, -0.0861,  0.0340,\n",
       "                      -0.0544,  0.1170,  0.0075, -0.1828,  0.1205,  0.0748, -0.0910, -0.1021,\n",
       "                       0.2054, -0.0303, -0.0413, -0.0509, -0.0476, -0.1253,  0.2850, -0.1101,\n",
       "                       0.1319, -0.1093,  0.0799, -0.1549, -0.0907, -0.0462, -0.0237,  0.0369,\n",
       "                      -0.1338, -0.0478,  0.0520,  0.1075,  0.0308,  0.2082, -0.1373, -0.0850,\n",
       "                       0.0811,  0.0615,  0.0605, -0.0348,  0.1103,  0.0416,  0.0560,  0.0440,\n",
       "                       0.1322, -0.0850,  0.0760,  0.0182,  0.0423,  0.0949,  0.0755,  0.0719,\n",
       "                       0.0044,  0.1862,  0.1279, -0.0741, -0.2467, -0.1818,  0.0497, -0.0846,\n",
       "                      -0.0010, -0.0164,  0.0239,  0.0329,  0.0924,  0.0029, -0.1037,  0.0877,\n",
       "                      -0.1750, -0.1600, -0.0524, -0.0891,  0.1765,  0.0045, -0.1309,  0.0281,\n",
       "                       0.1399,  0.1106, -0.2360, -0.0789, -0.1474, -0.1283, -0.0516, -0.0967,\n",
       "                       0.0992, -0.1714, -0.0793,  0.0388, -0.0244,  0.1167, -0.0685,  0.0497,\n",
       "                       0.0275, -0.2316, -0.1710,  0.0713,  0.0766,  0.0284,  0.0056, -0.0744,\n",
       "                      -0.1378,  0.0265,  0.2135,  0.0529, -0.2134,  0.2021, -0.1395,  0.1209,\n",
       "                      -0.0675,  0.2644,  0.1323,  0.2124, -0.0949, -0.0694, -0.0932, -0.0833,\n",
       "                      -0.1045, -0.1435, -0.1602,  0.0983,  0.0789,  0.0459,  0.1071,  0.2040,\n",
       "                      -0.1175,  0.0321,  0.1251,  0.0016,  0.0922, -0.0436, -0.1788, -0.0411,\n",
       "                       0.0519, -0.0088,  0.0541, -0.1374,  0.1145, -0.1519,  0.0165,  0.0156,\n",
       "                       0.0511, -0.0297, -0.0581, -0.1529,  0.0605,  0.1916, -0.0217,  0.1742,\n",
       "                       0.0997, -0.0027,  0.0123,  0.0841,  0.0438,  0.0222, -0.0040,  0.0549,\n",
       "                       0.1142,  0.1989,  0.2441, -0.0179,  0.0006,  0.0900,  0.1693,  0.2072,\n",
       "                       0.1098, -0.0763, -0.2947, -0.0348, -0.0088, -0.1369,  0.0738,  0.0130,\n",
       "                      -0.2748,  0.1902, -0.1064,  0.0306, -0.0631, -0.2502, -0.0629,  0.1216,\n",
       "                       0.1169, -0.1659, -0.1605,  0.0797, -0.1552, -0.2179, -0.0873,  0.0464,\n",
       "                      -0.0964,  0.0713,  0.0480, -0.0507, -0.0345, -0.2206, -0.0583, -0.1532,\n",
       "                       0.1270,  0.0976,  0.1058, -0.1766, -0.0004, -0.1628, -0.1246, -0.1802,\n",
       "                      -0.0126, -0.0419,  0.1014, -0.1076, -0.0470,  0.0261, -0.0235,  0.3010,\n",
       "                       0.0209,  0.0774, -0.0824,  0.1435, -0.1310, -0.1155,  0.0981, -0.1138,\n",
       "                      -0.1457,  0.0646, -0.1367, -0.0005, -0.0486, -0.2190,  0.1041, -0.1332,\n",
       "                      -0.2470, -0.2408,  0.0130,  0.0995,  0.0005,  0.1640, -0.0798,  0.0347,\n",
       "                      -0.0264, -0.0451,  0.0452, -0.0399, -0.0267,  0.0095,  0.0788, -0.1872,\n",
       "                      -0.1490, -0.1124,  0.0712,  0.1098, -0.1724,  0.1318,  0.0757,  0.0972,\n",
       "                      -0.1217,  0.0953, -0.1000,  0.0620,  0.2627, -0.1705,  0.0050, -0.1824,\n",
       "                       0.1566,  0.0856, -0.0221,  0.1061,  0.0048,  0.1128, -0.1035,  0.1399,\n",
       "                       0.1473,  0.0200,  0.2301, -0.0440, -0.0444,  0.1352, -0.1216,  0.1083,\n",
       "                       0.0533, -0.0807,  0.0702, -0.1174, -0.2207,  0.0253, -0.0143,  0.0484],\n",
       "                     device='cuda:0')),\n",
       "             ('f.c.weight',\n",
       "              tensor([[[[ 0.0760]],\n",
       "              \n",
       "                       [[ 0.0974]],\n",
       "              \n",
       "                       [[-0.1621]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0174]],\n",
       "              \n",
       "                       [[ 0.0661]],\n",
       "              \n",
       "                       [[-0.1065]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0520]],\n",
       "              \n",
       "                       [[-0.1277]],\n",
       "              \n",
       "                       [[-0.1872]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0702]],\n",
       "              \n",
       "                       [[-0.1006]],\n",
       "              \n",
       "                       [[-0.0391]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1074]],\n",
       "              \n",
       "                       [[ 0.1075]],\n",
       "              \n",
       "                       [[-0.0668]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0452]],\n",
       "              \n",
       "                       [[ 0.1219]],\n",
       "              \n",
       "                       [[-0.1028]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0412]],\n",
       "              \n",
       "                       [[-0.1640]],\n",
       "              \n",
       "                       [[ 0.0104]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0549]],\n",
       "              \n",
       "                       [[-0.1303]],\n",
       "              \n",
       "                       [[-0.1471]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0861]],\n",
       "              \n",
       "                       [[ 0.0538]],\n",
       "              \n",
       "                       [[-0.0029]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.1734]],\n",
       "              \n",
       "                       [[-0.0132]],\n",
       "              \n",
       "                       [[ 0.1367]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1065]],\n",
       "              \n",
       "                       [[-0.0681]],\n",
       "              \n",
       "                       [[ 0.1572]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0682]],\n",
       "              \n",
       "                       [[-0.0625]],\n",
       "              \n",
       "                       [[ 0.1112]]]], device='cuda:0')),\n",
       "             ('f.c_bn.weight',\n",
       "              tensor([0.8301, 0.7834, 0.8708, 0.7679, 0.7484, 0.8163, 0.8293, 0.8101, 0.9532,\n",
       "                      0.8705, 0.8502, 0.8811, 0.8162, 0.8106, 0.7410, 0.8243, 0.8157, 0.7693,\n",
       "                      0.8376, 0.8074, 0.9531, 0.7757, 0.7092, 0.8339, 0.7986, 0.8516, 0.8134,\n",
       "                      0.8940, 0.9200, 0.7888, 0.7324, 0.8176, 0.8155, 0.8275, 0.6831, 0.8344,\n",
       "                      0.7905, 0.7973, 0.7627, 0.7973, 0.7443, 0.7911, 0.7002, 0.6899, 0.7261,\n",
       "                      0.8313, 0.8134, 1.0171, 0.8605, 0.9063, 0.8217, 0.8238, 0.7934, 0.8430,\n",
       "                      0.8575, 0.7865, 0.7862, 0.9025, 0.9425, 0.7984, 0.7978, 0.7351, 0.8005,\n",
       "                      0.8118, 0.7940, 0.8742, 0.7895, 0.8409, 0.7485, 0.7383, 0.8150, 0.8337,\n",
       "                      0.6787, 0.8249, 0.7682, 0.8919, 0.7543, 0.8050, 0.7639, 0.8510, 0.9563,\n",
       "                      0.7356, 0.8628, 0.9003, 0.9033, 0.7641, 0.6846, 0.8561, 0.8018, 0.8239,\n",
       "                      0.7832, 0.8787, 0.7793, 0.8272, 0.7495, 0.8388, 0.7480, 0.8247, 0.7700,\n",
       "                      0.8381, 0.8126, 0.8072, 0.8247, 0.8943, 0.7740, 0.7500, 0.8255, 0.8070,\n",
       "                      0.8936, 0.8213, 0.8905, 0.6861, 0.8168, 0.9157, 0.7676, 0.8477, 0.7667,\n",
       "                      0.7775, 0.7595, 0.7590, 0.8445, 0.8685, 0.9026, 0.7401, 0.8461, 0.8403,\n",
       "                      0.8389, 0.7528, 0.8535, 0.8839, 0.7709, 0.8788, 0.8464, 0.8647, 0.8707,\n",
       "                      0.7803, 0.7281, 0.8377, 0.7875, 0.8473, 0.7404, 0.8631, 0.7908, 0.7779,\n",
       "                      0.7292, 0.8353, 0.8099, 0.7253, 0.8888, 0.7288, 0.8647, 0.7849, 0.9567,\n",
       "                      0.8634, 0.8595, 0.7962, 0.7972, 0.8365, 0.7745, 0.7844, 0.8808, 0.8280,\n",
       "                      0.8372, 0.6762, 0.8285, 0.8807, 0.8213, 0.7827, 0.7797, 0.7421, 0.8237,\n",
       "                      0.7167, 0.9179, 0.8555, 0.8619, 0.7519, 0.8240, 0.7249, 0.8485, 0.8078,\n",
       "                      0.8705, 0.8794, 0.8378, 0.7684, 0.7805, 1.0693, 0.8302, 0.9058, 0.7133,\n",
       "                      0.8663, 0.7105, 0.7884, 0.8873, 0.9068, 0.9134, 0.8473, 0.8332, 0.8835,\n",
       "                      0.7781, 0.8379, 0.7943, 0.8447, 0.8825, 0.8599, 0.9351, 0.8481, 0.8011,\n",
       "                      0.7238, 0.8273, 0.7691, 0.7286, 0.7253, 0.9279, 0.7158, 0.7864, 0.8279,\n",
       "                      0.7664, 0.7919, 0.8152, 0.9003, 0.8338, 0.8266, 0.7842, 0.8519, 0.8285,\n",
       "                      0.7239, 0.7827, 0.7119, 0.8262, 0.6864, 0.8273, 0.7444, 0.7992, 0.8493,\n",
       "                      0.8419, 0.7778, 0.8627, 0.7869, 0.9387, 0.8212, 0.7939, 0.8172, 0.7736,\n",
       "                      0.8490, 0.6957, 0.9037, 0.8236, 0.9185, 0.7124, 0.6985, 0.7510, 0.9090,\n",
       "                      0.7898, 0.7703, 0.8873, 0.8330, 0.8224, 0.8333, 0.8264, 0.7622, 0.7826,\n",
       "                      0.7520, 0.9253, 0.7912, 0.8706, 0.7939, 0.7394, 0.8482, 0.7699, 0.7852,\n",
       "                      0.8374, 0.9175, 0.9728, 0.9134, 0.8425, 0.7825, 0.8416, 0.7296, 1.0087,\n",
       "                      0.7843, 0.8287, 0.7153, 0.8090, 0.7789, 0.6978, 0.7448, 0.7227, 0.8147],\n",
       "                     device='cuda:0')),\n",
       "             ('f.c_bn.bias',\n",
       "              tensor([-1.1343e-01, -2.3066e-01, -1.0725e-01, -1.4998e-01, -1.3801e-01,\n",
       "                      -6.0928e-02, -3.0279e-02, -1.2117e-01, -9.1940e-02, -9.7427e-02,\n",
       "                      -1.0881e-01, -9.1320e-02, -5.5861e-02, -9.1187e-02, -1.9451e-01,\n",
       "                      -9.3701e-02, -5.6753e-02, -1.6434e-01, -1.5808e-01, -4.0370e-02,\n",
       "                      -1.7413e-03, -1.1232e-02, -6.4017e-02, -1.7807e-01, -1.0050e-01,\n",
       "                       2.6264e-02, -8.6619e-02, -1.0381e-01, -5.4459e-02, -3.6503e-02,\n",
       "                      -1.8050e-01,  5.6517e-02, -2.3363e-04, -6.1672e-02, -1.8950e-01,\n",
       "                      -1.3272e-01, -1.5088e-01, -1.1754e-02, -1.7520e-01, -1.0326e-01,\n",
       "                      -1.5742e-01, -5.6900e-02, -1.8602e-01, -1.9838e-01, -1.7508e-01,\n",
       "                      -1.3405e-01, -7.5414e-02, -1.1272e-01, -7.6561e-02, -9.0361e-03,\n",
       "                      -7.0531e-02, -1.8685e-01, -2.1740e-01, -2.3863e-02, -7.8424e-02,\n",
       "                      -1.9713e-01, -1.2048e-01, -1.4795e-02, -1.0070e-01, -9.3753e-02,\n",
       "                      -1.2398e-01, -8.0067e-02, -1.4659e-01, -1.0843e-01, -9.0906e-02,\n",
       "                      -7.1848e-02, -6.5761e-02, -1.5782e-01, -1.1998e-01, -8.6299e-02,\n",
       "                      -6.1572e-02, -7.4693e-02, -1.9508e-01, -1.4552e-02, -1.6928e-01,\n",
       "                      -1.3409e-01, -1.4192e-01, -8.1417e-02, -6.9481e-02, -3.7872e-02,\n",
       "                      -1.7170e-01, -1.2110e-01, -1.3339e-01, -2.3014e-02, -3.8072e-02,\n",
       "                      -1.3849e-02, -1.6944e-01, -1.1756e-01, -6.1568e-02, -1.4301e-01,\n",
       "                      -8.6634e-02, -3.0292e-02, -1.0099e-01, -4.0699e-02, -1.0310e-01,\n",
       "                      -6.0011e-02, -4.5711e-02, -1.0065e-01, -1.0278e-01, -1.4604e-01,\n",
       "                      -1.7023e-01, -1.6576e-01, -4.1184e-02, -4.7718e-02, -4.3929e-02,\n",
       "                      -1.1594e-01, -1.4064e-01, -3.3646e-02, -1.2662e-01, -1.0159e-01,\n",
       "                      -3.6797e-02, -1.8809e-01, -1.2847e-01, -3.7208e-02, -8.2752e-02,\n",
       "                      -1.3660e-01, -9.1269e-02, -8.5834e-02, -1.7469e-01, -5.9764e-02,\n",
       "                      -1.9688e-01, -1.1163e-01, -1.0302e-01, -9.3448e-02, -1.2746e-01,\n",
       "                      -7.6673e-02, -7.5044e-02, -1.2415e-01, -6.8566e-02, -1.0595e-02,\n",
       "                      -8.1820e-02, -5.7159e-02, -1.2932e-01, -1.5176e-01, -1.5378e-01,\n",
       "                      -1.0193e-01, -3.1814e-03,  1.0722e-02,  1.9018e-03, -1.3662e-01,\n",
       "                      -1.5744e-01, -1.5251e-01, -1.2020e-01, -7.1552e-02, -1.1812e-01,\n",
       "                      -6.8432e-02, -9.4883e-02,  1.9372e-03,  3.2066e-02, -1.5572e-01,\n",
       "                      -2.5390e-01, -1.6760e-01, -9.5109e-02, -6.4075e-02, -5.8026e-02,\n",
       "                      -1.3359e-01, -8.8108e-02, -5.1949e-02, -1.4024e-01, -1.2653e-01,\n",
       "                      -5.7308e-02, -1.1546e-01, -1.8073e-01, -1.9166e-01, -1.4119e-01,\n",
       "                      -6.8780e-02, -4.8603e-02, -1.8880e-01, -4.4960e-02,  1.1872e-01,\n",
       "                      -2.1194e-01, -1.6768e-01, -1.8481e-01, -4.8295e-02, -1.1187e-01,\n",
       "                      -4.8673e-02, -8.1877e-02, -2.0409e-01,  2.1547e-02, -8.5556e-02,\n",
       "                      -1.3984e-01, -1.1850e-01, -1.5967e-01, -1.5818e-01, -8.7549e-02,\n",
       "                       1.0070e-02, -6.1154e-02, -7.1271e-02,  8.1441e-02,  3.0676e-02,\n",
       "                      -2.0313e-01, -5.0725e-02, -1.0310e-01, -1.9090e-01, -1.1960e-01,\n",
       "                      -1.2381e-01, -1.4890e-01, -1.1175e-01,  4.9358e-02, -1.4189e-01,\n",
       "                       4.6896e-02, -1.0256e-01, -1.0317e-01, -7.8951e-03, -1.4079e-01,\n",
       "                      -2.9885e-01, -8.9663e-02, -5.6989e-02, -1.7977e-02, -5.9872e-02,\n",
       "                      -1.7170e-01, -2.0422e-01, -1.5169e-01, -2.0470e-01, -8.1564e-02,\n",
       "                      -1.1339e-01, -1.2257e-01, -1.2478e-01, -1.0978e-01, -1.1109e-01,\n",
       "                      -1.2881e-02, -1.2792e-01, -3.8526e-02, -7.2898e-02, -1.1539e-01,\n",
       "                      -1.4992e-01, -1.2188e-01, -1.6935e-01, -6.2938e-02, -7.7030e-02,\n",
       "                      -9.9138e-02, -1.7226e-01, -8.2611e-02, -1.1928e-01, -7.5184e-02,\n",
       "                      -1.7261e-01, -3.0643e-02, -1.2363e-01, -1.2704e-01,  4.0170e-02,\n",
       "                      -1.7029e-01, -1.3205e-01, -1.2177e-01, -1.6386e-02, -2.0472e-01,\n",
       "                      -1.1314e-01, -5.8256e-02, -3.8358e-02, -1.9970e-01, -1.6544e-01,\n",
       "                      -1.5304e-01, -8.4701e-02, -1.5379e-01, -5.1489e-02, -3.3584e-02,\n",
       "                      -1.0026e-01, -1.1849e-01,  2.8030e-03, -1.0319e-01, -2.0414e-01,\n",
       "                      -4.3780e-03, -1.9297e-01, -1.0007e-01, -1.1571e-01, -1.8916e-01,\n",
       "                       4.3304e-02, -1.4564e-01, -9.1084e-02, -1.7360e-01, -1.3348e-01,\n",
       "                      -1.4833e-01, -7.9423e-02, -5.0037e-02, -4.0430e-02, -1.2157e-01,\n",
       "                       9.0689e-03, -1.0146e-01, -9.5212e-02,  6.0165e-02, -1.2238e-01,\n",
       "                      -1.3215e-01, -2.0411e-01, -4.6212e-02, -7.6107e-02, -1.7417e-01,\n",
       "                      -6.4369e-02, -1.6472e-01,  2.1136e-02], device='cuda:0')),\n",
       "             ('f.c_bn.running_mean',\n",
       "              tensor([ 0.3772, -0.5836, -0.3678, -0.2145, -0.0753, -0.0517, -0.7803, -0.7569,\n",
       "                       0.4078, -1.1151, -1.0540, -1.0852,  0.5847, -0.1402, -0.3031,  0.4829,\n",
       "                      -0.2169,  0.1529, -0.1045, -0.4512, -0.5551,  0.3466, -0.8922,  0.4981,\n",
       "                       0.2913, -1.2149,  0.0454, -0.5617, -0.1959, -1.2105, -0.5878, -0.6825,\n",
       "                      -0.6210, -0.3715, -1.3872, -0.4050,  0.1846, -1.1588, -0.4182, -0.7067,\n",
       "                       0.1360, -0.4940, -0.1209, -0.4762, -0.7705,  0.3541,  0.3049, -0.4652,\n",
       "                      -0.0322, -0.7015, -0.6604,  0.0116, -0.2515, -0.6881, -0.1969,  0.0379,\n",
       "                      -0.0291,  0.1932,  0.4819, -0.4030,  0.3228, -0.0692, -0.6610, -0.3475,\n",
       "                      -0.7062, -0.5589, -1.2235,  0.1739,  0.0555, -0.4930,  0.5118, -0.4036,\n",
       "                      -0.6913, -0.0168,  0.1772,  0.1171, -0.7728, -0.7598,  0.3534, -0.5214,\n",
       "                       0.0782, -0.3946,  0.6363,  0.0146, -1.6778,  0.1463, -0.9371, -0.2902,\n",
       "                      -0.2364,  1.1051, -0.5976,  0.2474, -0.6816, -1.0185,  0.6982, -0.2375,\n",
       "                      -0.3388,  0.3263, -0.3000, -0.5803,  0.4770, -0.0519, -1.0081, -0.7010,\n",
       "                      -0.7620, -0.0918, -0.5485,  0.4263,  0.2781, -0.0432, -0.4386, -0.5956,\n",
       "                       0.0034, -0.0935, -0.4676, -0.4281, -0.3915, -1.0303,  0.7578, -1.3567,\n",
       "                       0.0981, -0.7216, -0.0758,  0.1683, -0.3989, -0.7497, -0.2077,  0.3024,\n",
       "                      -0.4898,  0.2530, -0.0952,  0.1063, -0.3630,  0.3793,  0.1237,  0.1474,\n",
       "                      -1.4622, -0.2847, -0.3502,  0.1986,  0.1462, -0.2332,  0.3399, -0.6610,\n",
       "                      -0.4974,  0.1878, -0.8364, -0.7240, -1.4118, -1.2890,  0.1891,  0.2750,\n",
       "                       0.4707, -0.8458, -0.4235,  0.2947,  0.2218,  0.7603, -0.2317, -0.6148,\n",
       "                      -0.6288, -0.5798, -0.7793, -0.0539, -0.1545, -0.4151,  0.2767, -0.0798,\n",
       "                      -1.3121, -0.4503,  0.0810, -1.0151, -0.0282, -0.1161, -0.0928, -0.6262,\n",
       "                      -0.7841,  1.1251, -0.9925, -0.5722, -1.0546,  0.2567,  0.1555, -0.3135,\n",
       "                       0.4033, -0.3485, -1.3805, -0.8581, -0.5917, -1.1477, -0.6924,  0.6463,\n",
       "                      -0.1568, -0.6382, -0.0658,  0.8349, -1.2704,  0.4770, -1.2811, -0.5160,\n",
       "                      -1.1070,  0.9247,  0.0856, -0.6833, -0.4479, -0.8030, -1.0006, -0.5654,\n",
       "                      -1.2573, -0.4059, -0.2763,  0.0114, -0.1761, -0.2678, -0.2096, -0.5644,\n",
       "                       0.1682, -0.4327, -0.7921, -0.2227, -1.0422, -0.0377,  0.1070, -0.5331,\n",
       "                       0.0373, -0.5037, -0.8775,  0.9445, -0.8771, -0.7754, -0.6453, -0.9327,\n",
       "                       0.0198, -0.5106, -1.3151,  0.7063, -0.0896, -0.1763, -0.4278, -0.0333,\n",
       "                       0.3856, -0.7331, -0.5174, -0.6348,  0.4935, -1.1966,  0.0710,  0.0092,\n",
       "                       0.1125,  0.5886, -0.1395,  0.8149, -0.0665,  0.5608, -0.2184,  0.1227,\n",
       "                      -0.7033,  0.8748,  0.5182, -0.4667, -0.8117, -0.0722, -1.1882, -0.5554,\n",
       "                      -0.1916, -0.4650, -0.7359, -1.3434, -0.6796, -1.4087, -0.4637, -0.5501,\n",
       "                      -0.2140, -0.3825, -0.2983, -0.7129, -0.2889, -0.5982, -0.4709, -1.0605,\n",
       "                      -0.2864, -1.1157, -0.1407, -0.5114,  0.1414, -0.5591,  0.2538, -0.9942],\n",
       "                     device='cuda:0')),\n",
       "             ('f.c_bn.running_var',\n",
       "              tensor([0.7775, 2.1370, 0.9142, 1.2265, 0.8637, 1.1693, 1.5510, 1.7163, 1.6228,\n",
       "                      2.3778, 1.0301, 1.0995, 2.9180, 1.1065, 1.9015, 1.5588, 1.7095, 1.0702,\n",
       "                      1.7739, 2.0674, 0.7025, 1.2349, 1.2825, 0.7097, 1.1174, 1.6530, 1.8043,\n",
       "                      0.7667, 2.8894, 1.9454, 2.2756, 0.9639, 1.6675, 1.3888, 2.5962, 1.8736,\n",
       "                      1.7846, 1.4015, 2.0234, 0.7956, 0.9243, 1.3145, 1.4404, 3.2474, 3.3044,\n",
       "                      1.4162, 0.8924, 1.5275, 1.5691, 0.7518, 2.3155, 1.1537, 1.0110, 1.0862,\n",
       "                      1.2174, 1.0120, 1.8531, 1.4202, 0.7587, 1.2793, 0.8384, 0.9487, 1.8118,\n",
       "                      1.2140, 2.0274, 1.0359, 3.3724, 0.8519, 1.5406, 0.9298, 1.3811, 0.9877,\n",
       "                      0.7605, 1.7856, 1.2091, 1.4775, 1.1648, 1.9198, 1.5583, 1.3687, 0.9060,\n",
       "                      0.9786, 1.3101, 1.3167, 1.0479, 1.6071, 2.8060, 1.2529, 2.0460, 2.2482,\n",
       "                      0.8102, 1.0166, 1.0865, 2.3252, 1.1429, 1.0994, 1.3460, 1.6074, 1.6219,\n",
       "                      1.9894, 1.7793, 0.9392, 2.5554, 1.5688, 0.9623, 2.5847, 3.1858, 0.8406,\n",
       "                      0.9801, 1.3503, 1.1260, 2.1037, 0.9738, 1.3152, 2.5134, 1.5311, 2.2763,\n",
       "                      1.7716, 1.2745, 2.7814, 0.6648, 2.8859, 1.0386, 2.9932, 1.1346, 1.7601,\n",
       "                      2.1386, 1.0334, 2.1678, 0.7732, 2.5605, 0.9713, 0.8097, 0.7247, 0.6991,\n",
       "                      2.4637, 0.8750, 1.7041, 0.8394, 1.3554, 0.7314, 0.6203, 1.7888, 0.8737,\n",
       "                      1.8663, 0.7148, 3.4246, 1.0437, 0.6783, 2.4291, 0.6112, 1.1511, 0.8160,\n",
       "                      2.7059, 1.2807, 2.7964, 0.9751, 1.4838, 0.9970, 1.7842, 0.8158, 1.2977,\n",
       "                      1.7131, 1.6037, 1.2689, 1.2143, 1.2091, 1.3081, 1.3110, 1.0704, 0.9430,\n",
       "                      3.0612, 1.7236, 0.9399, 2.4532, 1.9156, 1.5610, 1.0052, 2.1583, 3.0068,\n",
       "                      1.3695, 1.1458, 1.2875, 0.9757, 0.8580, 0.7850, 2.5822, 1.4971, 0.9371,\n",
       "                      2.0210, 2.1014, 1.4217, 0.7569, 0.5327, 1.0196, 1.2372, 1.9101, 1.1301,\n",
       "                      0.8182, 2.1642, 2.1431, 2.2411, 0.9227, 1.6718, 1.0652, 0.9428, 2.1029,\n",
       "                      1.1692, 2.1374, 2.3223, 0.9122, 1.2316, 1.1452, 0.8104, 1.3370, 0.7490,\n",
       "                      1.7962, 1.4158, 1.7801, 1.0355, 1.5710, 0.8561, 1.8345, 2.9056, 1.6423,\n",
       "                      1.6704, 1.5742, 0.9043, 1.8535, 2.3340, 1.7796, 1.4621, 1.4264, 2.2093,\n",
       "                      1.2145, 1.0629, 1.3035, 2.1500, 1.2062, 0.9326, 0.6966, 1.6090, 1.2093,\n",
       "                      1.7448, 3.5778, 2.4998, 0.9546, 0.6153, 0.6516, 2.3031, 1.5277, 0.8547,\n",
       "                      0.8276, 2.8215, 1.5626, 1.0225, 1.3942, 3.5341, 0.5855, 1.6219, 1.2609,\n",
       "                      1.6194, 1.2240, 1.4687, 1.5980, 2.2699, 1.3560, 2.4756, 1.2430, 3.0062,\n",
       "                      0.6564, 0.9638, 1.7768, 1.6723, 1.4722, 2.5748, 0.9386, 1.5735, 0.7760,\n",
       "                      3.1689, 0.7395, 2.4251, 1.6474, 0.8533, 3.4388, 1.7796, 1.2652, 1.5111],\n",
       "                     device='cuda:0')),\n",
       "             ('f.c_bn.num_batches_tracked', tensor(16497, device='cuda:0')),\n",
       "             ('downsample.1.conv.weight',\n",
       "              tensor([[[[-0.1544]],\n",
       "              \n",
       "                       [[-0.0198]],\n",
       "              \n",
       "                       [[-0.0257]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.1474]],\n",
       "              \n",
       "                       [[-0.0688]],\n",
       "              \n",
       "                       [[-0.1865]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1245]],\n",
       "              \n",
       "                       [[-0.0906]],\n",
       "              \n",
       "                       [[-0.0328]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0615]],\n",
       "              \n",
       "                       [[ 0.0911]],\n",
       "              \n",
       "                       [[-0.0519]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1589]],\n",
       "              \n",
       "                       [[-0.0429]],\n",
       "              \n",
       "                       [[ 0.0087]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.2250]],\n",
       "              \n",
       "                       [[ 0.0671]],\n",
       "              \n",
       "                       [[-0.1527]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0227]],\n",
       "              \n",
       "                       [[-0.0107]],\n",
       "              \n",
       "                       [[-0.0273]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0564]],\n",
       "              \n",
       "                       [[-0.0509]],\n",
       "              \n",
       "                       [[-0.0794]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0489]],\n",
       "              \n",
       "                       [[ 0.0507]],\n",
       "              \n",
       "                       [[-0.0860]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0078]],\n",
       "              \n",
       "                       [[ 0.0134]],\n",
       "              \n",
       "                       [[ 0.0605]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0389]],\n",
       "              \n",
       "                       [[-0.1319]],\n",
       "              \n",
       "                       [[-0.0734]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0811]],\n",
       "              \n",
       "                       [[-0.0102]],\n",
       "              \n",
       "                       [[-0.0200]]]], device='cuda:0')),\n",
       "             ('downsample.1.bn.weight',\n",
       "              tensor([0.8981, 0.8235, 0.8411, 1.0170, 0.8699, 0.9258, 0.9512, 0.9541, 0.8404,\n",
       "                      0.8544, 0.9791, 1.0079, 0.9530, 0.9436, 0.8992, 1.0205, 0.9193, 0.8763,\n",
       "                      0.8363, 0.8702, 0.8143, 0.9148, 0.8921, 0.9097, 0.8747, 0.9576, 0.9844,\n",
       "                      0.8246, 0.9698, 0.9703, 0.8731, 1.0302, 0.8375, 0.8212, 0.7781, 0.8949,\n",
       "                      1.0024, 0.9343, 0.9982, 0.8686, 0.8969, 0.9653, 0.8376, 0.8992, 0.7962,\n",
       "                      0.8918, 0.9178, 0.9636, 0.8170, 0.9234, 0.9931, 1.0064, 0.8478, 0.9220,\n",
       "                      0.9641, 0.8330, 0.8186, 0.8734, 0.8823, 0.9412, 0.9858, 1.0209, 0.8481,\n",
       "                      0.9750, 0.8549, 0.9578, 0.9967, 1.0461, 0.8434, 0.9080, 0.9019, 0.9079,\n",
       "                      0.8589, 1.0176, 0.9656, 0.8903, 0.8528, 1.0040, 1.0205, 0.9098, 0.9043,\n",
       "                      1.0606, 0.8798, 0.8636, 0.9560, 0.9274, 0.8362, 0.8800, 0.9246, 0.9862,\n",
       "                      0.9921, 0.9781, 0.9590, 1.0479, 0.8283, 0.8350, 0.9616, 0.9063, 0.8187,\n",
       "                      0.8946, 0.9353, 0.9189, 0.9271, 0.9524, 0.8485, 0.9525, 0.8644, 0.9230,\n",
       "                      0.8455, 0.9678, 0.8439, 0.7878, 1.0034, 0.9401, 0.9253, 0.9581, 0.8919,\n",
       "                      0.8480, 0.9874, 0.8944, 0.8128, 0.8779, 0.8735, 1.0448, 0.8590, 0.9297,\n",
       "                      0.9531, 0.9972, 0.8993, 0.7874, 0.8928, 0.9579, 0.9304, 0.8499, 0.9429,\n",
       "                      0.9612, 1.0431, 0.8328, 0.9729, 1.0854, 0.9207, 0.8779, 0.9534, 0.8771,\n",
       "                      0.8733, 0.9516, 0.9410, 0.9448, 0.9048, 0.7749, 0.8819, 0.9412, 0.8542,\n",
       "                      1.0201, 1.0080, 0.9036, 1.0034, 0.9367, 0.7966, 0.9965, 0.9529, 0.9286,\n",
       "                      0.8168, 0.8289, 0.8281, 1.0761, 0.9554, 0.9217, 0.9985, 0.9510, 0.9305,\n",
       "                      0.7667, 0.8987, 0.9358, 0.9791, 0.9798, 0.8487, 0.8532, 0.8372, 0.9861,\n",
       "                      0.8701, 0.8854, 0.8035, 0.8196, 0.9470, 0.9601, 1.0199, 0.9176, 1.0200,\n",
       "                      0.8937, 0.8021, 0.9292, 0.9574, 0.8842, 0.9223, 0.9711, 0.8982, 0.8251,\n",
       "                      0.9228, 0.8639, 0.9490, 1.0652, 0.9335, 0.9076, 0.9587, 0.7860, 0.8915,\n",
       "                      0.8866, 0.9679, 0.9630, 0.8574, 0.9552, 0.7681, 0.8309, 0.9790, 0.9837,\n",
       "                      0.8391, 0.8648, 0.9515, 0.9075, 0.9558, 0.9394, 0.9661, 0.9656, 0.8734,\n",
       "                      0.8328, 0.8337, 0.9509, 0.8558, 0.7790, 0.8383, 0.7219, 0.8753, 0.8042,\n",
       "                      0.9522, 0.8954, 0.9261, 0.9043, 0.8267, 0.8982, 0.8677, 0.9158, 0.9284,\n",
       "                      0.8330, 0.9305, 0.7476, 0.9502, 0.9343, 0.8955, 0.8748, 0.9671, 0.9199,\n",
       "                      0.9174, 0.9455, 0.9071, 0.9093, 0.9637, 0.9248, 0.9475, 0.7696, 0.9876,\n",
       "                      0.9917, 0.9297, 1.0041, 0.7586, 0.9687, 0.7915, 0.9193, 0.8449, 0.9188,\n",
       "                      0.9937, 0.9235, 1.0085, 0.8672, 0.7768, 0.9695, 0.7944, 0.8866, 0.8357,\n",
       "                      0.8135, 0.8493, 0.8663, 1.0004, 0.8684, 0.8470, 0.9859, 0.8539, 0.9583],\n",
       "                     device='cuda:0')),\n",
       "             ('downsample.1.bn.bias',\n",
       "              tensor([-6.9699e-02, -2.4424e-01, -8.2578e-02, -7.4972e-02, -1.0733e-01,\n",
       "                       5.2976e-03, -6.6481e-03, -2.7897e-02, -4.4730e-02, -5.1320e-02,\n",
       "                      -9.7838e-02, -1.7090e-02, -2.2991e-02, -7.4220e-02, -9.4174e-02,\n",
       "                      -5.9951e-03, -4.8451e-02, -1.3443e-01, -1.5922e-01, -4.0329e-02,\n",
       "                      -1.2165e-02,  1.9215e-02,  2.5288e-03, -1.4760e-01, -3.2370e-02,\n",
       "                       4.8184e-02,  1.7745e-02, -8.3410e-02, -1.0807e-02, -2.8175e-03,\n",
       "                      -1.7967e-01,  8.8201e-02, -5.2597e-03, -5.1701e-02, -1.5560e-01,\n",
       "                      -8.7951e-02, -1.0883e-01, -2.3147e-02, -1.2190e-01, -7.6546e-02,\n",
       "                      -9.8423e-02, -1.6387e-04, -7.4424e-02, -1.3858e-01, -1.4669e-01,\n",
       "                      -1.3038e-01, -1.0071e-01, -3.6373e-02, -4.7910e-02,  5.5958e-02,\n",
       "                      -1.5549e-03, -1.1845e-01, -1.4009e-01,  3.1290e-02, -9.2442e-02,\n",
       "                      -2.1177e-01, -9.3402e-02,  8.4189e-02, -4.3771e-02, -6.5197e-02,\n",
       "                      -8.2332e-02, -3.1055e-02, -1.2312e-01, -7.3267e-02, -6.6087e-02,\n",
       "                      -1.9294e-02, -4.9816e-02, -1.0210e-01, -9.9617e-02, -6.9798e-02,\n",
       "                      -1.4125e-02, -2.0664e-02, -1.6719e-01, -1.0133e-02, -5.9090e-02,\n",
       "                      -2.3582e-02, -7.5654e-02, -2.0707e-02, -3.9235e-02, -8.2959e-03,\n",
       "                      -1.2227e-01, -7.5370e-03, -9.9413e-02,  2.8990e-02, -1.0475e-01,\n",
       "                      -1.3343e-02, -1.3286e-01, -1.2231e-01, -9.1863e-02, -5.8317e-02,\n",
       "                      -1.0878e-01,  1.7166e-02, -1.0150e-02,  3.8978e-02, -8.8603e-02,\n",
       "                      -6.7050e-02,  2.1375e-02, -6.6521e-02, -8.1246e-02, -1.0281e-01,\n",
       "                      -1.0991e-01, -3.2852e-02, -2.9549e-02, -4.9432e-03, -3.8676e-02,\n",
       "                      -4.9799e-02, -1.5467e-01, -3.4933e-02, -1.2092e-01, -3.4203e-02,\n",
       "                      -6.1957e-02, -1.6996e-01, -8.6354e-02,  3.2033e-02, -6.3452e-02,\n",
       "                      -7.2521e-02, -1.2447e-01, -5.7967e-02, -1.8998e-01, -5.9358e-02,\n",
       "                      -1.5399e-01, -8.4798e-02, -5.2039e-02,  5.3778e-02, -6.5614e-02,\n",
       "                       1.3116e-02, -6.6193e-02, -7.5165e-02, -2.3833e-02, -5.8757e-02,\n",
       "                      -7.2691e-02, -4.4648e-04, -1.1618e-01, -1.4307e-01, -9.8935e-02,\n",
       "                      -4.7400e-02, -8.9823e-03, -3.6048e-02,  1.6101e-02, -8.3867e-02,\n",
       "                      -2.0875e-01, -1.4609e-01, -5.8683e-02, -8.9690e-02, -9.7445e-02,\n",
       "                      -3.7206e-02, -3.8889e-02,  2.3885e-02,  6.9768e-03, -1.5185e-01,\n",
       "                      -2.4844e-01, -1.8261e-01, -1.1109e-01,  3.7521e-02,  2.8614e-02,\n",
       "                      -4.1642e-02, -9.1319e-02, -5.6196e-02, -1.4721e-01, -5.3236e-02,\n",
       "                      -1.7322e-02, -1.0065e-01, -1.4535e-01, -1.1985e-01, -1.2093e-01,\n",
       "                       3.6250e-02, -3.2577e-03, -1.0754e-01, -3.6062e-02,  1.1270e-01,\n",
       "                      -1.9799e-01, -1.5782e-01, -1.2725e-01,  3.6416e-02, -4.7681e-02,\n",
       "                      -6.2050e-02, -1.1730e-01, -1.4879e-01, -9.8116e-03, -4.4691e-02,\n",
       "                      -8.0682e-02, -7.6342e-02, -1.8879e-01, -1.4944e-01, -1.1489e-02,\n",
       "                       5.2550e-02,  2.1398e-02, -7.3502e-02,  1.5971e-01,  1.5219e-02,\n",
       "                      -2.1341e-01, -5.4333e-02, -4.9835e-02, -1.1830e-01, -4.9608e-02,\n",
       "                      -1.3973e-01, -1.6740e-01, -1.0536e-01,  8.3972e-02, -9.9948e-02,\n",
       "                       1.2163e-01, -1.5661e-02, -1.6623e-02,  2.2241e-02, -4.9682e-02,\n",
       "                      -3.0444e-01, -7.1104e-02, -3.7479e-02,  1.8665e-02, -3.6957e-02,\n",
       "                      -1.2254e-01, -2.2702e-01, -1.3212e-01, -1.5855e-01, -4.1492e-02,\n",
       "                      -1.2948e-02, -1.1829e-01, -1.1780e-01, -2.5378e-02, -3.2922e-02,\n",
       "                      -1.3872e-03, -1.3659e-01,  1.0900e-01,  2.0612e-02, -1.8026e-01,\n",
       "                      -9.8708e-02, -1.3112e-01, -1.7359e-01, -7.2220e-02, -5.9272e-02,\n",
       "                      -1.6705e-01, -1.8880e-01, -6.5212e-02, -1.5186e-01, -9.0693e-04,\n",
       "                      -6.1339e-02, -5.0048e-02, -8.2350e-02, -8.4398e-02,  4.3270e-02,\n",
       "                      -1.6694e-01, -1.1174e-01, -1.0971e-01, -3.5923e-02, -9.4359e-02,\n",
       "                      -1.6487e-01, -7.2517e-02, -2.9432e-02, -1.9032e-01, -1.3143e-01,\n",
       "                      -9.0344e-02, -7.1224e-02, -7.1796e-02,  5.0078e-02, -4.6778e-02,\n",
       "                      -7.2890e-02, -4.5046e-02,  2.6413e-02, -8.6210e-02, -1.9790e-01,\n",
       "                       3.5619e-03, -1.1104e-01, -6.0467e-02,  5.2128e-04, -1.3035e-01,\n",
       "                       9.0847e-02, -9.3723e-02, -5.8470e-02, -1.6357e-01, -1.2081e-01,\n",
       "                      -9.2111e-02, -9.0982e-02,  2.7813e-02, -1.8594e-02, -1.1487e-01,\n",
       "                      -1.6160e-02, -1.3745e-01, -2.3222e-02,  8.2433e-02, -1.9375e-01,\n",
       "                      -8.4949e-02, -1.3936e-01,  5.0256e-02, -4.3389e-02, -1.3633e-01,\n",
       "                       8.8591e-03, -1.2000e-01,  7.8399e-04], device='cuda:0')),\n",
       "             ('downsample.1.bn.running_mean',\n",
       "              tensor([-2.5549, -0.3990, -0.1299, -2.1016,  1.3242,  1.1850,  0.2019, -0.3652,\n",
       "                      -3.7601, -0.6739, -0.1253, -0.4724, -0.5566, -0.3749, -1.4851,  0.2702,\n",
       "                       0.7863, -3.5138, -1.7515,  1.3546, -2.0297, -2.2520,  1.7419, -0.6549,\n",
       "                      -1.8615,  1.6562,  0.1850, -2.8365, -0.0217, -0.1851, -0.2505,  2.0219,\n",
       "                       0.5974, -1.5999, -0.7142, -2.2254, -1.0715, -0.9941,  0.6549, -1.2717,\n",
       "                       1.8092,  2.2019,  2.0298,  3.0418, -1.7362, -3.0521, -2.0132,  0.7337,\n",
       "                      -2.1017, -2.0438, -0.7204, -1.7918, -1.8964, -0.6482, -2.1528, -3.8064,\n",
       "                      -2.4225, -2.5636,  0.1815, -2.0199, -2.7352, -4.0365, -1.5489, -1.6936,\n",
       "                      -2.5569, -0.5774,  0.3567, -2.2090, -0.2304,  0.6544, -0.9371, -2.4804,\n",
       "                      -1.0257, -1.5920, -1.4622, -1.4049,  1.1163, -1.8663, -4.2450, -1.4100,\n",
       "                      -2.3349, -1.6574, -4.1444, -1.5909, -1.4983,  0.7998,  3.4908, -1.0058,\n",
       "                       0.8667,  0.8345, -4.7217, -2.1186, -2.6282, -0.6143, -3.0401, -2.9495,\n",
       "                       0.7253, -1.1541, -0.2867, -0.1774, -2.2079,  0.2678, -0.3224, -0.9132,\n",
       "                      -1.1986, -0.9218, -0.5519, -0.2210, -2.6360, -0.6570, -1.2324, -0.4359,\n",
       "                       2.4893, -1.1767,  1.6210,  0.0888, -1.3907, -0.5532, -2.5835, -1.3810,\n",
       "                       0.2304, -0.1070, -2.6643,  0.3989, -2.2103,  1.5390,  0.4435, -1.7298,\n",
       "                      -0.0855, -2.8852,  2.5332, -1.4325, -3.0807, -1.2125, -2.2075, -0.5903,\n",
       "                      -0.5367,  3.7981,  1.8784, -1.3633, -1.5969, -3.0960,  1.6524,  1.1327,\n",
       "                       3.0819, -1.9212,  0.3912,  0.2963, -0.1287, -2.9129, -3.8441, -2.8559,\n",
       "                      -2.3230, -1.5323, -0.3071,  0.1780, -2.8881, -3.7489, -2.4171, -1.1720,\n",
       "                       0.4685, -3.7575, -2.9943,  0.1536,  2.9347, -1.1727, -1.9387, -0.4506,\n",
       "                       0.3322,  1.4771, -2.6132, -1.1814,  0.7010, -1.4639,  0.8675,  1.7893,\n",
       "                      -0.6293,  0.0318,  1.4824,  0.3148, -1.1402, -2.5340, -0.8857, -2.0043,\n",
       "                      -0.6469,  0.9308,  1.2578,  0.1848,  1.9382,  0.6435, -3.7585,  0.4822,\n",
       "                      -0.9429, -2.1472, -0.6501, -1.3377, -0.8911, -2.4238, -1.9372,  0.8316,\n",
       "                       0.8643, -0.9815, -3.0027, -2.7754, -0.3452, -1.5058, -0.7977,  1.2921,\n",
       "                       0.7515,  2.3110, -1.8048, -1.4670, -3.9867, -1.3492, -0.6589, -2.9355,\n",
       "                       2.1391, -0.1968, -2.6876, -1.5683,  0.1279, -4.4727, -1.6789, -1.2092,\n",
       "                      -3.5955, -0.3417, -1.6782, -1.5563, -0.0430, -0.9331, -0.5850, -1.0643,\n",
       "                      -0.8618,  0.2978, -1.0852,  1.0578, -1.0522,  0.4630,  0.0439,  1.7685,\n",
       "                      -1.9131, -0.5321, -0.7995,  3.7649,  0.3401,  1.8822,  2.1733, -1.0559,\n",
       "                      -2.9331, -0.3216, -1.8321, -2.2793,  1.1647,  2.6295, -1.2834, -3.0613,\n",
       "                      -1.2574,  0.2150, -1.2752, -2.4659, -0.6534, -0.9491, -0.8793, -0.8658,\n",
       "                      -3.7639,  2.0938, -0.8108,  1.2007, -0.8735, -1.2713, -1.4406,  0.8860,\n",
       "                      -1.1323,  2.7502, -0.6764,  0.0884, -2.5772,  0.4367, -2.7386, -1.3347,\n",
       "                       0.0765, -3.2072, -1.7982,  0.0968,  1.2114, -1.2767, -1.3841, -1.1759],\n",
       "                     device='cuda:0')),\n",
       "             ('downsample.1.bn.running_var',\n",
       "              tensor([ 8.4315,  2.2007,  3.7556,  5.8140,  6.4748,  4.0853,  2.2519,  3.4739,\n",
       "                       7.2280,  5.9322,  3.6785,  2.2238,  3.0472,  2.8436,  3.1539,  4.3972,\n",
       "                       3.6257,  7.8833,  7.3230,  3.8001,  6.2313,  3.7818,  5.1016,  6.3805,\n",
       "                       6.1973,  4.1176,  4.4618,  1.8534,  3.4897,  4.0329,  3.6539,  3.1590,\n",
       "                       3.7324,  5.2553,  4.9924,  4.5605,  4.1280,  5.2247,  3.3980,  2.4845,\n",
       "                       2.2640,  3.8911,  6.5612,  7.7743, 10.2817,  8.0525,  8.9979,  4.9763,\n",
       "                       7.3262,  3.1688,  2.7664,  6.0957,  4.3122,  6.0153,  7.1402,  4.5288,\n",
       "                       6.1349,  6.9018,  1.9569,  3.9823,  8.1262,  9.1643,  5.8230,  2.6085,\n",
       "                       2.7870,  3.4915,  3.9544,  8.2305,  2.1515,  3.0092,  4.1537,  7.6952,\n",
       "                       2.3233,  5.7540,  3.2125,  2.7150,  5.6045,  1.6859,  7.0421,  2.6922,\n",
       "                       5.3824,  5.8375, 12.1594,  6.4286,  2.5166,  5.0633,  7.9700,  5.0197,\n",
       "                       2.7024,  4.0156,  7.5710,  4.7840,  4.3071,  4.3494,  7.6360,  8.7355,\n",
       "                       3.0797,  5.9463,  2.5208,  2.6184, 10.4375,  4.9667,  2.8553,  2.5763,\n",
       "                       5.3027,  3.3181,  3.0322,  3.0841,  8.0044,  4.4526,  3.3130,  2.8669,\n",
       "                       3.5616,  3.2243,  3.4381,  2.1557,  2.3690,  5.2206, 10.6053,  2.5950,\n",
       "                       3.6506,  2.7502,  5.8947,  2.3104,  5.2839,  4.2116,  1.8888, 10.8241,\n",
       "                       3.7055,  7.1985,  7.7720,  3.6944,  8.1829,  4.5803,  5.5104,  4.3489,\n",
       "                       3.8965,  8.1566,  5.2995,  4.5138,  6.4940,  4.6213,  3.7215,  5.5382,\n",
       "                       9.9744,  7.1639,  2.9278,  5.7226,  2.9762,  4.4889,  8.8888, 13.4503,\n",
       "                       6.3129,  4.3130,  3.0842,  7.0121,  8.8410,  5.9017,  4.7300,  2.7717,\n",
       "                       3.5482,  4.8529,  5.7239,  4.9405,  6.4051,  6.7326,  4.5513,  3.2923,\n",
       "                       4.1118,  6.2733,  6.9711,  4.8135,  3.9121,  2.4872,  4.2022,  2.3229,\n",
       "                       2.5595,  3.1605,  5.0011,  3.2645,  3.5037,  4.7006,  3.0948,  6.7658,\n",
       "                       2.9721,  2.6317,  1.8698,  3.1145,  8.8116,  3.0467,  3.0276,  1.7797,\n",
       "                       2.4959,  2.7951,  3.0741,  5.2918,  2.3362,  3.7408,  3.9350,  4.2033,\n",
       "                       4.8935,  3.9323,  5.7840, 12.9871,  3.8130,  1.3063,  2.9295,  2.6781,\n",
       "                       4.3191,  2.4927,  1.7774,  8.9827,  4.3220,  2.9635,  2.6314,  3.6619,\n",
       "                       7.8457,  2.3496,  2.0832,  3.6382,  3.5611,  9.9684,  3.3942,  1.8557,\n",
       "                       9.1176,  3.0559,  3.9111,  6.2225,  3.7616,  6.7158,  3.3634,  3.4817,\n",
       "                       4.8253,  4.2425,  4.0376,  5.4823,  1.8279,  2.6626,  3.5743,  1.5908,\n",
       "                       8.8493,  6.0323,  4.3174, 12.6683,  4.5512,  7.1022,  1.9451,  5.8368,\n",
       "                      10.9814,  4.8034,  5.2081,  9.5947,  4.3885,  9.6459,  2.2029,  9.0258,\n",
       "                       2.3837,  3.8303,  4.4538,  2.9016,  3.2527,  1.8912,  3.1672,  3.1861,\n",
       "                       3.4159,  4.7551,  5.0195,  3.5220,  2.7673,  4.4270,  4.0477,  3.8453,\n",
       "                       3.2762,  5.0561,  2.3767,  3.2397, 11.9976,  2.2458,  3.9435,  2.1936,\n",
       "                       4.4465,  4.2152,  3.5358,  1.8099,  7.2581,  2.6845,  1.6617,  1.4907],\n",
       "                     device='cuda:0')),\n",
       "             ('downsample.1.bn.num_batches_tracked',\n",
       "              tensor(16497, device='cuda:0'))])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_state_dict(attribute_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b804fe-58ab-4f3b-be34-63b2885c1e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9250ff48-8133-4a16-81e4-c929c7b4946e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "#for model_name, chrom in itertools.islice(chromosomes.items(), 4):\n",
    "for model_name, chrom in chromosomes.items():\n",
    "    model=models[model_name]\n",
    "    for stage, ws in enumerate(chrom[\"ws\"], start=1): \n",
    "        if chrom[\"ds\"][stage-1]>1:\n",
    "            main_block_name=f\"{ws}\"\n",
    "            if main_block_name not in super_model:\n",
    "                main_block_index=f\"model.s{stage}.b2.state_dict\"\n",
    "                super_model[main_block_name]=get_state_dict(main_block_index)\n",
    "        if stage>1:\n",
    "            transition_block_name=f\"{chrom[\"ws\"][stage-2]}-{ws}\"\n",
    "            if transition_block_name not in super_model:\n",
    "                block_index=f\"model.s{stage}.b1.state_dict\"\n",
    "                super_model[transition_block_name]=get_state_dict(block_index)\n",
    "            \n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0df2d05b-f424-4872-8d32-00de85a27a15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caitie\n",
      "Chester\n",
      "Gutenberg\n",
      "LaMelo\n",
      "Mateo\n",
      "Sadie\n"
     ]
    }
   ],
   "source": [
    "from utils.train_cfg import save_checkpoint, load_checkpoint\n",
    "#save_checkpoint(super_model, f\"/home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/blocks_pool/{metadata[\"codename\"]}_super_model\")\n",
    "super_models=[]\n",
    "for codename in [\"Adaline\", \"Caitie\",\"Chester\",\"Gutenberg\",\"LaMelo\",\"Mateo\", \"Sadie\"]:\n",
    "    if metadata[\"codename\"]!=codename:\n",
    "        print(codename)\n",
    "        super_models.append(load_checkpoint(f\"/home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/blocks_pool/{codename}_super_model\"))\n",
    "        \n",
    "\n",
    "\n",
    "# Merging process\n",
    "blocks_pool = {}  # Start with dict1\n",
    "\n",
    "for d in super_models:\n",
    "    for key, value in d.items():\n",
    "        if key not in blocks_pool:\n",
    "            blocks_pool[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fbd3dca-ae0a-46a5-82d7-3b58dc6d4dcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(super_models[4].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "495308c9-ab79-4fe7-af6a-09d83aca57bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ws': [64, 168, 448, 1192],\n",
       " 'bs': [1.0, 1.0, 1.0, 1.0],\n",
       " 'gs': [8, 8, 8, 8],\n",
       " 'ds': [1, 3, 8, 7],\n",
       " 'num_stages': 4,\n",
       " 'total_size_mb': 111.36352920532227,\n",
       " 'h': 1,\n",
       " 'w': 1,\n",
       " 'flops': 5343381,\n",
       " 'params': 29193281,\n",
       " 'acts': 15349,\n",
       " 'WA': 56.0,\n",
       " 'W0': 64,\n",
       " 'WM': 2.6499999999999977,\n",
       " 'DEPTH': 19,\n",
       " 'GROUP_W': 8}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chromosomes[list(chromosomes.keys())[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8820f4-cb39-4b3b-a666-f9615a1df1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models['rampant_myna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "430f06ab-7b91-4f1c-b543-59b5289062eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['rampant_myna'].s2.b1.load_state_dict(blocks_pool[\"64-168\"])\n",
    "models['rampant_myna'].s2.b2.load_state_dict(blocks_pool[\"168\"])\n",
    "models['rampant_myna'].s2.b3.load_state_dict(blocks_pool[\"168\"])\n",
    "models['rampant_myna'].s3.b1.load_state_dict(blocks_pool[\"168-448\"])\n",
    "models['rampant_myna'].s3.b2.load_state_dict(blocks_pool[\"448\"])\n",
    "models['rampant_myna'].s3.b3.load_state_dict(blocks_pool[\"448\"])\n",
    "models['rampant_myna'].s3.b4.load_state_dict(blocks_pool[\"448\"])\n",
    "models['rampant_myna'].s3.b5.load_state_dict(blocks_pool[\"448\"])\n",
    "models['rampant_myna'].s3.b6.load_state_dict(blocks_pool[\"448\"])\n",
    "models['rampant_myna'].s3.b7.load_state_dict(blocks_pool[\"448\"])\n",
    "models['rampant_myna'].s3.b8.load_state_dict(blocks_pool[\"448\"])\n",
    "models['rampant_myna'].s4.b1.load_state_dict(blocks_pool[\"448-1192\"])\n",
    "models['rampant_myna'].s4.b2.load_state_dict(blocks_pool[\"1192\"])\n",
    "models['rampant_myna'].s4.b3.load_state_dict(blocks_pool[\"1192\"])\n",
    "models['rampant_myna'].s4.b4.load_state_dict(blocks_pool[\"1192\"])\n",
    "models['rampant_myna'].s4.b5.load_state_dict(blocks_pool[\"1192\"])\n",
    "models['rampant_myna'].s4.b6.load_state_dict(blocks_pool[\"1192\"])\n",
    "models['rampant_myna'].s4.b7.load_state_dict(blocks_pool[\"1192\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "039b0d40-2676-4e3f-bcd8-5b7c379681c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rampant_myna'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(chromosomes.keys())[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13d8ebb8-89e6-403e-8d70-3e4594a6d0ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ws': [64, 144, 336],\n",
       " 'bs': [1.0, 1.0, 1.0],\n",
       " 'gs': [8, 8, 8],\n",
       " 'ds': [1, 3, 8],\n",
       " 'num_stages': 3,\n",
       " 'total_size_mb': 10.019161224365234,\n",
       " 'h': 1,\n",
       " 'w': 1,\n",
       " 'flops': 456211,\n",
       " 'params': 2626463,\n",
       " 'acts': 3923,\n",
       " 'WA': 40.0,\n",
       " 'W0': 64,\n",
       " 'WM': 2.299999999999999,\n",
       " 'DEPTH': 12,\n",
       " 'GROUP_W': 8}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chromosomes['abiding_markhor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ed79880-ad34-417f-ad95-c3644ff5fd51",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('f.a.weight',\n",
       "              tensor([[[[-0.1518]],\n",
       "              \n",
       "                       [[ 0.1302]],\n",
       "              \n",
       "                       [[-0.0680]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.1605]],\n",
       "              \n",
       "                       [[ 0.0698]],\n",
       "              \n",
       "                       [[ 0.0785]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1096]],\n",
       "              \n",
       "                       [[-0.2180]],\n",
       "              \n",
       "                       [[-0.1583]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0429]],\n",
       "              \n",
       "                       [[ 0.0279]],\n",
       "              \n",
       "                       [[ 0.2393]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1702]],\n",
       "              \n",
       "                       [[ 0.0710]],\n",
       "              \n",
       "                       [[ 0.0414]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.1214]],\n",
       "              \n",
       "                       [[-0.1560]],\n",
       "              \n",
       "                       [[-0.1277]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2280]],\n",
       "              \n",
       "                       [[ 0.0582]],\n",
       "              \n",
       "                       [[-0.1478]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.2939]],\n",
       "              \n",
       "                       [[-0.1281]],\n",
       "              \n",
       "                       [[ 0.0512]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0130]],\n",
       "              \n",
       "                       [[-0.0755]],\n",
       "              \n",
       "                       [[ 0.1599]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0272]],\n",
       "              \n",
       "                       [[ 0.0804]],\n",
       "              \n",
       "                       [[-0.0542]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0821]],\n",
       "              \n",
       "                       [[-0.1620]],\n",
       "              \n",
       "                       [[ 0.1639]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0044]],\n",
       "              \n",
       "                       [[ 0.0220]],\n",
       "              \n",
       "                       [[ 0.0495]]]])),\n",
       "             ('f.a_bn.weight',\n",
       "              tensor([1.0584, 0.9508, 0.8595, 1.0133, 0.8132, 0.8382, 0.9349, 0.8109, 0.9159,\n",
       "                      0.8520, 0.9617, 0.9006, 0.8442, 1.0440, 0.9251, 0.8407, 0.8508, 0.9921,\n",
       "                      0.8841, 0.8882, 0.9520, 0.9420, 0.8869, 0.8838, 0.9006, 0.9396, 1.0308,\n",
       "                      0.9200, 0.7960, 0.8365, 0.9434, 0.9281, 0.9006, 0.9439, 0.8001, 0.8362,\n",
       "                      0.9623, 0.9556, 0.9287, 0.9859, 0.9810, 0.7587, 0.9631, 0.9936, 0.8941,\n",
       "                      0.7777, 0.9227, 0.9868, 0.9640, 0.9189, 0.8688, 0.9168, 0.9263, 0.9080,\n",
       "                      0.8781, 0.9903, 1.0132, 0.9201, 0.9749, 0.9646, 0.9198, 0.7132, 0.7949,\n",
       "                      0.9109, 0.8495, 0.9914, 0.9423, 0.8922, 0.9799, 0.8721, 0.7863, 0.9647,\n",
       "                      0.9271, 0.8820, 0.9074, 1.0152, 0.8801, 1.0540, 0.7573, 0.8427, 0.7559,\n",
       "                      0.9316, 0.8420, 1.0178, 0.7447, 1.0658, 0.8993, 0.9250, 0.7836, 0.9612,\n",
       "                      0.9550, 0.9580, 0.8336, 0.7920, 1.0247, 0.9043, 1.0050, 0.8767, 0.8579,\n",
       "                      1.0116, 0.8645, 0.8488, 0.9472, 0.8503, 0.9700, 0.8722, 0.9770, 0.8786,\n",
       "                      0.9014, 1.0439, 0.8618, 0.7484, 1.1084, 0.9772, 0.8713, 0.8616, 0.8252,\n",
       "                      0.9138, 0.8134, 0.8240, 0.9064, 0.9431, 0.9261, 0.9177, 0.8293, 0.8684,\n",
       "                      0.9112, 1.0176, 0.8326, 0.9151, 0.8417, 0.9338, 1.0601, 0.8847, 0.7795,\n",
       "                      0.9646, 1.0599, 0.8846, 0.9762, 0.7791, 0.8550, 0.8369, 0.9523, 0.8455])),\n",
       "             ('f.a_bn.bias',\n",
       "              tensor([ 7.4158e-02, -2.3759e-02, -6.7536e-02,  1.2611e-01, -6.5498e-02,\n",
       "                      -4.2872e-02,  1.7988e-01, -1.3092e-01, -1.4828e-02, -9.4204e-02,\n",
       "                       2.5583e-02,  1.1264e-02, -4.2019e-02,  9.3758e-02,  8.9357e-02,\n",
       "                      -1.8039e-02, -1.4350e-01,  9.0733e-02, -5.3188e-03, -5.0607e-02,\n",
       "                      -1.1599e-01,  4.1782e-02, -8.2638e-03,  8.6554e-02,  1.4129e-03,\n",
       "                       9.1278e-05,  8.3708e-02, -4.3494e-02, -1.3884e-01, -9.3357e-02,\n",
       "                      -7.7751e-02,  2.4890e-02, -4.3546e-02, -2.8628e-02, -3.5847e-02,\n",
       "                      -2.8824e-02,  1.6411e-02, -2.8210e-02, -1.0637e-02,  8.0047e-03,\n",
       "                       1.4804e-01, -1.1567e-01,  6.5742e-02, -1.1102e-01,  1.1268e-01,\n",
       "                      -1.2297e-01,  4.4581e-02,  1.3472e-01,  6.8141e-02,  4.2766e-02,\n",
       "                       1.5995e-02, -6.1928e-02, -5.4853e-04, -3.8884e-02, -4.4383e-02,\n",
       "                       4.6165e-02,  1.3565e-01, -1.6973e-02,  1.4536e-01,  5.1711e-02,\n",
       "                       3.0356e-02, -1.3689e-01,  1.5352e-02,  7.1098e-02, -1.7508e-01,\n",
       "                       6.1052e-02,  2.8996e-02,  2.5841e-02,  1.6014e-02,  3.4367e-02,\n",
       "                      -9.1176e-02, -8.0796e-02,  4.0488e-02, -3.8838e-02,  1.5075e-02,\n",
       "                       8.8665e-02, -1.0785e-01,  8.0305e-02, -7.5572e-02, -4.7698e-03,\n",
       "                      -1.4504e-01,  1.1815e-01, -1.7098e-01,  1.3830e-01, -1.3167e-01,\n",
       "                       6.2865e-02,  1.3596e-01, -2.1871e-02, -5.8390e-02, -9.3560e-03,\n",
       "                      -2.1635e-02,  2.5188e-02,  1.3837e-03, -9.6761e-02,  1.4095e-01,\n",
       "                      -3.9636e-02, -2.5503e-02, -1.0206e-01, -1.6069e-01, -7.9358e-02,\n",
       "                      -2.6037e-02, -1.4134e-01,  4.6560e-02,  4.5291e-02,  1.3564e-01,\n",
       "                      -1.4596e-01,  7.5281e-02,  3.2052e-02, -9.5988e-02,  1.1313e-01,\n",
       "                      -1.0956e-02, -4.7192e-02,  1.1421e-01,  3.0271e-03,  5.9722e-02,\n",
       "                      -7.5936e-03, -1.4215e-01,  1.7263e-02, -1.3368e-01, -1.0408e-01,\n",
       "                      -4.1492e-02,  9.0945e-02,  5.4103e-02, -3.5787e-02, -7.5215e-02,\n",
       "                      -3.4535e-03, -2.4296e-04,  5.9073e-02, -1.1474e-01,  1.1917e-01,\n",
       "                      -8.4662e-02,  2.4117e-03,  7.2122e-02, -3.8212e-03, -1.1594e-01,\n",
       "                       1.4783e-02,  1.8416e-02, -9.6739e-03, -5.2568e-02, -9.2455e-02,\n",
       "                      -1.0157e-01,  6.1017e-03,  1.1994e-01, -4.7376e-02])),\n",
       "             ('f.a_bn.running_mean',\n",
       "              tensor([ 0.9109, -0.5568, -0.1678, -1.1782,  0.2305, -0.4257, -0.5541, -1.1991,\n",
       "                      -1.4963,  2.2388, -1.0844, -0.1301, -1.9455, -1.2296, -0.1876, -1.6120,\n",
       "                       0.4313, -1.6481,  0.6195,  0.4117, -0.6631, -1.9670, -0.4821, -0.9524,\n",
       "                      -1.8317, -0.7732, -1.9928, -1.4180,  0.3651, -0.6902, -1.6392, -1.2321,\n",
       "                       0.1062, -0.4837,  1.0982, -1.0181,  1.1066,  0.0767,  0.7353,  0.6739,\n",
       "                       0.3217,  0.0417,  0.8271, -1.4448, -1.7333, -0.0685, -0.6377, -1.0069,\n",
       "                      -1.2976,  0.3623,  1.0254, -0.5783, -0.9554, -0.6208, -0.1707, -0.7472,\n",
       "                      -2.2788, -0.7462, -1.4018, -0.2688, -2.6274,  0.5184, -0.8753, -0.6024,\n",
       "                      -0.6554, -1.3872, -0.5786, -0.9420, -1.3143,  0.2478, -1.6224, -0.0691,\n",
       "                       1.3860, -1.4992,  0.0354,  0.2638,  2.1607, -1.3511, -1.1196, -0.4661,\n",
       "                      -0.4788, -0.8754,  0.2895,  0.1242,  0.3416, -0.7425, -0.9168, -0.5695,\n",
       "                       0.5702, -0.7773, -2.1196, -1.4189,  1.4240, -1.9118, -1.8933, -0.0512,\n",
       "                      -0.8151,  0.5912,  0.3562, -1.5731,  0.1242, -1.0308, -1.1165, -0.1600,\n",
       "                      -1.6023, -0.9521, -1.6270,  0.2412,  0.5956,  0.1816, -0.0893, -1.7689,\n",
       "                      -1.0206, -0.8294, -0.6719,  1.5827, -0.1341, -0.1941, -1.4486,  0.1338,\n",
       "                       1.8624, -0.4646, -1.5109,  0.6507, -2.2635, -0.7702, -1.6688, -0.5267,\n",
       "                      -0.9847,  0.8303, -1.0359, -0.7777, -0.3265, -1.1029, -0.0978,  0.4435,\n",
       "                      -2.1782, -0.1528,  0.0283,  0.0920, -0.2759, -0.1967,  0.5644,  1.4469])),\n",
       "             ('f.a_bn.running_var',\n",
       "              tensor([5.6037, 4.1565, 2.6744, 4.5581, 4.8130, 2.7379, 3.5541, 4.0318, 4.4133,\n",
       "                      4.9593, 7.3993, 4.8076, 5.2648, 6.7900, 3.8919, 2.8742, 1.9956, 4.4149,\n",
       "                      3.1217, 2.7410, 5.7183, 3.4176, 2.9864, 2.2748, 5.1676, 4.1180, 5.4378,\n",
       "                      4.2257, 2.4536, 2.4215, 4.7968, 2.9239, 1.9086, 4.4079, 2.6693, 3.0284,\n",
       "                      3.1635, 3.8680, 2.6249, 3.2119, 3.4680, 2.3506, 4.2858, 6.1285, 3.5667,\n",
       "                      3.4320, 5.3015, 7.0576, 4.1373, 3.2351, 3.9740, 3.5537, 3.3602, 2.9837,\n",
       "                      2.5992, 3.5766, 4.2256, 5.3362, 4.3196, 3.6387, 4.7919, 3.4153, 4.1880,\n",
       "                      4.2023, 1.9277, 3.1109, 5.4170, 2.0870, 3.6776, 2.5491, 2.6215, 3.7969,\n",
       "                      4.7025, 1.8918, 3.8876, 3.6152, 2.7363, 3.6783, 2.5214, 2.6410, 3.7013,\n",
       "                      3.4650, 3.0701, 6.9073, 2.9973, 7.0327, 4.0109, 4.1250, 2.4159, 5.3140,\n",
       "                      4.6576, 4.5428, 2.5431, 2.1380, 3.9911, 3.8661, 2.6425, 2.6210, 2.1362,\n",
       "                      3.0618, 3.7540, 3.0781, 2.3306, 3.3812, 3.8527, 2.1673, 3.4913, 3.6593,\n",
       "                      3.1652, 5.4750, 3.2182, 1.6705, 4.3603, 3.9089, 4.1922, 5.3336, 3.2779,\n",
       "                      3.2370, 2.4215, 3.6784, 3.2638, 5.6833, 2.6318, 3.1962, 4.3341, 3.3998,\n",
       "                      2.5498, 3.6052, 2.7515, 5.8626, 4.2949, 3.7276, 4.5445, 4.5693, 3.2977,\n",
       "                      4.3519, 6.1046, 4.4560, 3.0828, 3.6810, 4.2652, 2.1723, 3.6072, 3.3834])),\n",
       "             ('f.a_bn.num_batches_tracked', tensor(17199)),\n",
       "             ('f.b.weight',\n",
       "              tensor([[[[-0.0010, -0.0019, -0.0407],\n",
       "                        [ 0.0447, -0.1239, -0.1476],\n",
       "                        [ 0.1329, -0.0330, -0.1027]],\n",
       "              \n",
       "                       [[-0.1586, -0.0216,  0.0777],\n",
       "                        [-0.1213, -0.0702, -0.0210],\n",
       "                        [ 0.0299,  0.1279, -0.0460]],\n",
       "              \n",
       "                       [[-0.0492, -0.0246,  0.0031],\n",
       "                        [-0.0039, -0.0655,  0.1396],\n",
       "                        [ 0.0473, -0.0022,  0.1130]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0557,  0.0493, -0.0153],\n",
       "                        [ 0.1061,  0.0226, -0.1058],\n",
       "                        [-0.0258,  0.0179,  0.0253]],\n",
       "              \n",
       "                       [[-0.1170,  0.0213, -0.0310],\n",
       "                        [-0.1876,  0.0018,  0.0915],\n",
       "                        [-0.1661, -0.0517, -0.0009]],\n",
       "              \n",
       "                       [[-0.0168, -0.0181, -0.1382],\n",
       "                        [ 0.0104, -0.0204, -0.1525],\n",
       "                        [-0.1073,  0.0034, -0.0481]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0253, -0.0215,  0.0162],\n",
       "                        [ 0.0722,  0.1237, -0.2478],\n",
       "                        [ 0.0128,  0.0566, -0.1488]],\n",
       "              \n",
       "                       [[-0.0074,  0.0907,  0.0206],\n",
       "                        [-0.1435, -0.0046,  0.1361],\n",
       "                        [-0.1289, -0.1001,  0.1407]],\n",
       "              \n",
       "                       [[ 0.0082,  0.0021,  0.0191],\n",
       "                        [-0.0388, -0.0642,  0.0818],\n",
       "                        [-0.0434, -0.0181,  0.0554]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.1193,  0.0096,  0.0325],\n",
       "                        [-0.0598, -0.0186,  0.0615],\n",
       "                        [-0.0459,  0.0863,  0.0501]],\n",
       "              \n",
       "                       [[-0.0713,  0.0513,  0.0559],\n",
       "                        [-0.1112,  0.0321,  0.1077],\n",
       "                        [-0.1027, -0.0761,  0.1188]],\n",
       "              \n",
       "                       [[-0.0097, -0.0425,  0.0851],\n",
       "                        [-0.0028, -0.0198,  0.0623],\n",
       "                        [ 0.0321, -0.0196, -0.0173]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0590, -0.0534, -0.0023],\n",
       "                        [ 0.1083,  0.0671,  0.0388],\n",
       "                        [-0.0911, -0.0511, -0.0236]],\n",
       "              \n",
       "                       [[-0.0677, -0.0224,  0.0087],\n",
       "                        [ 0.0735, -0.0784,  0.0336],\n",
       "                        [ 0.0793, -0.0939,  0.0420]],\n",
       "              \n",
       "                       [[-0.0560,  0.0539,  0.0345],\n",
       "                        [-0.0865, -0.0465,  0.0029],\n",
       "                        [ 0.0910, -0.0447, -0.0349]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0545, -0.0151,  0.0211],\n",
       "                        [ 0.0306,  0.1243,  0.0369],\n",
       "                        [-0.0062,  0.0047,  0.0063]],\n",
       "              \n",
       "                       [[ 0.0183, -0.0243, -0.0203],\n",
       "                        [ 0.1835,  0.1072,  0.1165],\n",
       "                        [ 0.0784, -0.0282, -0.0564]],\n",
       "              \n",
       "                       [[-0.0035,  0.0758,  0.0487],\n",
       "                        [-0.0235,  0.0528, -0.0537],\n",
       "                        [-0.0206,  0.0204, -0.0298]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0911,  0.0894, -0.0052],\n",
       "                        [ 0.1346,  0.0854,  0.0872],\n",
       "                        [ 0.0827, -0.0600,  0.0251]],\n",
       "              \n",
       "                       [[-0.0166, -0.1354, -0.0735],\n",
       "                        [-0.0123, -0.0571, -0.0649],\n",
       "                        [ 0.0441, -0.1051, -0.0383]],\n",
       "              \n",
       "                       [[-0.1267,  0.0830,  0.1858],\n",
       "                        [-0.0287, -0.1224,  0.1618],\n",
       "                        [ 0.0610,  0.0224,  0.0086]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0173, -0.0665, -0.0608],\n",
       "                        [ 0.0241,  0.0021, -0.0354],\n",
       "                        [ 0.0299,  0.0923, -0.0401]],\n",
       "              \n",
       "                       [[ 0.0212, -0.0085,  0.0634],\n",
       "                        [ 0.0135,  0.0570,  0.0209],\n",
       "                        [ 0.0355,  0.0108,  0.0219]],\n",
       "              \n",
       "                       [[-0.0551, -0.1419, -0.0747],\n",
       "                        [ 0.0650, -0.0900, -0.0630],\n",
       "                        [ 0.0398,  0.0284,  0.0365]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0550, -0.0253, -0.0832],\n",
       "                        [ 0.1164,  0.0107,  0.0554],\n",
       "                        [ 0.1220,  0.1009,  0.0901]],\n",
       "              \n",
       "                       [[ 0.0868,  0.0618,  0.0047],\n",
       "                        [ 0.0462,  0.0458, -0.0382],\n",
       "                        [ 0.0232, -0.0219,  0.0918]],\n",
       "              \n",
       "                       [[ 0.0978,  0.0785, -0.0019],\n",
       "                        [-0.0157,  0.0553,  0.0282],\n",
       "                        [-0.1517, -0.0714, -0.0473]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0612,  0.1076,  0.0965],\n",
       "                        [ 0.1015,  0.1312, -0.0417],\n",
       "                        [-0.0243, -0.1442, -0.1153]],\n",
       "              \n",
       "                       [[-0.0028, -0.0062,  0.0056],\n",
       "                        [-0.0119, -0.0091, -0.2158],\n",
       "                        [-0.0494, -0.1527, -0.1183]],\n",
       "              \n",
       "                       [[-0.0125, -0.0131,  0.0040],\n",
       "                        [ 0.0626,  0.1258,  0.0453],\n",
       "                        [-0.1140, -0.1002, -0.0397]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0505, -0.0221, -0.0004],\n",
       "                        [-0.0166, -0.0447,  0.1321],\n",
       "                        [-0.1013,  0.2643,  0.0647]],\n",
       "              \n",
       "                       [[-0.0658,  0.0314,  0.0244],\n",
       "                        [ 0.0969,  0.0603, -0.0324],\n",
       "                        [ 0.0538,  0.0436,  0.0007]],\n",
       "              \n",
       "                       [[-0.0411,  0.0509,  0.0387],\n",
       "                        [ 0.0822,  0.0012, -0.0563],\n",
       "                        [ 0.0737,  0.0522, -0.0059]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0421, -0.0951,  0.0485],\n",
       "                        [-0.1059,  0.0415,  0.0122],\n",
       "                        [ 0.0340, -0.0110, -0.0621]],\n",
       "              \n",
       "                       [[ 0.0931,  0.1081,  0.0643],\n",
       "                        [ 0.0173,  0.0163, -0.0551],\n",
       "                        [ 0.0211, -0.1166, -0.0342]],\n",
       "              \n",
       "                       [[ 0.0781,  0.0401, -0.0185],\n",
       "                        [-0.0687, -0.0223, -0.0369],\n",
       "                        [-0.0029, -0.0450,  0.0067]]]])),\n",
       "             ('f.b_bn.weight',\n",
       "              tensor([0.8620, 0.9769, 0.8812, 0.9836, 0.9404, 0.8617, 0.9495, 0.8400, 0.9035,\n",
       "                      0.9407, 0.9563, 0.9388, 0.9425, 1.0047, 1.0269, 0.9660, 0.9504, 0.8560,\n",
       "                      0.8314, 0.8809, 0.8429, 0.8396, 0.9131, 0.9591, 0.9309, 0.9419, 0.8754,\n",
       "                      0.9637, 0.8976, 0.9291, 0.9414, 0.8698, 0.8571, 0.9182, 0.8869, 0.8714,\n",
       "                      0.7341, 0.8980, 0.9100, 0.8445, 0.9142, 0.9599, 0.9158, 0.9498, 0.9062,\n",
       "                      0.9060, 0.8310, 0.9478, 0.8625, 0.9666, 0.8056, 0.8818, 0.8413, 0.8518,\n",
       "                      0.9421, 0.9537, 0.9783, 0.9359, 0.9176, 0.9638, 0.9297, 0.9029, 0.8765,\n",
       "                      0.9230, 0.9517, 0.8102, 0.9203, 0.9600, 0.8737, 0.7856, 0.9416, 0.8960,\n",
       "                      0.7830, 0.8765, 0.8642, 0.8211, 0.9195, 0.9001, 0.9683, 0.8480, 0.9335,\n",
       "                      0.9510, 0.9869, 0.8901, 0.9391, 0.8144, 0.9198, 0.9532, 0.8542, 0.8869,\n",
       "                      0.8144, 0.9355, 0.9070, 0.9367, 0.8571, 0.9878, 0.8516, 0.8516, 0.8690,\n",
       "                      0.8262, 0.8771, 0.8858, 0.8494, 0.8985, 0.9273, 1.0379, 0.9809, 0.8977,\n",
       "                      0.7961, 0.8677, 0.8645, 0.8518, 0.8350, 0.9239, 0.8829, 0.8633, 0.8903,\n",
       "                      0.9296, 0.9852, 0.9017, 0.8794, 0.9992, 0.8516, 0.8965, 0.9468, 0.8535,\n",
       "                      0.8447, 0.8927, 0.9653, 0.9777, 0.8555, 0.9445, 0.9507, 0.9113, 0.9386,\n",
       "                      0.8944, 0.9227, 0.8495, 0.9413, 0.9619, 0.9596, 0.9399, 0.9567, 0.9062])),\n",
       "             ('f.b_bn.bias',\n",
       "              tensor([-0.0441, -0.0267, -0.0570, -0.0672, -0.0285, -0.0717, -0.0140, -0.0955,\n",
       "                      -0.0619, -0.0008, -0.0520, -0.1675, -0.1175, -0.1238, -0.0799, -0.0094,\n",
       "                      -0.0615, -0.0038, -0.1409, -0.0312, -0.1310, -0.1102, -0.0757, -0.0442,\n",
       "                       0.0378, -0.0234, -0.1062, -0.0564,  0.0016, -0.0156, -0.0143, -0.1403,\n",
       "                      -0.1337,  0.0188, -0.0620, -0.0959, -0.1173, -0.1007, -0.1353, -0.0771,\n",
       "                      -0.1252, -0.1185, -0.0820, -0.0324,  0.0571, -0.0972, -0.1751, -0.0537,\n",
       "                      -0.1874, -0.0142, -0.1207,  0.0289, -0.1309, -0.2098, -0.0683, -0.0715,\n",
       "                      -0.1333, -0.0506, -0.0742, -0.0497, -0.1298,  0.0346, -0.1362,  0.0594,\n",
       "                      -0.1079, -0.0258,  0.0257, -0.0167, -0.1219, -0.1449, -0.0091, -0.0861,\n",
       "                      -0.1324, -0.0984,  0.0365,  0.0012, -0.0404, -0.0691, -0.1326, -0.0077,\n",
       "                      -0.1996,  0.0265, -0.0978, -0.0488, -0.0546, -0.1210, -0.0310, -0.1075,\n",
       "                      -0.1012, -0.0900, -0.1866, -0.1240, -0.0935,  0.0577, -0.1202, -0.0166,\n",
       "                      -0.0416, -0.1242, -0.1570, -0.1100, -0.1556, -0.0035, -0.0241, -0.0460,\n",
       "                      -0.0460, -0.1396, -0.0821, -0.1084, -0.2650, -0.1099, -0.1214, -0.0853,\n",
       "                      -0.1396, -0.0837, -0.2123, -0.0996, -0.0894, -0.1663, -0.1318, -0.0542,\n",
       "                      -0.0575, -0.0228,  0.0079,  0.0181, -0.1162, -0.1615, -0.1639, -0.1300,\n",
       "                       0.0065, -0.0929, -0.0761, -0.0116,  0.0027,  0.0298,  0.0560, -0.0440,\n",
       "                       0.1045, -0.1292,  0.0101, -0.1156,  0.0215, -0.0013, -0.0501, -0.1207])),\n",
       "             ('f.b_bn.running_mean',\n",
       "              tensor([-2.4666e-01, -1.3228e-02, -1.4102e-01, -3.6787e-01, -1.0073e-01,\n",
       "                      -1.4382e-01, -1.2736e-01, -6.5750e-01,  5.0965e-01, -4.9073e-03,\n",
       "                      -1.3327e-01, -2.4436e-01, -6.6279e-01, -2.8026e-01, -2.4922e-01,\n",
       "                      -6.3601e-02, -2.6118e-01, -1.8697e-01, -7.0422e-02,  3.2363e-01,\n",
       "                      -1.2073e-02, -1.4700e-01, -1.9078e-01,  2.2568e-01,  3.6914e-02,\n",
       "                      -3.3826e-01, -2.4122e-01, -4.7569e-01,  5.9428e-02, -4.4155e-01,\n",
       "                      -1.7586e-01, -8.0400e-02,  3.9601e-01, -3.7174e-01, -2.1556e-01,\n",
       "                      -9.8866e-02, -3.9178e-01,  5.6015e-02, -2.9716e-01,  6.8829e-01,\n",
       "                       5.2939e-02, -7.1647e-02,  4.9582e-02, -3.6862e-02,  1.5940e-01,\n",
       "                      -6.3898e-01,  5.5795e-02,  9.4387e-02,  1.4913e-01, -1.6577e-01,\n",
       "                      -4.6467e-01, -2.9932e-01, -1.0135e-01, -1.2704e-01, -7.2498e-02,\n",
       "                       1.1345e-01, -1.7488e-01, -7.1218e-01,  3.4758e-01, -5.1853e-02,\n",
       "                       1.3036e-01,  2.4910e-01, -2.2038e-01, -5.7118e-01, -3.2831e-02,\n",
       "                      -1.0002e-01, -3.3718e-01, -3.3366e-01, -1.3934e-01,  1.4912e-01,\n",
       "                      -2.3977e-01, -3.5712e-02, -1.6205e-01, -5.7084e-02, -3.5372e-01,\n",
       "                      -2.7182e-01, -4.2327e-01, -7.2366e-02, -7.3288e-02, -2.7190e-02,\n",
       "                       5.0173e-01, -1.1444e-01, -1.8829e-01, -8.4593e-01, -4.5213e-01,\n",
       "                       1.3657e-01,  2.0213e-01,  1.0485e-01,  8.7760e-02, -9.2919e-02,\n",
       "                       3.2530e-01, -4.1019e-01, -1.5887e-01,  2.9372e-01, -4.1084e-01,\n",
       "                      -1.7501e-01,  1.1409e-01,  1.5908e-01,  1.6548e-01, -4.6013e-01,\n",
       "                      -3.1642e-01, -5.4216e-01, -1.5067e-01, -9.8530e-02, -5.6292e-02,\n",
       "                      -1.7977e-02, -2.0031e-02, -1.6284e-01,  4.6577e-01, -2.1509e-01,\n",
       "                      -2.3453e-01, -2.8552e-02,  3.3285e-02, -2.2969e-01, -1.5809e-01,\n",
       "                       4.1603e-01, -8.0015e-02,  1.7355e-01, -6.8683e-02, -3.2480e-02,\n",
       "                      -2.1254e-01,  6.6895e-02, -5.6547e-04,  3.2642e-02,  5.8294e-02,\n",
       "                      -1.0797e-01, -7.9364e-01, -7.8593e-02,  9.8459e-02, -2.2849e-01,\n",
       "                      -1.4886e-01, -4.5720e-02, -8.3771e-02, -3.0365e-01, -2.2292e-01,\n",
       "                      -9.1670e-02, -3.0123e-02, -2.6863e-02,  7.8118e-03, -1.3047e-01,\n",
       "                       3.4770e-02,  5.3336e-02, -3.9776e-02,  1.6034e-01])),\n",
       "             ('f.b_bn.running_var',\n",
       "              tensor([0.2009, 0.3660, 0.2202, 0.3261, 0.1573, 0.1085, 0.2063, 0.2650, 0.1825,\n",
       "                      0.2673, 0.1119, 0.2403, 0.2483, 0.2182, 0.1744, 0.2202, 0.1987, 0.1467,\n",
       "                      0.1358, 0.1832, 0.1271, 0.1619, 0.1477, 0.2560, 0.1678, 0.1559, 0.3564,\n",
       "                      0.2314, 0.1751, 0.2317, 0.1822, 0.1807, 0.2173, 0.3206, 0.1151, 0.1327,\n",
       "                      0.1119, 0.1866, 0.2102, 0.2110, 0.2052, 0.1280, 0.1888, 0.1106, 0.2631,\n",
       "                      0.2530, 0.2053, 0.1886, 0.2281, 0.2014, 0.3383, 0.1707, 0.1457, 0.2297,\n",
       "                      0.1797, 0.1319, 0.1452, 0.3049, 0.1732, 0.1681, 0.2901, 0.1409, 0.1667,\n",
       "                      0.2653, 0.1742, 0.1647, 0.1541, 0.1740, 0.1318, 0.1211, 0.1894, 0.1492,\n",
       "                      0.1996, 0.1403, 0.1989, 0.1549, 0.1887, 0.1399, 0.1518, 0.2062, 0.3315,\n",
       "                      0.1879, 0.2820, 0.2423, 0.1901, 0.1664, 0.1464, 0.1943, 0.1725, 0.1514,\n",
       "                      0.2399, 0.2511, 0.2196, 0.2328, 0.1792, 0.2761, 0.1610, 0.1407, 0.1093,\n",
       "                      0.1497, 0.2427, 0.2250, 0.1460, 0.1600, 0.1496, 0.2196, 0.1966, 0.1961,\n",
       "                      0.1965, 0.1316, 0.1914, 0.1381, 0.1246, 0.1366, 0.1056, 0.1698, 0.1277,\n",
       "                      0.1683, 0.1657, 0.1664, 0.1842, 0.1538, 0.1619, 0.2667, 0.1983, 0.1869,\n",
       "                      0.2745, 0.1439, 0.2217, 0.1435, 0.1912, 0.1683, 0.1638, 0.1473, 0.2984,\n",
       "                      0.2807, 0.1472, 0.2004, 0.1303, 0.2741, 0.1506, 0.2179, 0.1862, 0.1027])),\n",
       "             ('f.b_bn.num_batches_tracked', tensor(17199)),\n",
       "             ('f.se.f_ex.0.weight',\n",
       "              tensor([[[[-0.3907]],\n",
       "              \n",
       "                       [[-0.4376]],\n",
       "              \n",
       "                       [[-0.2636]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0276]],\n",
       "              \n",
       "                       [[-0.0373]],\n",
       "              \n",
       "                       [[-0.0065]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.5254]],\n",
       "              \n",
       "                       [[-0.3000]],\n",
       "              \n",
       "                       [[ 0.0948]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.1848]],\n",
       "              \n",
       "                       [[ 0.0209]],\n",
       "              \n",
       "                       [[ 0.2121]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1941]],\n",
       "              \n",
       "                       [[ 0.2079]],\n",
       "              \n",
       "                       [[-0.0157]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.4306]],\n",
       "              \n",
       "                       [[-0.1925]],\n",
       "              \n",
       "                       [[ 0.1786]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2087]],\n",
       "              \n",
       "                       [[-0.0304]],\n",
       "              \n",
       "                       [[ 0.1931]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0555]],\n",
       "              \n",
       "                       [[-0.4379]],\n",
       "              \n",
       "                       [[-0.0914]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0965]],\n",
       "              \n",
       "                       [[-0.1558]],\n",
       "              \n",
       "                       [[-0.0166]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.1550]],\n",
       "              \n",
       "                       [[ 0.2028]],\n",
       "              \n",
       "                       [[ 0.0786]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.4053]],\n",
       "              \n",
       "                       [[-0.0979]],\n",
       "              \n",
       "                       [[ 0.3122]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0351]],\n",
       "              \n",
       "                       [[ 0.1382]],\n",
       "              \n",
       "                       [[ 0.1630]]]])),\n",
       "             ('f.se.f_ex.0.bias',\n",
       "              tensor([-0.0229,  0.0195,  0.0077, -0.0580,  0.0122,  0.0489,  0.0134, -0.0650,\n",
       "                      -0.0517, -0.0526, -0.0428,  0.0199,  0.0522,  0.0430, -0.0332, -0.0728,\n",
       "                       0.0181, -0.0317, -0.0181, -0.0735, -0.0373,  0.0320, -0.0160, -0.0536,\n",
       "                      -0.0571,  0.0344, -0.0570,  0.0239, -0.0729, -0.0648, -0.1088, -0.0563,\n",
       "                      -0.0063, -0.0405, -0.0653,  0.0358])),\n",
       "             ('f.se.f_ex.2.weight',\n",
       "              tensor([[[[-0.1173]],\n",
       "              \n",
       "                       [[-0.2776]],\n",
       "              \n",
       "                       [[-0.1453]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0496]],\n",
       "              \n",
       "                       [[ 0.0142]],\n",
       "              \n",
       "                       [[ 0.0321]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1313]],\n",
       "              \n",
       "                       [[-0.0596]],\n",
       "              \n",
       "                       [[ 0.2297]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.2659]],\n",
       "              \n",
       "                       [[-0.0402]],\n",
       "              \n",
       "                       [[ 0.0521]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0131]],\n",
       "              \n",
       "                       [[-0.0711]],\n",
       "              \n",
       "                       [[-0.1359]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0593]],\n",
       "              \n",
       "                       [[-0.0034]],\n",
       "              \n",
       "                       [[ 0.0773]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.1260]],\n",
       "              \n",
       "                       [[-0.1347]],\n",
       "              \n",
       "                       [[-0.0841]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0693]],\n",
       "              \n",
       "                       [[-0.0194]],\n",
       "              \n",
       "                       [[ 0.1128]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0903]],\n",
       "              \n",
       "                       [[ 0.1984]],\n",
       "              \n",
       "                       [[-0.0422]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0080]],\n",
       "              \n",
       "                       [[ 0.1630]],\n",
       "              \n",
       "                       [[-0.0146]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2221]],\n",
       "              \n",
       "                       [[ 0.0028]],\n",
       "              \n",
       "                       [[ 0.1175]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0126]],\n",
       "              \n",
       "                       [[-0.0327]],\n",
       "              \n",
       "                       [[-0.0353]]]])),\n",
       "             ('f.se.f_ex.2.bias',\n",
       "              tensor([-7.0817e-02,  2.5660e-01, -4.4059e-02,  2.4567e-01,  1.5216e-01,\n",
       "                       8.4647e-02,  8.4644e-02, -5.2752e-02, -1.1775e-01, -1.6485e-02,\n",
       "                      -4.6615e-02,  1.1906e-01, -6.1605e-02,  9.2687e-02,  1.2877e-01,\n",
       "                       1.9798e-01,  6.1762e-02,  3.5042e-02, -1.9358e-01,  7.0582e-02,\n",
       "                      -1.1187e-02, -4.1822e-02, -6.0977e-02,  6.5496e-02, -7.6669e-02,\n",
       "                       7.3467e-02, -1.4704e-01,  9.8380e-02, -3.2598e-02,  7.6710e-02,\n",
       "                       5.7718e-02,  2.9191e-02,  6.4492e-02,  3.4615e-02, -1.5002e-01,\n",
       "                      -8.2662e-02, -3.0246e-01, -1.0315e-01, -4.4827e-02, -1.6310e-01,\n",
       "                      -1.1451e-01,  9.4938e-02,  8.8593e-02,  6.4434e-02, -3.7822e-02,\n",
       "                      -1.2101e-01, -1.3795e-01, -9.2802e-02,  1.2012e-02,  1.5110e-01,\n",
       "                       5.7859e-02, -7.4909e-02,  7.3756e-02,  6.1702e-02,  1.4618e-01,\n",
       "                      -4.1927e-02,  1.8634e-01,  2.7554e-02,  1.6274e-01,  1.0327e-01,\n",
       "                      -1.0367e-01, -6.9250e-02,  8.5395e-02,  1.3204e-01,  1.4543e-02,\n",
       "                      -7.9084e-02, -9.7989e-02,  1.2734e-01, -1.1156e-01,  4.2577e-02,\n",
       "                       5.2324e-03,  4.1316e-02, -2.5116e-01,  6.8257e-02,  5.0880e-02,\n",
       "                      -6.6250e-02, -1.0600e-01,  8.4191e-02,  6.9917e-03,  3.8176e-02,\n",
       "                      -8.0993e-02,  1.1844e-02,  2.2513e-01, -4.0428e-02, -5.7063e-02,\n",
       "                      -1.1498e-01,  6.9989e-02,  2.2757e-01, -1.5634e-01,  1.0620e-01,\n",
       "                      -7.6034e-02,  1.2985e-02, -3.4781e-02,  1.8248e-01, -9.5377e-02,\n",
       "                      -2.2668e-02, -1.7713e-01,  1.1268e-01, -1.3688e-01, -1.8547e-01,\n",
       "                       1.4780e-02, -6.4843e-02, -1.0698e-01,  1.3724e-01, -1.0463e-01,\n",
       "                       1.0400e-01,  2.1470e-01,  1.2002e-01, -5.6683e-02, -6.2843e-03,\n",
       "                       4.5416e-02,  1.4176e-03, -1.6242e-01,  1.8149e-01,  9.2526e-02,\n",
       "                       6.1032e-02,  3.7724e-02,  1.2432e-01,  1.6144e-02,  3.7765e-02,\n",
       "                      -5.2963e-02,  7.9044e-02, -2.7398e-01, -2.3779e-02, -1.9793e-04,\n",
       "                       9.9149e-02, -1.2926e-02, -6.8736e-02,  5.1669e-02,  1.5917e-02,\n",
       "                       1.8384e-02, -5.7693e-02,  1.2828e-02,  1.3828e-01,  1.7054e-01,\n",
       "                      -1.4543e-01,  1.6527e-01, -4.1963e-02,  5.5631e-02,  1.5241e-01,\n",
       "                      -1.5843e-02,  1.0875e-01,  4.4678e-02, -3.8063e-02])),\n",
       "             ('f.c.weight',\n",
       "              tensor([[[[-9.2571e-02]],\n",
       "              \n",
       "                       [[-1.3243e-01]],\n",
       "              \n",
       "                       [[ 2.6667e-01]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 7.9407e-02]],\n",
       "              \n",
       "                       [[-4.0816e-02]],\n",
       "              \n",
       "                       [[-1.7209e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.4477e-02]],\n",
       "              \n",
       "                       [[ 1.1337e-01]],\n",
       "              \n",
       "                       [[-1.0713e-01]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-7.4742e-02]],\n",
       "              \n",
       "                       [[ 2.0166e-01]],\n",
       "              \n",
       "                       [[ 1.9026e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.4388e-03]],\n",
       "              \n",
       "                       [[-2.6081e-02]],\n",
       "              \n",
       "                       [[-2.2093e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 6.1482e-02]],\n",
       "              \n",
       "                       [[-5.7317e-02]],\n",
       "              \n",
       "                       [[-1.5073e-01]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-2.0393e-01]],\n",
       "              \n",
       "                       [[ 2.2934e-04]],\n",
       "              \n",
       "                       [[ 1.5160e-01]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 5.7481e-02]],\n",
       "              \n",
       "                       [[-7.1908e-02]],\n",
       "              \n",
       "                       [[-2.8370e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.4921e-01]],\n",
       "              \n",
       "                       [[ 5.6105e-02]],\n",
       "              \n",
       "                       [[ 2.5304e-01]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 7.8503e-02]],\n",
       "              \n",
       "                       [[-2.6211e-02]],\n",
       "              \n",
       "                       [[-1.1227e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.4080e-01]],\n",
       "              \n",
       "                       [[ 1.1885e-01]],\n",
       "              \n",
       "                       [[-6.6797e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.1871e-01]],\n",
       "              \n",
       "                       [[ 2.8887e-02]],\n",
       "              \n",
       "                       [[-3.8967e-02]]]])),\n",
       "             ('f.c_bn.weight',\n",
       "              tensor([0.9066, 0.9437, 0.8054, 0.8305, 0.8725, 0.8865, 0.8506, 0.9683, 0.8367,\n",
       "                      0.9403, 0.9241, 0.8496, 0.9103, 0.9630, 0.9036, 0.8588, 0.8564, 0.9253,\n",
       "                      0.9540, 0.8452, 0.9240, 0.8516, 0.9827, 0.9641, 1.0006, 1.0136, 0.9878,\n",
       "                      0.8301, 0.9526, 0.9148, 0.9408, 0.8671, 0.8630, 0.9705, 0.8024, 0.9294,\n",
       "                      0.9726, 0.8852, 0.8764, 0.9388, 0.8726, 0.9592, 0.9315, 0.9039, 0.9108,\n",
       "                      0.9171, 0.9187, 0.9002, 0.8240, 0.8968, 0.7787, 0.8739, 0.8396, 0.9511,\n",
       "                      0.8248, 0.8714, 0.9042, 0.9000, 1.0278, 0.9379, 1.0163, 0.9496, 0.9560,\n",
       "                      0.7993, 0.8636, 0.9464, 0.8349, 0.9171, 0.9194, 0.9113, 0.8427, 0.9452,\n",
       "                      0.9275, 0.9816, 0.9123, 0.8736, 0.8641, 1.0208, 0.8811, 1.0747, 1.0187,\n",
       "                      0.9182, 1.0018, 0.7792, 0.8629, 0.9635, 0.9460, 0.8251, 0.8651, 0.8384,\n",
       "                      0.8797, 0.9862, 0.9328, 0.9402, 0.8858, 0.9071, 0.9531, 0.8935, 0.8494,\n",
       "                      0.8958, 0.9919, 0.8620, 0.9171, 0.9370, 0.9779, 0.8027, 0.8800, 0.8878,\n",
       "                      0.8790, 0.9263, 0.9100, 0.9907, 0.8719, 0.9423, 0.8763, 0.7822, 0.8312,\n",
       "                      0.8986, 0.8962, 0.9954, 0.9550, 0.9778, 0.8863, 0.9397, 0.8709, 0.9209,\n",
       "                      0.8153, 0.9512, 0.9928, 0.8779, 0.8866, 0.8430, 0.9089, 1.0046, 0.8657,\n",
       "                      0.8901, 0.9934, 0.8318, 0.9988, 1.0433, 0.9216, 0.9665, 0.9162, 0.9524])),\n",
       "             ('f.c_bn.bias',\n",
       "              tensor([-0.0435, -0.1184, -0.1016, -0.1411, -0.0158, -0.0445, -0.0574,  0.0415,\n",
       "                      -0.0589, -0.0476, -0.0514, -0.0177, -0.0946, -0.1009, -0.0125, -0.0935,\n",
       "                      -0.0645, -0.1688, -0.0984, -0.0930, -0.0546, -0.1319, -0.0340,  0.0727,\n",
       "                      -0.0800, -0.1816,  0.0529, -0.1010, -0.1628, -0.0162, -0.0467, -0.1029,\n",
       "                      -0.1225, -0.0004, -0.0209, -0.0508, -0.0254, -0.0168,  0.0490, -0.1595,\n",
       "                      -0.1113, -0.0509, -0.0959, -0.1487, -0.0927, -0.2185, -0.0701, -0.0543,\n",
       "                      -0.1346, -0.0497, -0.1048, -0.1218, -0.0699,  0.0107, -0.1250, -0.0119,\n",
       "                      -0.0288, -0.0962, -0.1172, -0.0824, -0.0144, -0.0143, -0.0020, -0.1485,\n",
       "                      -0.0535, -0.1583, -0.1880, -0.0564, -0.1319, -0.1071, -0.0501, -0.0388,\n",
       "                      -0.1187, -0.0613, -0.0375, -0.0838, -0.1612, -0.0749, -0.0662,  0.0576,\n",
       "                      -0.1111, -0.0523, -0.0026, -0.0989,  0.0771, -0.0997, -0.1519, -0.1757,\n",
       "                      -0.0585, -0.0293, -0.0509, -0.1162, -0.1501, -0.0725, -0.0503, -0.0991,\n",
       "                      -0.0381, -0.0649,  0.0361, -0.0066, -0.0332, -0.0485,  0.0397, -0.0106,\n",
       "                      -0.0749, -0.1269, -0.0731, -0.0418, -0.1005, -0.0753, -0.0784, -0.0818,\n",
       "                      -0.1697,  0.0396, -0.0011, -0.0749, -0.1418, -0.1167, -0.0916, -0.1585,\n",
       "                      -0.0628, -0.0491, -0.1230, -0.1202,  0.0494, -0.0806, -0.1550, -0.1118,\n",
       "                      -0.0023, -0.0680, -0.0154, -0.1268,  0.0061,  0.0075, -0.0343, -0.0498,\n",
       "                      -0.0752, -0.0519, -0.0891, -0.0091, -0.0286, -0.1373, -0.0539, -0.0403])),\n",
       "             ('f.c_bn.running_mean',\n",
       "              tensor([-0.2717,  0.3619, -0.2740,  0.1929, -0.1402, -0.2972,  0.5264,  0.1512,\n",
       "                      -0.1905, -0.0595,  0.0074, -0.2479, -0.0021, -0.2461,  0.2080,  0.0447,\n",
       "                       0.3849, -0.4578,  0.2608, -0.3761, -0.1989,  0.3002, -0.3516, -0.3877,\n",
       "                      -0.2930,  0.2105,  0.2511,  0.1147, -0.2767, -0.0972, -0.1811, -0.0736,\n",
       "                      -0.0915, -0.3013, -0.1996, -0.3085, -0.0652, -0.5935,  0.1251, -0.2020,\n",
       "                       0.2587, -0.2323,  0.3618, -0.4430,  0.0594,  0.0801,  0.1192, -0.3129,\n",
       "                       0.2669,  0.1306, -0.4827, -0.2979, -0.4709,  0.2484,  0.2706,  0.2780,\n",
       "                      -0.4431, -0.0569, -0.0506,  0.0319, -0.0104, -0.2382, -0.2791, -0.1162,\n",
       "                       0.0327, -0.0858,  0.1574, -0.0445, -0.7047, -0.4705, -0.3739, -0.0728,\n",
       "                      -0.1373,  0.0028, -0.0012,  0.0129, -0.1596, -0.2135, -0.3205, -0.1196,\n",
       "                      -0.0469, -0.0101, -0.4219,  0.2786, -0.4086, -0.2720, -0.0154, -0.4248,\n",
       "                       0.3531,  0.1419,  0.1054, -0.1367,  0.3428, -0.2910, -0.2211, -0.0353,\n",
       "                       0.2380, -0.1382, -0.2422, -0.5547, -0.2826,  0.2850, -0.1769,  0.0616,\n",
       "                      -0.0049, -0.1880,  0.2122, -0.1640,  0.1701, -0.0693, -0.3821,  0.1943,\n",
       "                       0.0862,  0.2130,  0.3064,  0.7388,  0.1974,  0.0597, -0.2865, -0.3144,\n",
       "                      -0.1565, -0.1882, -0.1461,  0.0055, -0.1977,  0.1561, -0.5930, -0.7424,\n",
       "                      -0.4314, -0.1939, -0.3042,  0.4795, -0.1176, -0.4351, -0.9600,  0.2103,\n",
       "                       0.0430, -0.0637, -0.2148,  0.0211,  0.0907, -0.2900,  0.4279, -0.2499])),\n",
       "             ('f.c_bn.running_var',\n",
       "              tensor([0.2106, 0.2179, 0.1807, 0.1673, 0.2445, 0.1542, 0.2263, 0.1516, 0.1870,\n",
       "                      0.2088, 0.2874, 0.2198, 0.2397, 0.2479, 0.1921, 0.2187, 0.2527, 0.2578,\n",
       "                      0.1704, 0.2186, 0.2430, 0.2058, 0.2125, 0.2115, 0.1559, 0.2177, 0.1860,\n",
       "                      0.2238, 0.1930, 0.2205, 0.2512, 0.2248, 0.2450, 0.2076, 0.2280, 0.1638,\n",
       "                      0.2214, 0.1786, 0.2041, 0.2714, 0.1211, 0.2450, 0.2150, 0.2424, 0.1456,\n",
       "                      0.1848, 0.1816, 0.2220, 0.2087, 0.1672, 0.2010, 0.2334, 0.2549, 0.2101,\n",
       "                      0.2160, 0.1631, 0.2117, 0.1701, 0.2081, 0.2184, 0.2031, 0.1915, 0.2020,\n",
       "                      0.1516, 0.2564, 0.2302, 0.1770, 0.2613, 0.2798, 0.1780, 0.2517, 0.1424,\n",
       "                      0.1761, 0.2814, 0.2048, 0.2947, 0.2303, 0.2530, 0.2299, 0.2276, 0.2207,\n",
       "                      0.2758, 0.3212, 0.1873, 0.2306, 0.1392, 0.2229, 0.2071, 0.1555, 0.2246,\n",
       "                      0.2150, 0.1957, 0.2006, 0.2744, 0.2154, 0.1729, 0.1666, 0.1794, 0.2608,\n",
       "                      0.2123, 0.1879, 0.1836, 0.2163, 0.1580, 0.2558, 0.2435, 0.3103, 0.1886,\n",
       "                      0.1871, 0.1706, 0.1827, 0.1814, 0.2179, 0.1640, 0.1949, 0.1766, 0.1549,\n",
       "                      0.3020, 0.1573, 0.1971, 0.2256, 0.2738, 0.2831, 0.2572, 0.2117, 0.2634,\n",
       "                      0.2228, 0.2320, 0.1663, 0.2297, 0.1478, 0.2503, 0.1820, 0.1781, 0.1808,\n",
       "                      0.2144, 0.2032, 0.1748, 0.1655, 0.1658, 0.2563, 0.2834, 0.1963, 0.2242])),\n",
       "             ('f.c_bn.num_batches_tracked', tensor(17199))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks_pool[\"144\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbc72274-dc26-40e7-853d-4b2aedc9628c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s2.b1.f.a.weight\n",
      "s2.b1.f.a_bn.weight\n",
      "s2.b1.f.a_bn.bias\n",
      "s2.b1.f.b.weight\n",
      "s2.b1.f.b_bn.weight\n",
      "s2.b1.f.b_bn.bias\n",
      "s2.b1.f.se.f_ex.0.weight\n",
      "s2.b1.f.se.f_ex.0.bias\n",
      "s2.b1.f.se.f_ex.2.weight\n",
      "s2.b1.f.se.f_ex.2.bias\n",
      "s2.b1.f.c.weight\n",
      "s2.b1.f.c_bn.weight\n",
      "s2.b1.f.c_bn.bias\n",
      "s2.b1.downsample.1.conv.weight\n",
      "s2.b1.downsample.1.bn.weight\n",
      "s2.b1.downsample.1.bn.bias\n",
      "s2.b2.f.a.weight\n",
      "s2.b2.f.a_bn.weight\n",
      "s2.b2.f.a_bn.bias\n",
      "s2.b2.f.b.weight\n",
      "s2.b2.f.b_bn.weight\n",
      "s2.b2.f.b_bn.bias\n",
      "s2.b2.f.se.f_ex.0.weight\n",
      "s2.b2.f.se.f_ex.0.bias\n",
      "s2.b2.f.se.f_ex.2.weight\n",
      "s2.b2.f.se.f_ex.2.bias\n",
      "s2.b2.f.c.weight\n",
      "s2.b2.f.c_bn.weight\n",
      "s2.b2.f.c_bn.bias\n",
      "s2.b3.f.a.weight\n",
      "s2.b3.f.a_bn.weight\n",
      "s2.b3.f.a_bn.bias\n",
      "s2.b3.f.b.weight\n",
      "s2.b3.f.b_bn.weight\n",
      "s2.b3.f.b_bn.bias\n",
      "s2.b3.f.se.f_ex.0.weight\n",
      "s2.b3.f.se.f_ex.0.bias\n",
      "s2.b3.f.se.f_ex.2.weight\n",
      "s2.b3.f.se.f_ex.2.bias\n",
      "s2.b3.f.c.weight\n",
      "s2.b3.f.c_bn.weight\n",
      "s2.b3.f.c_bn.bias\n",
      "s3.b1.f.a.weight\n",
      "s3.b1.f.a_bn.weight\n",
      "s3.b1.f.a_bn.bias\n",
      "s3.b1.f.b.weight\n",
      "s3.b1.f.b_bn.weight\n",
      "s3.b1.f.b_bn.bias\n",
      "s3.b1.f.se.f_ex.0.weight\n",
      "s3.b1.f.se.f_ex.0.bias\n",
      "s3.b1.f.se.f_ex.2.weight\n",
      "s3.b1.f.se.f_ex.2.bias\n",
      "s3.b1.f.c.weight\n",
      "s3.b1.f.c_bn.weight\n",
      "s3.b1.f.c_bn.bias\n",
      "s3.b1.downsample.1.conv.weight\n",
      "s3.b1.downsample.1.bn.weight\n",
      "s3.b1.downsample.1.bn.bias\n",
      "s3.b2.f.a.weight\n",
      "s3.b2.f.a_bn.weight\n",
      "s3.b2.f.a_bn.bias\n",
      "s3.b2.f.b.weight\n",
      "s3.b2.f.b_bn.weight\n",
      "s3.b2.f.b_bn.bias\n",
      "s3.b2.f.se.f_ex.0.weight\n",
      "s3.b2.f.se.f_ex.0.bias\n",
      "s3.b2.f.se.f_ex.2.weight\n",
      "s3.b2.f.se.f_ex.2.bias\n",
      "s3.b2.f.c.weight\n",
      "s3.b2.f.c_bn.weight\n",
      "s3.b2.f.c_bn.bias\n",
      "s3.b3.f.a.weight\n",
      "s3.b3.f.a_bn.weight\n",
      "s3.b3.f.a_bn.bias\n",
      "s3.b3.f.b.weight\n",
      "s3.b3.f.b_bn.weight\n",
      "s3.b3.f.b_bn.bias\n",
      "s3.b3.f.se.f_ex.0.weight\n",
      "s3.b3.f.se.f_ex.0.bias\n",
      "s3.b3.f.se.f_ex.2.weight\n",
      "s3.b3.f.se.f_ex.2.bias\n",
      "s3.b3.f.c.weight\n",
      "s3.b3.f.c_bn.weight\n",
      "s3.b3.f.c_bn.bias\n",
      "s3.b4.f.a.weight\n",
      "s3.b4.f.a_bn.weight\n",
      "s3.b4.f.a_bn.bias\n",
      "s3.b4.f.b.weight\n",
      "s3.b4.f.b_bn.weight\n",
      "s3.b4.f.b_bn.bias\n",
      "s3.b4.f.se.f_ex.0.weight\n",
      "s3.b4.f.se.f_ex.0.bias\n",
      "s3.b4.f.se.f_ex.2.weight\n",
      "s3.b4.f.se.f_ex.2.bias\n",
      "s3.b4.f.c.weight\n",
      "s3.b4.f.c_bn.weight\n",
      "s3.b4.f.c_bn.bias\n",
      "s3.b5.f.a.weight\n",
      "s3.b5.f.a_bn.weight\n",
      "s3.b5.f.a_bn.bias\n",
      "s3.b5.f.b.weight\n",
      "s3.b5.f.b_bn.weight\n",
      "s3.b5.f.b_bn.bias\n",
      "s3.b5.f.se.f_ex.0.weight\n",
      "s3.b5.f.se.f_ex.0.bias\n",
      "s3.b5.f.se.f_ex.2.weight\n",
      "s3.b5.f.se.f_ex.2.bias\n",
      "s3.b5.f.c.weight\n",
      "s3.b5.f.c_bn.weight\n",
      "s3.b5.f.c_bn.bias\n",
      "s3.b6.f.a.weight\n",
      "s3.b6.f.a_bn.weight\n",
      "s3.b6.f.a_bn.bias\n",
      "s3.b6.f.b.weight\n",
      "s3.b6.f.b_bn.weight\n",
      "s3.b6.f.b_bn.bias\n",
      "s3.b6.f.se.f_ex.0.weight\n",
      "s3.b6.f.se.f_ex.0.bias\n",
      "s3.b6.f.se.f_ex.2.weight\n",
      "s3.b6.f.se.f_ex.2.bias\n",
      "s3.b6.f.c.weight\n",
      "s3.b6.f.c_bn.weight\n",
      "s3.b6.f.c_bn.bias\n",
      "s3.b7.f.a.weight\n",
      "s3.b7.f.a_bn.weight\n",
      "s3.b7.f.a_bn.bias\n",
      "s3.b7.f.b.weight\n",
      "s3.b7.f.b_bn.weight\n",
      "s3.b7.f.b_bn.bias\n",
      "s3.b7.f.se.f_ex.0.weight\n",
      "s3.b7.f.se.f_ex.0.bias\n",
      "s3.b7.f.se.f_ex.2.weight\n",
      "s3.b7.f.se.f_ex.2.bias\n",
      "s3.b7.f.c.weight\n",
      "s3.b7.f.c_bn.weight\n",
      "s3.b7.f.c_bn.bias\n",
      "s3.b8.f.a.weight\n",
      "s3.b8.f.a_bn.weight\n",
      "s3.b8.f.a_bn.bias\n",
      "s3.b8.f.b.weight\n",
      "s3.b8.f.b_bn.weight\n",
      "s3.b8.f.b_bn.bias\n",
      "s3.b8.f.se.f_ex.0.weight\n",
      "s3.b8.f.se.f_ex.0.bias\n",
      "s3.b8.f.se.f_ex.2.weight\n",
      "s3.b8.f.se.f_ex.2.bias\n",
      "s3.b8.f.c.weight\n",
      "s3.b8.f.c_bn.weight\n",
      "s3.b8.f.c_bn.bias\n",
      "s4.b1.f.a.weight\n",
      "s4.b1.f.a_bn.weight\n",
      "s4.b1.f.a_bn.bias\n",
      "s4.b1.f.b.weight\n",
      "s4.b1.f.b_bn.weight\n",
      "s4.b1.f.b_bn.bias\n",
      "s4.b1.f.se.f_ex.0.weight\n",
      "s4.b1.f.se.f_ex.0.bias\n",
      "s4.b1.f.se.f_ex.2.weight\n",
      "s4.b1.f.se.f_ex.2.bias\n",
      "s4.b1.f.c.weight\n",
      "s4.b1.f.c_bn.weight\n",
      "s4.b1.f.c_bn.bias\n",
      "s4.b1.downsample.1.conv.weight\n",
      "s4.b1.downsample.1.bn.weight\n",
      "s4.b1.downsample.1.bn.bias\n",
      "s4.b2.f.a.weight\n",
      "s4.b2.f.a_bn.weight\n",
      "s4.b2.f.a_bn.bias\n",
      "s4.b2.f.b.weight\n",
      "s4.b2.f.b_bn.weight\n",
      "s4.b2.f.b_bn.bias\n",
      "s4.b2.f.se.f_ex.0.weight\n",
      "s4.b2.f.se.f_ex.0.bias\n",
      "s4.b2.f.se.f_ex.2.weight\n",
      "s4.b2.f.se.f_ex.2.bias\n",
      "s4.b2.f.c.weight\n",
      "s4.b2.f.c_bn.weight\n",
      "s4.b2.f.c_bn.bias\n",
      "s4.b3.f.a.weight\n",
      "s4.b3.f.a_bn.weight\n",
      "s4.b3.f.a_bn.bias\n",
      "s4.b3.f.b.weight\n",
      "s4.b3.f.b_bn.weight\n",
      "s4.b3.f.b_bn.bias\n",
      "s4.b3.f.se.f_ex.0.weight\n",
      "s4.b3.f.se.f_ex.0.bias\n",
      "s4.b3.f.se.f_ex.2.weight\n",
      "s4.b3.f.se.f_ex.2.bias\n",
      "s4.b3.f.c.weight\n",
      "s4.b3.f.c_bn.weight\n",
      "s4.b3.f.c_bn.bias\n",
      "s4.b4.f.a.weight\n",
      "s4.b4.f.a_bn.weight\n",
      "s4.b4.f.a_bn.bias\n",
      "s4.b4.f.b.weight\n",
      "s4.b4.f.b_bn.weight\n",
      "s4.b4.f.b_bn.bias\n",
      "s4.b4.f.se.f_ex.0.weight\n",
      "s4.b4.f.se.f_ex.0.bias\n",
      "s4.b4.f.se.f_ex.2.weight\n",
      "s4.b4.f.se.f_ex.2.bias\n",
      "s4.b4.f.c.weight\n",
      "s4.b4.f.c_bn.weight\n",
      "s4.b4.f.c_bn.bias\n",
      "s4.b5.f.a.weight\n",
      "s4.b5.f.a_bn.weight\n",
      "s4.b5.f.a_bn.bias\n",
      "s4.b5.f.b.weight\n",
      "s4.b5.f.b_bn.weight\n",
      "s4.b5.f.b_bn.bias\n",
      "s4.b5.f.se.f_ex.0.weight\n",
      "s4.b5.f.se.f_ex.0.bias\n",
      "s4.b5.f.se.f_ex.2.weight\n",
      "s4.b5.f.se.f_ex.2.bias\n",
      "s4.b5.f.c.weight\n",
      "s4.b5.f.c_bn.weight\n",
      "s4.b5.f.c_bn.bias\n",
      "s4.b6.f.a.weight\n",
      "s4.b6.f.a_bn.weight\n",
      "s4.b6.f.a_bn.bias\n",
      "s4.b6.f.b.weight\n",
      "s4.b6.f.b_bn.weight\n",
      "s4.b6.f.b_bn.bias\n",
      "s4.b6.f.se.f_ex.0.weight\n",
      "s4.b6.f.se.f_ex.0.bias\n",
      "s4.b6.f.se.f_ex.2.weight\n",
      "s4.b6.f.se.f_ex.2.bias\n",
      "s4.b6.f.c.weight\n",
      "s4.b6.f.c_bn.weight\n",
      "s4.b6.f.c_bn.bias\n",
      "s4.b7.f.a.weight\n",
      "s4.b7.f.a_bn.weight\n",
      "s4.b7.f.a_bn.bias\n",
      "s4.b7.f.b.weight\n",
      "s4.b7.f.b_bn.weight\n",
      "s4.b7.f.b_bn.bias\n",
      "s4.b7.f.se.f_ex.0.weight\n",
      "s4.b7.f.se.f_ex.0.bias\n",
      "s4.b7.f.se.f_ex.2.weight\n",
      "s4.b7.f.se.f_ex.2.bias\n",
      "s4.b7.f.c.weight\n",
      "s4.b7.f.c_bn.weight\n",
      "s4.b7.f.c_bn.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in models['rampant_myna'].named_parameters():\n",
    "    if any(substring in name for substring in ['s2', 's3', 's4']):  # Adjust this condition based on your model\n",
    "        param.requires_grad = False\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5878274a-8692-4a3b-a415-c03b44e60acd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['abiding_markhor'].s2.b1.load_state_dict(blocks_pool[\"64-144\"])\n",
    "models['abiding_markhor'].s2.b2.load_state_dict(blocks_pool[\"144\"])\n",
    "models['abiding_markhor'].s2.b3.load_state_dict(blocks_pool[\"144\"])\n",
    "models['abiding_markhor'].s3.b1.load_state_dict(blocks_pool[\"144-336\"])\n",
    "models['abiding_markhor'].s3.b2.load_state_dict(blocks_pool[\"336\"])\n",
    "models['abiding_markhor'].s3.b3.load_state_dict(blocks_pool[\"336\"])\n",
    "models['abiding_markhor'].s3.b4.load_state_dict(blocks_pool[\"336\"])\n",
    "models['abiding_markhor'].s3.b5.load_state_dict(blocks_pool[\"336\"])\n",
    "models['abiding_markhor'].s3.b6.load_state_dict(blocks_pool[\"336\"])\n",
    "models['abiding_markhor'].s3.b7.load_state_dict(blocks_pool[\"336\"])\n",
    "models['abiding_markhor'].s3.b8.load_state_dict(blocks_pool[\"336\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4249fb-e430-4f74-af0f-6fc30c4a405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=models[model_name]\n",
    "\n",
    "models[\"frisky_koala_9\"].s1.load_state_dict(trained_model.s1.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a427f339-6228-43c8-aee9-ed04650e39c9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emerald_aardwark',\n",
       " 'onyx_gaur',\n",
       " 'rampant_myna',\n",
       " 'succinct_chimpanzee',\n",
       " 'red_caterpillar',\n",
       " 'thistle_owl',\n",
       " 'mahogany_collie',\n",
       " 'quizzical_caiman',\n",
       " 'cinnamon_dove',\n",
       " 'mature_hare',\n",
       " 'silky_dachshund',\n",
       " 'crystal_fennec',\n",
       " 'chubby_sawfish',\n",
       " 'amaranth_ringtail',\n",
       " 'mahogany_raven',\n",
       " 'cerulean_prawn',\n",
       " 'viridian_muskrat',\n",
       " 'original_reindeer',\n",
       " 'kind_armadillo',\n",
       " 'gifted_goldfish',\n",
       " 'roaring_snake',\n",
       " 'optimal_wildebeest',\n",
       " 'portable_griffin',\n",
       " 'outrageous_flounder',\n",
       " 'vehement_lizard',\n",
       " 'shrewd_muskrat',\n",
       " 'encouraging_whippet',\n",
       " 'eggplant_partridge',\n",
       " 'lavender_agama',\n",
       " 'sexy_skylark',\n",
       " 'brainy_cobra',\n",
       " 'delectable_crab',\n",
       " 'raspberry_bullfinch',\n",
       " 'red_pudu',\n",
       " 'pristine_woodpecker',\n",
       " 'awesome_wombat',\n",
       " 'red_bullmastiff',\n",
       " 'sly_saluki',\n",
       " 'denim_butterfly',\n",
       " 'white_cow',\n",
       " 'bulky_coati',\n",
       " 'helpful_ermine',\n",
       " 'monumental_trout',\n",
       " 'hypersonic_porcupine',\n",
       " 'carrot_bison',\n",
       " 'vivacious_beetle',\n",
       " 'tangerine_beaver',\n",
       " 'nostalgic_bison',\n",
       " 'vigilant_ibex',\n",
       " 'massive_jerboa',\n",
       " 'prompt_heron',\n",
       " 'powerful_chupacabra',\n",
       " 'mighty_crane',\n",
       " 'tourmaline_jacamar',\n",
       " 'chirpy_swallow',\n",
       " 'frisky_clam',\n",
       " 'loutish_snail',\n",
       " 'remarkable_seahorse',\n",
       " 'visionary_adder',\n",
       " 'interesting_dormouse',\n",
       " 'quartz_dove',\n",
       " 'miniature_sawfly',\n",
       " 'cerulean_emu',\n",
       " 'pygmy_waxbill',\n",
       " 'crystal_swallow',\n",
       " 'overjoyed_caracal',\n",
       " 'dark_goat',\n",
       " 'glittering_ostrich',\n",
       " 'proficient_duck',\n",
       " 'beneficial_axolotl',\n",
       " 'macho_oxpecker',\n",
       " 'carmine_chinchilla',\n",
       " 'pistachio_mouflon',\n",
       " 'unnatural_rook',\n",
       " 'discerning_urchin',\n",
       " 'hulking_wombat',\n",
       " 'hulking_bee',\n",
       " 'abiding_narwhal',\n",
       " 'courageous_puffin',\n",
       " 'overjoyed_guillemot',\n",
       " 'woodoo_herring',\n",
       " 'mustard_marten',\n",
       " 'abiding_markhor',\n",
       " 'elegant_vicugna',\n",
       " 'rainbow_smilodon',\n",
       " 'hot_sunfish',\n",
       " 'micro_lion',\n",
       " 'industrious_whippet',\n",
       " 'tidy_seal',\n",
       " 'rampant_sunfish',\n",
       " 'magenta_mink',\n",
       " 'hot_pogona',\n",
       " 'gorgeous_goose',\n",
       " 'tentacled_squid',\n",
       " 'refined_panda',\n",
       " 'impartial_bullfinch',\n",
       " 'illustrious_taipan',\n",
       " 'fearless_vulture',\n",
       " 'fanatic_magpie',\n",
       " 'logical_salmon',\n",
       " 'onyx_groundhog',\n",
       " 'merciful_elephant',\n",
       " 'lurking_okapi',\n",
       " 'athletic_dogfish',\n",
       " 'zippy_curassow',\n",
       " 'therapeutic_spoonbill',\n",
       " 'tungsten_nautilus',\n",
       " 'athletic_manatee',\n",
       " 'talented_cuscus',\n",
       " 'denim_hippo',\n",
       " 'lurking_chamois',\n",
       " 'primitive_harrier',\n",
       " 'dainty_rattlesnake',\n",
       " 'debonair_bloodhound',\n",
       " 'tall_koel',\n",
       " 'cyan_tench',\n",
       " 'finicky_armadillo',\n",
       " 'nocturnal_scorpion',\n",
       " 'uptight_dugong',\n",
       " 'striped_dalmatian']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(chromosomes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b3ae0f4-a804-4fb5-af18-7887914aa3b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[\"abiding_markhor\"].load_state_dict(filtered_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebbf626d-cc5e-447b-b04e-d4fc79d1af5f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResBottleneckBlock(\n",
       "  (f): BottleneckTransform(\n",
       "    (a): Conv2d(288, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (a_bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (a_af): ReLU(inplace=True)\n",
       "    (b): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36, bias=False)\n",
       "    (b_bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (b_af): ReLU(inplace=True)\n",
       "    (se): SE(\n",
       "      (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (f_ex): Sequential(\n",
       "        (0): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (c): Conv2d(288, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (c_bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (af): ReLU(inplace=True)\n",
       "  (downsample): Identity()\n",
       "  (drop_path): DropPath()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[\"frisky_koala_9\"].stem.load_state_dict(trained_model.stem.state_dict())\n",
    "models[\"frisky_koala_9\"].s1.load_state_dict(trained_model.s1.state_dict())\n",
    "models[\"frisky_koala_9\"].s2.load_state_dict(trained_model.s2.state_dict())\n",
    "\n",
    "models[\"frisky_koala_9\"].s3.b1.load_state_dict(trained_model.s3.b1.state_dict())\n",
    "models[\"frisky_koala_9\"].s3.b2.load_state_dict(trained_model.s3.b2.state_dict())\n",
    "models[\"frisky_koala_9\"].s3.b3.load_state_dict(trained_model.s3.b3.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e10e0a5a-4dbc-47ae-8176-2505ee000e06",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'emerald_aardwark': {'ws': [112, 288],\n",
       "  'bs': [1.0, 1.0],\n",
       "  'gs': [8, 8],\n",
       "  'ds': [3, 8],\n",
       "  'num_stages': 2,\n",
       "  'total_size_mb': 7.304210662841797,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 329559,\n",
       "  'params': 1914755,\n",
       "  'acts': 3255,\n",
       "  'WA': 24.0,\n",
       "  'W0': 112,\n",
       "  'WM': 2.599999999999998,\n",
       "  'DEPTH': 11,\n",
       "  'GROUP_W': 8},\n",
       " 'onyx_gaur': {'ws': [96, 200, 400, 824],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [2, 4, 10, 3],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 37.75217819213867,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 1704959,\n",
       "  'params': 9896507,\n",
       "  'acts': 9151,\n",
       "  'WA': 32.0,\n",
       "  'W0': 96,\n",
       "  'WM': 2.05,\n",
       "  'DEPTH': 19,\n",
       "  'GROUP_W': 8},\n",
       " 'rampant_myna': {'ws': [64, 168, 448, 1192],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 3, 8, 7],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 111.36352920532227,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 5343381,\n",
       "  'params': 29193281,\n",
       "  'acts': 15349,\n",
       "  'WA': 56.0,\n",
       "  'W0': 64,\n",
       "  'WM': 2.6499999999999977,\n",
       "  'DEPTH': 19,\n",
       "  'GROUP_W': 8},\n",
       " 'succinct_chimpanzee': {'ws': [112, 320],\n",
       "  'bs': [1.0, 1.0],\n",
       "  'gs': [8, 8],\n",
       "  'ds': [5, 10],\n",
       "  'num_stages': 2,\n",
       "  'total_size_mb': 11.245067596435547,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 516423,\n",
       "  'params': 2947827,\n",
       "  'acts': 4647,\n",
       "  'WA': 16.0,\n",
       "  'W0': 112,\n",
       "  'WM': 2.849999999999997,\n",
       "  'DEPTH': 15,\n",
       "  'GROUP_W': 8},\n",
       " 'red_caterpillar': {'ws': [112, 288, 760],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 4, 11],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 65.58210372924805,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 3173031,\n",
       "  'params': 17191955,\n",
       "  'acts': 12007,\n",
       "  'WA': 64.0,\n",
       "  'W0': 112,\n",
       "  'WM': 2.599999999999998,\n",
       "  'DEPTH': 17,\n",
       "  'GROUP_W': 8},\n",
       " 'thistle_owl': {'ws': [56, 120, 248, 520],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 2, 3, 7],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 21.172969818115234,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 979555,\n",
       "  'params': 5550367,\n",
       "  'acts': 5747,\n",
       "  'WA': 56.0,\n",
       "  'W0': 56,\n",
       "  'WM': 2.0999999999999996,\n",
       "  'DEPTH': 13,\n",
       "  'GROUP_W': 8},\n",
       " 'mahogany_collie': {'ws': [80, 200, 520],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 6, 5],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 15.903240203857422,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 720367,\n",
       "  'params': 4168939,\n",
       "  'acts': 4847,\n",
       "  'WA': 32.0,\n",
       "  'W0': 80,\n",
       "  'WM': 2.549999999999998,\n",
       "  'DEPTH': 13,\n",
       "  'GROUP_W': 8},\n",
       " 'quizzical_caiman': {'ws': [96, 256, 672],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 5, 7],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 34.26491928100586,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 1611099,\n",
       "  'params': 8982343,\n",
       "  'acts': 7579,\n",
       "  'WA': 48.0,\n",
       "  'W0': 96,\n",
       "  'WM': 2.6499999999999977,\n",
       "  'DEPTH': 14,\n",
       "  'GROUP_W': 8},\n",
       " 'cinnamon_dove': {'ws': [56, 128, 296, 680],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 3, 6, 4],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 23.81203842163086,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 1072603,\n",
       "  'params': 6242183,\n",
       "  'acts': 6027,\n",
       "  'WA': 40.0,\n",
       "  'W0': 56,\n",
       "  'WM': 2.299999999999999,\n",
       "  'DEPTH': 14,\n",
       "  'GROUP_W': 8},\n",
       " 'mature_hare': {'ws': [104, 248, 576],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [3, 9, 10],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 39.11689376831055,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 1857383,\n",
       "  'params': 10254259,\n",
       "  'acts': 10263,\n",
       "  'WA': 24.0,\n",
       "  'W0': 104,\n",
       "  'WM': 2.3499999999999988,\n",
       "  'DEPTH': 22,\n",
       "  'GROUP_W': 8},\n",
       " 'silky_dachshund': {'ws': [40, 88, 184],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 4, 2],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 1.1844520568847656,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 44421,\n",
       "  'params': 310497,\n",
       "  'acts': 981,\n",
       "  'WA': 16.0,\n",
       "  'W0': 40,\n",
       "  'WM': 2.1499999999999995,\n",
       "  'DEPTH': 8,\n",
       "  'GROUP_W': 8},\n",
       " 'crystal_fennec': {'ws': [64, 144, 312],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 5, 2],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 3.2336158752441406,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 128189,\n",
       "  'params': 847673,\n",
       "  'acts': 1789,\n",
       "  'WA': 24.0,\n",
       "  'W0': 64,\n",
       "  'WM': 2.1999999999999993,\n",
       "  'DEPTH': 9,\n",
       "  'GROUP_W': 8},\n",
       " 'chubby_sawfish': {'ws': [88, 232, 616, 1640],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 5, 11, 2],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 90.61266708374023,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 3984291,\n",
       "  'params': 23753567,\n",
       "  'acts': 13747,\n",
       "  'WA': 56.0,\n",
       "  'W0': 88,\n",
       "  'WM': 2.6499999999999977,\n",
       "  'DEPTH': 19,\n",
       "  'GROUP_W': 8},\n",
       " 'amaranth_ringtail': {'ws': [24, 48, 104, 224, 464],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8, 8],\n",
       "  'ds': [1, 2, 3, 7, 3],\n",
       "  'num_stages': 5,\n",
       "  'total_size_mb': 10.619335174560547,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 458247,\n",
       "  'params': 2783795,\n",
       "  'acts': 4151,\n",
       "  'WA': 24.0,\n",
       "  'W0': 24,\n",
       "  'WM': 2.0999999999999996,\n",
       "  'DEPTH': 16,\n",
       "  'GROUP_W': 8},\n",
       " 'mahogany_raven': {'ws': [80, 160, 336, 688],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 2, 4, 7],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 37.8260612487793,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 1773975,\n",
       "  'params': 9915875,\n",
       "  'acts': 8055,\n",
       "  'WA': 64.0,\n",
       "  'W0': 80,\n",
       "  'WM': 2.05,\n",
       "  'DEPTH': 14,\n",
       "  'GROUP_W': 8},\n",
       " 'cerulean_prawn': {'ws': [64, 176, 504],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 4, 4],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 11.173831939697266,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 493957,\n",
       "  'params': 2929153,\n",
       "  'acts': 3461,\n",
       "  'WA': 40.0,\n",
       "  'W0': 64,\n",
       "  'WM': 2.799999999999997,\n",
       "  'DEPTH': 10,\n",
       "  'GROUP_W': 8},\n",
       " 'viridian_muskrat': {'ws': [96, 240],\n",
       "  'bs': [1.0, 1.0],\n",
       "  'gs': [8, 8],\n",
       "  'ds': [4, 7],\n",
       "  'num_stages': 2,\n",
       "  'total_size_mb': 4.707637786865234,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 206835,\n",
       "  'params': 1234079,\n",
       "  'acts': 2547,\n",
       "  'WA': 16.0,\n",
       "  'W0': 96,\n",
       "  'WM': 2.4999999999999982,\n",
       "  'DEPTH': 11,\n",
       "  'GROUP_W': 8},\n",
       " 'original_reindeer': {'ws': [96, 248, 624],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [3, 10, 6],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 29.53152084350586,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 1370539,\n",
       "  'params': 7741511,\n",
       "  'acts': 8011,\n",
       "  'WA': 24.0,\n",
       "  'W0': 96,\n",
       "  'WM': 2.549999999999998,\n",
       "  'DEPTH': 19,\n",
       "  'GROUP_W': 8},\n",
       " 'kind_armadillo': {'ws': [96, 224, 528, 1248],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [2, 4, 9, 4],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 85.61567306518555,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 3970183,\n",
       "  'params': 22443635,\n",
       "  'acts': 13255,\n",
       "  'WA': 48.0,\n",
       "  'W0': 96,\n",
       "  'WM': 2.3499999999999988,\n",
       "  'DEPTH': 19,\n",
       "  'GROUP_W': 8},\n",
       " 'gifted_goldfish': {'ws': [48, 120, 312],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 5, 9],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 9.947589874267578,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 452169,\n",
       "  'params': 2607701,\n",
       "  'acts': 4329,\n",
       "  'WA': 24.0,\n",
       "  'W0': 48,\n",
       "  'WM': 2.549999999999998,\n",
       "  'DEPTH': 16,\n",
       "  'GROUP_W': 8},\n",
       " 'roaring_snake': {'ws': [112, 232, 472],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [1, 3, 4],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 10.685588836669922,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 470031,\n",
       "  'params': 2801163,\n",
       "  'acts': 3279,\n",
       "  'WA': 56.0,\n",
       "  'W0': 112,\n",
       "  'WM': 2.05,\n",
       "  'DEPTH': 8,\n",
       "  'GROUP_W': 8},\n",
       " 'optimal_wildebeest': {'ws': [96, 272, 776],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 4, 11],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 67.57258224487305,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 3274023,\n",
       "  'params': 17713747,\n",
       "  'acts': 12103,\n",
       "  'WA': 64.0,\n",
       "  'W0': 96,\n",
       "  'WM': 2.849999999999997,\n",
       "  'DEPTH': 17,\n",
       "  'GROUP_W': 8},\n",
       " 'portable_griffin': {'ws': [48, 104, 208, 448],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 2, 3, 6],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 13.756542205810547,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 624343,\n",
       "  'params': 3606195,\n",
       "  'acts': 4375,\n",
       "  'WA': 48.0,\n",
       "  'W0': 48,\n",
       "  'WM': 2.0999999999999996,\n",
       "  'DEPTH': 12,\n",
       "  'GROUP_W': 8},\n",
       " 'outrageous_flounder': {'ws': [80, 168, 352],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 4, 7],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 10.30733871459961,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 465755,\n",
       "  'params': 2702007,\n",
       "  'acts': 4059,\n",
       "  'WA': 32.0,\n",
       "  'W0': 80,\n",
       "  'WM': 2.0999999999999996,\n",
       "  'DEPTH': 13,\n",
       "  'GROUP_W': 8},\n",
       " 'vehement_lizard': {'ws': [64, 128, 272, 552, 1128],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8, 8],\n",
       "  'ds': [1, 2, 4, 9, 1],\n",
       "  'num_stages': 5,\n",
       "  'total_size_mb': 42.066837310791016,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 1778469,\n",
       "  'params': 11027569,\n",
       "  'acts': 9125,\n",
       "  'WA': 48.0,\n",
       "  'W0': 64,\n",
       "  'WM': 2.05,\n",
       "  'DEPTH': 17,\n",
       "  'GROUP_W': 8},\n",
       " 'shrewd_muskrat': {'ws': [120, 296],\n",
       "  'bs': [1.0, 1.0],\n",
       "  'gs': [8, 8],\n",
       "  'ds': [5, 17],\n",
       "  'num_stages': 2,\n",
       "  'total_size_mb': 16.409717559814453,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 762081,\n",
       "  'params': 4301709,\n",
       "  'acts': 6993,\n",
       "  'WA': 16.0,\n",
       "  'W0': 120,\n",
       "  'WM': 2.4499999999999984,\n",
       "  'DEPTH': 22,\n",
       "  'GROUP_W': 8},\n",
       " 'encouraging_whippet': {'ws': [80, 200, 496],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [1, 4, 7],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 18.819194793701172,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 872031,\n",
       "  'params': 4933339,\n",
       "  'acts': 5343,\n",
       "  'WA': 48.0,\n",
       "  'W0': 80,\n",
       "  'WM': 2.4999999999999982,\n",
       "  'DEPTH': 12,\n",
       "  'GROUP_W': 8},\n",
       " 'eggplant_partridge': {'ws': [72, 144, 304, 624],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 3, 6, 7],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 33.44719314575195,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 1563553,\n",
       "  'params': 8767981,\n",
       "  'acts': 8241,\n",
       "  'WA': 40.0,\n",
       "  'W0': 72,\n",
       "  'WM': 2.05,\n",
       "  'DEPTH': 17,\n",
       "  'GROUP_W': 8},\n",
       " 'lavender_agama': {'ws': [104, 240, 552, 1264],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 4, 7, 9],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 161.7229881286621,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 7865707,\n",
       "  'params': 42394711,\n",
       "  'acts': 20091,\n",
       "  'WA': 64.0,\n",
       "  'W0': 104,\n",
       "  'WM': 2.299999999999999,\n",
       "  'DEPTH': 21,\n",
       "  'GROUP_W': 8},\n",
       " 'sexy_skylark': {'ws': [64, 176, 464],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 8, 8],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 19.967967987060547,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 927111,\n",
       "  'params': 5234483,\n",
       "  'acts': 6471,\n",
       "  'WA': 24.0,\n",
       "  'W0': 64,\n",
       "  'WM': 2.6999999999999975,\n",
       "  'DEPTH': 18,\n",
       "  'GROUP_W': 8},\n",
       " 'brainy_cobra': {'ws': [112, 280],\n",
       "  'bs': [1.0, 1.0],\n",
       "  'gs': [8, 8],\n",
       "  'ds': [2, 7],\n",
       "  'num_stages': 2,\n",
       "  'total_size_mb': 5.969158172607422,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 267007,\n",
       "  'params': 1564779,\n",
       "  'acts': 2687,\n",
       "  'WA': 40.0,\n",
       "  'W0': 112,\n",
       "  'WM': 2.4999999999999982,\n",
       "  'DEPTH': 9,\n",
       "  'GROUP_W': 8},\n",
       " 'delectable_crab': {'ws': [48, 120, 312, 792],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 3, 8, 8],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 57.122676849365234,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 2723571,\n",
       "  'params': 14974367,\n",
       "  'acts': 11379,\n",
       "  'WA': 40.0,\n",
       "  'W0': 48,\n",
       "  'WM': 2.549999999999998,\n",
       "  'DEPTH': 20,\n",
       "  'GROUP_W': 8},\n",
       " 'raspberry_bullfinch': {'ws': [40, 96, 240, 592],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 2, 6, 8],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 31.442798614501953,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 1480001,\n",
       "  'params': 8242541,\n",
       "  'acts': 7889,\n",
       "  'WA': 40.0,\n",
       "  'W0': 40,\n",
       "  'WM': 2.4499999999999984,\n",
       "  'DEPTH': 17,\n",
       "  'GROUP_W': 8},\n",
       " 'red_pudu': {'ws': [48, 128, 328],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [1, 4, 5],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 6.223094940185547,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 273735,\n",
       "  'params': 1631347,\n",
       "  'acts': 2695,\n",
       "  'WA': 32.0,\n",
       "  'W0': 48,\n",
       "  'WM': 2.599999999999998,\n",
       "  'DEPTH': 10,\n",
       "  'GROUP_W': 8},\n",
       " 'pristine_woodpecker': {'ws': [80, 216, 584],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 5, 2],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 8.76784896850586,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 354443,\n",
       "  'params': 2298439,\n",
       "  'acts': 2891,\n",
       "  'WA': 40.0,\n",
       "  'W0': 80,\n",
       "  'WM': 2.6999999999999975,\n",
       "  'DEPTH': 9,\n",
       "  'GROUP_W': 8},\n",
       " 'awesome_wombat': {'ws': [56, 152, 392, 1040],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 3, 9, 9],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 108.19348526000977,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 5240149,\n",
       "  'params': 28362273,\n",
       "  'acts': 16517,\n",
       "  'WA': 48.0,\n",
       "  'W0': 56,\n",
       "  'WM': 2.6499999999999977,\n",
       "  'DEPTH': 22,\n",
       "  'GROUP_W': 8},\n",
       " 'red_bullmastiff': {'ws': [64, 176, 480],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [1, 4, 6],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 14.963130950927734,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 685299,\n",
       "  'params': 3922495,\n",
       "  'acts': 4467,\n",
       "  'WA': 48.0,\n",
       "  'W0': 64,\n",
       "  'WM': 2.7499999999999973,\n",
       "  'DEPTH': 11,\n",
       "  'GROUP_W': 8},\n",
       " 'sly_saluki': {'ws': [104, 248, 600, 1440],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 4, 8, 1],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 47.631839752197266,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 1910645,\n",
       "  'params': 12486401,\n",
       "  'acts': 8837,\n",
       "  'WA': 64.0,\n",
       "  'W0': 104,\n",
       "  'WM': 2.3999999999999986,\n",
       "  'DEPTH': 14,\n",
       "  'GROUP_W': 8},\n",
       " 'denim_butterfly': {'ws': [112, 240, 520],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 6, 7],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 22.766643524169922,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 1055807,\n",
       "  'params': 5968139,\n",
       "  'acts': 6527,\n",
       "  'WA': 32.0,\n",
       "  'W0': 112,\n",
       "  'WM': 2.1499999999999995,\n",
       "  'DEPTH': 15,\n",
       "  'GROUP_W': 8},\n",
       " 'white_cow': {'ws': [88, 256, 744],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 9, 6],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 38.08119583129883,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 1780985,\n",
       "  'params': 9982757,\n",
       "  'acts': 8521,\n",
       "  'WA': 32.0,\n",
       "  'W0': 88,\n",
       "  'WM': 2.899999999999997,\n",
       "  'DEPTH': 17,\n",
       "  'GROUP_W': 8},\n",
       " 'bulky_coati': {'ws': [80, 224, 624],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 6, 14],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 57.10697555541992,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 2764703,\n",
       "  'params': 14970251,\n",
       "  'acts': 12671,\n",
       "  'WA': 40.0,\n",
       "  'W0': 80,\n",
       "  'WM': 2.799999999999997,\n",
       "  'DEPTH': 22,\n",
       "  'GROUP_W': 8},\n",
       " 'helpful_ermine': {'ws': [72, 200, 568, 1584],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 4, 11, 2],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 79.88631820678711,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 3486683,\n",
       "  'params': 20941719,\n",
       "  'acts': 12491,\n",
       "  'WA': 56.0,\n",
       "  'W0': 72,\n",
       "  'WM': 2.799999999999997,\n",
       "  'DEPTH': 18,\n",
       "  'GROUP_W': 8},\n",
       " 'monumental_trout': {'ws': [72, 160, 352, 768],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 2, 4, 2],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 16.578372955322266,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 683029,\n",
       "  'params': 4345921,\n",
       "  'acts': 4005,\n",
       "  'WA': 64.0,\n",
       "  'W0': 72,\n",
       "  'WM': 2.1999999999999993,\n",
       "  'DEPTH': 9,\n",
       "  'GROUP_W': 8},\n",
       " 'hypersonic_porcupine': {'ws': [120, 312, 808],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [3, 9, 5],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 40.568843841552734,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 1880947,\n",
       "  'params': 10634879,\n",
       "  'acts': 8835,\n",
       "  'WA': 32.0,\n",
       "  'W0': 120,\n",
       "  'WM': 2.599999999999998,\n",
       "  'DEPTH': 17,\n",
       "  'GROUP_W': 8},\n",
       " 'carrot_bison': {'ws': [88, 248],\n",
       "  'bs': [1.0, 1.0],\n",
       "  'gs': [8, 8],\n",
       "  'ds': [4, 11],\n",
       "  'num_stages': 2,\n",
       "  'total_size_mb': 7.538402557373047,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 340055,\n",
       "  'params': 1976147,\n",
       "  'acts': 3815,\n",
       "  'WA': 16.0,\n",
       "  'W0': 88,\n",
       "  'WM': 2.799999999999997,\n",
       "  'DEPTH': 15,\n",
       "  'GROUP_W': 8},\n",
       " 'vivacious_beetle': {'ws': [96, 248],\n",
       "  'bs': [1.0, 1.0],\n",
       "  'gs': [8, 8],\n",
       "  'ds': [2, 6],\n",
       "  'num_stages': 2,\n",
       "  'total_size_mb': 4.075489044189453,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 178641,\n",
       "  'params': 1068365,\n",
       "  'acts': 2065,\n",
       "  'WA': 40.0,\n",
       "  'W0': 96,\n",
       "  'WM': 2.549999999999998,\n",
       "  'DEPTH': 8,\n",
       "  'GROUP_W': 8},\n",
       " 'tangerine_beaver': {'ws': [88, 216, 528],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 6, 1],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 5.515956878662109,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 201227,\n",
       "  'params': 1445975,\n",
       "  'acts': 2395,\n",
       "  'WA': 32.0,\n",
       "  'W0': 88,\n",
       "  'WM': 2.4499999999999984,\n",
       "  'DEPTH': 9,\n",
       "  'GROUP_W': 8},\n",
       " 'nostalgic_bison': {'ws': [16, 48, 128],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [1, 3, 5],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 1.0675621032714844,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 42307,\n",
       "  'params': 279855,\n",
       "  'acts': 995,\n",
       "  'WA': 16.0,\n",
       "  'W0': 16,\n",
       "  'WM': 2.849999999999997,\n",
       "  'DEPTH': 9,\n",
       "  'GROUP_W': 8},\n",
       " 'vigilant_ibex': {'ws': [96, 272, 752],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 5, 5],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 30.856670379638672,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 1421775,\n",
       "  'params': 8088891,\n",
       "  'acts': 6479,\n",
       "  'WA': 56.0,\n",
       "  'W0': 96,\n",
       "  'WM': 2.799999999999997,\n",
       "  'DEPTH': 12,\n",
       "  'GROUP_W': 8},\n",
       " 'massive_jerboa': {'ws': [88, 248],\n",
       "  'bs': [1.0, 1.0],\n",
       "  'gs': [8, 8],\n",
       "  'ds': [4, 5],\n",
       "  'num_stages': 2,\n",
       "  'total_size_mb': 3.5692710876464844,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 153683,\n",
       "  'params': 935663,\n",
       "  'acts': 1955,\n",
       "  'WA': 16.0,\n",
       "  'W0': 88,\n",
       "  'WM': 2.799999999999997,\n",
       "  'DEPTH': 9,\n",
       "  'GROUP_W': 8},\n",
       " 'prompt_heron': {'ws': [32, 72, 176, 416],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 2, 5, 5],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 10.472164154052734,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 467363,\n",
       "  'params': 2745215,\n",
       "  'acts': 3843,\n",
       "  'WA': 32.0,\n",
       "  'W0': 32,\n",
       "  'WM': 2.3499999999999988,\n",
       "  'DEPTH': 13,\n",
       "  'GROUP_W': 8},\n",
       " 'powerful_chupacabra': {'ws': [120, 336],\n",
       "  'bs': [1.0, 1.0],\n",
       "  'gs': [8, 8],\n",
       "  'ds': [4, 14],\n",
       "  'num_stages': 2,\n",
       "  'total_size_mb': 16.909923553466797,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 790407,\n",
       "  'params': 4432835,\n",
       "  'acts': 6423,\n",
       "  'WA': 24.0,\n",
       "  'W0': 120,\n",
       "  'WM': 2.799999999999997,\n",
       "  'DEPTH': 18,\n",
       "  'GROUP_W': 8},\n",
       " 'mighty_crane': {'ws': [88, 184, 392, 816],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 2, 5, 4],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 34.22933578491211,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 1551611,\n",
       "  'params': 8973015,\n",
       "  'acts': 6923,\n",
       "  'WA': 64.0,\n",
       "  'W0': 88,\n",
       "  'WM': 2.0999999999999996,\n",
       "  'DEPTH': 12,\n",
       "  'GROUP_W': 8},\n",
       " 'tourmaline_jacamar': {'ws': [112, 248, 568, 1272],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [2, 5, 12, 1],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 55.5276985168457,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 2387937,\n",
       "  'params': 14556253,\n",
       "  'acts': 11649,\n",
       "  'WA': 40.0,\n",
       "  'W0': 112,\n",
       "  'WM': 2.249999999999999,\n",
       "  'DEPTH': 20,\n",
       "  'GROUP_W': 8},\n",
       " 'chirpy_swallow': {'ws': [112, 256, 592],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [4, 14, 1],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 13.32033920288086,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 553979,\n",
       "  'params': 3491847,\n",
       "  'acts': 5659,\n",
       "  'WA': 16.0,\n",
       "  'W0': 112,\n",
       "  'WM': 2.299999999999999,\n",
       "  'DEPTH': 19,\n",
       "  'GROUP_W': 8},\n",
       " 'frisky_clam': {'ws': [120, 280, 664],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 8, 12],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 59.00429153442383,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 2844169,\n",
       "  'params': 15467621,\n",
       "  'acts': 12921,\n",
       "  'WA': 32.0,\n",
       "  'W0': 120,\n",
       "  'WM': 2.3499999999999988,\n",
       "  'DEPTH': 22,\n",
       "  'GROUP_W': 8},\n",
       " 'loutish_snail': {'ws': [88, 240, 640, 1736],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 4, 11, 6],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 215.75689315795898,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 10370195,\n",
       "  'params': 56559375,\n",
       "  'acts': 22723,\n",
       "  'WA': 64.0,\n",
       "  'W0': 88,\n",
       "  'WM': 2.6999999999999975,\n",
       "  'DEPTH': 22,\n",
       "  'GROUP_W': 8},\n",
       " 'remarkable_seahorse': {'ws': [104, 288, 784, 2160],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [2, 4, 13, 3],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 206.5668601989746,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 9524635,\n",
       "  'params': 54150263,\n",
       "  'acts': 22027,\n",
       "  'WA': 64.0,\n",
       "  'W0': 104,\n",
       "  'WM': 2.7499999999999973,\n",
       "  'DEPTH': 22,\n",
       "  'GROUP_W': 8},\n",
       " 'visionary_adder': {'ws': [56, 152, 392],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [1, 4, 10],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 16.62386703491211,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 774475,\n",
       "  'params': 4357847,\n",
       "  'acts': 5659,\n",
       "  'WA': 40.0,\n",
       "  'W0': 56,\n",
       "  'WM': 2.6499999999999977,\n",
       "  'DEPTH': 15,\n",
       "  'GROUP_W': 8},\n",
       " 'interesting_dormouse': {'ws': [80, 176, 384],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [1, 4, 6],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 10.393291473388672,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 468527,\n",
       "  'params': 2724539,\n",
       "  'acts': 3791,\n",
       "  'WA': 40.0,\n",
       "  'W0': 80,\n",
       "  'WM': 2.1999999999999993,\n",
       "  'DEPTH': 11,\n",
       "  'GROUP_W': 8},\n",
       " 'quartz_dove': {'ws': [32, 64, 144, 296],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 2, 5, 5],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 5.870738983154297,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 254967,\n",
       "  'params': 1538979,\n",
       "  'acts': 2903,\n",
       "  'WA': 24.0,\n",
       "  'W0': 32,\n",
       "  'WM': 2.0999999999999996,\n",
       "  'DEPTH': 13,\n",
       "  'GROUP_W': 8},\n",
       " 'miniature_sawfly': {'ws': [80, 232],\n",
       "  'bs': [1.0, 1.0],\n",
       "  'gs': [8, 8],\n",
       "  'ds': [4, 15],\n",
       "  'num_stages': 2,\n",
       "  'total_size_mb': 8.969341278076172,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 406127,\n",
       "  'params': 2351259,\n",
       "  'acts': 4719,\n",
       "  'WA': 16.0,\n",
       "  'W0': 80,\n",
       "  'WM': 2.899999999999997,\n",
       "  'DEPTH': 19,\n",
       "  'GROUP_W': 8},\n",
       " 'cerulean_emu': {'ws': [120, 336, 944],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 5, 12],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 109.34440231323242,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 5349903,\n",
       "  'params': 28663979,\n",
       "  'acts': 16351,\n",
       "  'WA': 64.0,\n",
       "  'W0': 120,\n",
       "  'WM': 2.799999999999997,\n",
       "  'DEPTH': 19,\n",
       "  'GROUP_W': 8},\n",
       " 'pygmy_waxbill': {'ws': [48, 128, 352],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 5, 6],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 8.49496078491211,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 380491,\n",
       "  'params': 2226903,\n",
       "  'acts': 3499,\n",
       "  'WA': 24.0,\n",
       "  'W0': 48,\n",
       "  'WM': 2.6999999999999975,\n",
       "  'DEPTH': 13,\n",
       "  'GROUP_W': 8},\n",
       " 'crystal_swallow': {'ws': [56, 144, 376],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [1, 3, 5],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 7.778964996337891,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 345789,\n",
       "  'params': 2039209,\n",
       "  'acts': 2893,\n",
       "  'WA': 48.0,\n",
       "  'W0': 56,\n",
       "  'WM': 2.599999999999998,\n",
       "  'DEPTH': 9,\n",
       "  'GROUP_W': 8},\n",
       " 'overjoyed_caracal': {'ws': [64, 168, 448, 1192],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 3, 8, 1],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 27.898883819580078,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 1071849,\n",
       "  'params': 7313525,\n",
       "  'acts': 6409,\n",
       "  'WA': 56.0,\n",
       "  'W0': 64,\n",
       "  'WM': 2.6499999999999977,\n",
       "  'DEPTH': 13,\n",
       "  'GROUP_W': 8},\n",
       " 'dark_goat': {'ws': [32, 64, 136, 272, 568],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8, 8],\n",
       "  'ds': [1, 2, 4, 9, 3],\n",
       "  'num_stages': 5,\n",
       "  'total_size_mb': 17.428829193115234,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 767235,\n",
       "  'params': 4568863,\n",
       "  'acts': 5955,\n",
       "  'WA': 24.0,\n",
       "  'W0': 32,\n",
       "  'WM': 2.05,\n",
       "  'DEPTH': 19,\n",
       "  'GROUP_W': 8},\n",
       " 'glittering_ostrich': {'ws': [96, 256, 696],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 5, 10],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 50.87068557739258,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 2442585,\n",
       "  'params': 13335445,\n",
       "  'acts': 10393,\n",
       "  'WA': 48.0,\n",
       "  'W0': 96,\n",
       "  'WM': 2.6999999999999975,\n",
       "  'DEPTH': 17,\n",
       "  'GROUP_W': 8},\n",
       " 'proficient_duck': {'ws': [88, 184, 368, 760],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 3, 4, 7],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 46.2583122253418,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 2177143,\n",
       "  'params': 12126339,\n",
       "  'acts': 9127,\n",
       "  'WA': 56.0,\n",
       "  'W0': 88,\n",
       "  'WM': 2.05,\n",
       "  'DEPTH': 15,\n",
       "  'GROUP_W': 8},\n",
       " 'beneficial_axolotl': {'ws': [120, 272, 632],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 5, 9],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 39.538692474365234,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 1878435,\n",
       "  'params': 10364831,\n",
       "  'acts': 8979,\n",
       "  'WA': 48.0,\n",
       "  'W0': 120,\n",
       "  'WM': 2.299999999999999,\n",
       "  'DEPTH': 16,\n",
       "  'GROUP_W': 8},\n",
       " 'macho_oxpecker': {'ws': [120, 272, 608],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 4, 11],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 43.65571212768555,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 2089239,\n",
       "  'params': 11444083,\n",
       "  'acts': 9895,\n",
       "  'WA': 48.0,\n",
       "  'W0': 120,\n",
       "  'WM': 2.249999999999999,\n",
       "  'DEPTH': 17,\n",
       "  'GROUP_W': 8},\n",
       " 'carmine_chinchilla': {'ws': [104, 272],\n",
       "  'bs': [1.0, 1.0],\n",
       "  'gs': [8, 8],\n",
       "  'ds': [5, 3],\n",
       "  'num_stages': 2,\n",
       "  'total_size_mb': 2.8752174377441406,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 118285,\n",
       "  'params': 753721,\n",
       "  'acts': 1629,\n",
       "  'WA': 16.0,\n",
       "  'W0': 104,\n",
       "  'WM': 2.6499999999999977,\n",
       "  'DEPTH': 8,\n",
       "  'GROUP_W': 8},\n",
       " 'pistachio_mouflon': {'ws': [112, 264],\n",
       "  'bs': [1.0, 1.0],\n",
       "  'gs': [8, 8],\n",
       "  'ds': [4, 4],\n",
       "  'num_stages': 2,\n",
       "  'total_size_mb': 3.459156036376953,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 146833,\n",
       "  'params': 906797,\n",
       "  'acts': 1841,\n",
       "  'WA': 16.0,\n",
       "  'W0': 112,\n",
       "  'WM': 2.3499999999999988,\n",
       "  'DEPTH': 8,\n",
       "  'GROUP_W': 8},\n",
       " 'unnatural_rook': {'ws': [56, 160, 472, 1368],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 3, 10, 4],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 91.5938835144043,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 4241591,\n",
       "  'params': 24010787,\n",
       "  'acts': 13095,\n",
       "  'WA': 56.0,\n",
       "  'W0': 56,\n",
       "  'WM': 2.899999999999997,\n",
       "  'DEPTH': 18,\n",
       "  'GROUP_W': 8},\n",
       " 'discerning_urchin': {'ws': [80, 224, 648],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 5, 9],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 39.614742279052734,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 1887539,\n",
       "  'params': 10384767,\n",
       "  'acts': 8755,\n",
       "  'WA': 48.0,\n",
       "  'W0': 80,\n",
       "  'WM': 2.849999999999997,\n",
       "  'DEPTH': 16,\n",
       "  'GROUP_W': 8},\n",
       " 'hulking_wombat': {'ws': [64, 144, 336],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [3, 7, 12],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 15.814083099365234,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 730131,\n",
       "  'params': 4145567,\n",
       "  'acts': 6483,\n",
       "  'WA': 16.0,\n",
       "  'W0': 64,\n",
       "  'WM': 2.299999999999999,\n",
       "  'DEPTH': 22,\n",
       "  'GROUP_W': 8},\n",
       " 'hulking_bee': {'ws': [120, 328, 904],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 7, 11],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 94.90156936645508,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 4619129,\n",
       "  'params': 24877877,\n",
       "  'acts': 15401,\n",
       "  'WA': 48.0,\n",
       "  'W0': 120,\n",
       "  'WM': 2.7499999999999973,\n",
       "  'DEPTH': 20,\n",
       "  'GROUP_W': 8},\n",
       " 'abiding_narwhal': {'ws': [112, 296, 784],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 6, 11],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 71.48481369018555,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 3461559,\n",
       "  'params': 18739315,\n",
       "  'acts': 13111,\n",
       "  'WA': 48.0,\n",
       "  'W0': 112,\n",
       "  'WM': 2.6499999999999977,\n",
       "  'DEPTH': 19,\n",
       "  'GROUP_W': 8},\n",
       " 'courageous_puffin': {'ws': [64, 128, 272, 552, 1128],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8, 8],\n",
       "  'ds': [1, 2, 4, 9, 2],\n",
       "  'num_stages': 5,\n",
       "  'total_size_mb': 54.5422477722168,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 2416071,\n",
       "  'params': 14297923,\n",
       "  'acts': 10535,\n",
       "  'WA': 48.0,\n",
       "  'W0': 64,\n",
       "  'WM': 2.05,\n",
       "  'DEPTH': 18,\n",
       "  'GROUP_W': 8},\n",
       " 'overjoyed_guillemot': {'ws': [40, 88, 208, 488],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 3, 5, 12],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 31.299274444580078,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 1484953,\n",
       "  'params': 8204917,\n",
       "  'acts': 8905,\n",
       "  'WA': 32.0,\n",
       "  'W0': 40,\n",
       "  'WM': 2.299999999999999,\n",
       "  'DEPTH': 21,\n",
       "  'GROUP_W': 8},\n",
       " 'woodoo_herring': {'ws': [56, 136, 320, 776],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 3, 6, 6],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 41.79042434692383,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 1954601,\n",
       "  'params': 10955109,\n",
       "  'acts': 8633,\n",
       "  'WA': 48.0,\n",
       "  'W0': 56,\n",
       "  'WM': 2.3999999999999986,\n",
       "  'DEPTH': 16,\n",
       "  'GROUP_W': 8},\n",
       " 'mustard_marten': {'ws': [64, 144, 336, 776, 1792],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8, 8],\n",
       "  'ds': [1, 2, 5, 10, 4],\n",
       "  'num_stages': 5,\n",
       "  'total_size_mb': 184.74099349975586,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 8674827,\n",
       "  'params': 48428743,\n",
       "  'acts': 20779,\n",
       "  'WA': 64.0,\n",
       "  'W0': 64,\n",
       "  'WM': 2.299999999999999,\n",
       "  'DEPTH': 22,\n",
       "  'GROUP_W': 8},\n",
       " 'abiding_markhor': {'ws': [64, 144, 336],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [1, 3, 8],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 10.019161224365234,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 456211,\n",
       "  'params': 2626463,\n",
       "  'acts': 3923,\n",
       "  'WA': 40.0,\n",
       "  'W0': 64,\n",
       "  'WM': 2.299999999999999,\n",
       "  'DEPTH': 12,\n",
       "  'GROUP_W': 8},\n",
       " 'elegant_vicugna': {'ws': [64, 128, 272, 552],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 3, 7, 9],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 33.44028854370117,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 1574703,\n",
       "  'params': 8766171,\n",
       "  'acts': 9039,\n",
       "  'WA': 32.0,\n",
       "  'W0': 64,\n",
       "  'WM': 2.05,\n",
       "  'DEPTH': 20,\n",
       "  'GROUP_W': 8},\n",
       " 'rainbow_smilodon': {'ws': [104, 296, 848],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 5, 9],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 67.03670120239258,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 3227833,\n",
       "  'params': 17573269,\n",
       "  'acts': 11465,\n",
       "  'WA': 64.0,\n",
       "  'W0': 104,\n",
       "  'WM': 2.849999999999997,\n",
       "  'DEPTH': 16,\n",
       "  'GROUP_W': 8},\n",
       " 'hot_sunfish': {'ws': [32, 96, 272],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [1, 5, 3],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 2.7990684509277344,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 114515,\n",
       "  'params': 733759,\n",
       "  'acts': 1619,\n",
       "  'WA': 24.0,\n",
       "  'W0': 32,\n",
       "  'WM': 2.899999999999997,\n",
       "  'DEPTH': 9,\n",
       "  'GROUP_W': 8},\n",
       " 'micro_lion': {'ws': [96, 272],\n",
       "  'bs': [1.0, 1.0],\n",
       "  'gs': [8, 8],\n",
       "  'ds': [2, 7],\n",
       "  'num_stages': 2,\n",
       "  'total_size_mb': 5.573482513427734,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 248979,\n",
       "  'params': 1461055,\n",
       "  'acts': 2579,\n",
       "  'WA': 40.0,\n",
       "  'W0': 96,\n",
       "  'WM': 2.849999999999997,\n",
       "  'DEPTH': 9,\n",
       "  'GROUP_W': 8},\n",
       " 'industrious_whippet': {'ws': [120, 256, 552],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 4, 6],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 21.116962432861328,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 971609,\n",
       "  'params': 5535685,\n",
       "  'acts': 5609,\n",
       "  'WA': 48.0,\n",
       "  'W0': 120,\n",
       "  'WM': 2.1499999999999995,\n",
       "  'DEPTH': 12,\n",
       "  'GROUP_W': 8},\n",
       " 'tidy_seal': {'ws': [112, 304, 848],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 6, 7],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 54.120906829833984,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 2568867,\n",
       "  'params': 14187471,\n",
       "  'acts': 9795,\n",
       "  'WA': 56.0,\n",
       "  'W0': 112,\n",
       "  'WM': 2.7499999999999973,\n",
       "  'DEPTH': 15,\n",
       "  'GROUP_W': 8},\n",
       " 'rampant_sunfish': {'ws': [120, 272],\n",
       "  'bs': [1.0, 1.0],\n",
       "  'gs': [8, 8],\n",
       "  'ds': [4, 13],\n",
       "  'num_stages': 2,\n",
       "  'total_size_mb': 10.800045013427734,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 493923,\n",
       "  'params': 2831167,\n",
       "  'acts': 4979,\n",
       "  'WA': 16.0,\n",
       "  'W0': 120,\n",
       "  'WM': 2.299999999999999,\n",
       "  'DEPTH': 17,\n",
       "  'GROUP_W': 8},\n",
       " 'magenta_mink': {'ws': [72, 144, 304, 624],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 3, 6, 1],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 10.035053253173828,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 390745,\n",
       "  'params': 2630629,\n",
       "  'acts': 3561,\n",
       "  'WA': 40.0,\n",
       "  'W0': 72,\n",
       "  'WM': 2.05,\n",
       "  'DEPTH': 11,\n",
       "  'GROUP_W': 8},\n",
       " 'hot_pogona': {'ws': [48, 128, 336, 896],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 3, 9, 5],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 48.94851303100586,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 2277019,\n",
       "  'params': 12831559,\n",
       "  'acts': 9723,\n",
       "  'WA': 40.0,\n",
       "  'W0': 48,\n",
       "  'WM': 2.6499999999999977,\n",
       "  'DEPTH': 18,\n",
       "  'GROUP_W': 8},\n",
       " 'gorgeous_goose': {'ws': [96, 208, 464, 1024],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 3, 6, 9],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 105.7199821472168,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 5109111,\n",
       "  'params': 27713859,\n",
       "  'acts': 15671,\n",
       "  'WA': 64.0,\n",
       "  'W0': 96,\n",
       "  'WM': 2.1999999999999993,\n",
       "  'DEPTH': 19,\n",
       "  'GROUP_W': 8},\n",
       " 'tentacled_squid': {'ws': [112, 320, 912],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 6, 13],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 111.26879501342773,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 5453683,\n",
       "  'params': 29168447,\n",
       "  'acts': 17299,\n",
       "  'WA': 56.0,\n",
       "  'W0': 112,\n",
       "  'WM': 2.849999999999997,\n",
       "  'DEPTH': 21,\n",
       "  'GROUP_W': 8},\n",
       " 'refined_panda': {'ws': [112, 232, 472, 968],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 3, 5, 1],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 21.22207260131836,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 821467,\n",
       "  'params': 5563239,\n",
       "  'acts': 4955,\n",
       "  'WA': 64.0,\n",
       "  'W0': 112,\n",
       "  'WM': 2.05,\n",
       "  'DEPTH': 10,\n",
       "  'GROUP_W': 8},\n",
       " 'impartial_bullfinch': {'ws': [48, 104, 208],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [1, 3, 4],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 2.307880401611328,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 95321,\n",
       "  'params': 604997,\n",
       "  'acts': 1465,\n",
       "  'WA': 32.0,\n",
       "  'W0': 48,\n",
       "  'WM': 2.0999999999999996,\n",
       "  'DEPTH': 8,\n",
       "  'GROUP_W': 8},\n",
       " 'illustrious_taipan': {'ws': [120, 296, 720],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 6, 5],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 30.624446868896484,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 1410307,\n",
       "  'params': 8028015,\n",
       "  'acts': 6867,\n",
       "  'WA': 48.0,\n",
       "  'W0': 120,\n",
       "  'WM': 2.4499999999999984,\n",
       "  'DEPTH': 13,\n",
       "  'GROUP_W': 8},\n",
       " 'fearless_vulture': {'ws': [112, 320, 912],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 6, 9],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 78.43743515014648,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 3785635,\n",
       "  'params': 20561903,\n",
       "  'acts': 12739,\n",
       "  'WA': 56.0,\n",
       "  'W0': 112,\n",
       "  'WM': 2.849999999999997,\n",
       "  'DEPTH': 17,\n",
       "  'GROUP_W': 8},\n",
       " 'fanatic_magpie': {'ws': [88, 192, 408, 872, 1880],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8, 8],\n",
       "  'ds': [1, 2, 5, 11, 1],\n",
       "  'num_stages': 5,\n",
       "  'total_size_mb': 120.76346206665039,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 5254333,\n",
       "  'params': 31657417,\n",
       "  'acts': 17037,\n",
       "  'WA': 64.0,\n",
       "  'W0': 88,\n",
       "  'WM': 2.1499999999999995,\n",
       "  'DEPTH': 20,\n",
       "  'GROUP_W': 8},\n",
       " 'logical_salmon': {'ws': [64, 176, 480],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 8, 1],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 4.704128265380859,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 171259,\n",
       "  'params': 1233159,\n",
       "  'acts': 2427,\n",
       "  'WA': 24.0,\n",
       "  'W0': 64,\n",
       "  'WM': 2.7499999999999973,\n",
       "  'DEPTH': 11,\n",
       "  'GROUP_W': 8},\n",
       " 'onyx_groundhog': {'ws': [24, 56, 120, 256, 560],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8, 8],\n",
       "  'ds': [1, 2, 4, 8, 7],\n",
       "  'num_stages': 5,\n",
       "  'total_size_mb': 28.063663482666016,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 1304069,\n",
       "  'params': 7356721,\n",
       "  'acts': 8117,\n",
       "  'WA': 24.0,\n",
       "  'W0': 24,\n",
       "  'WM': 2.1999999999999993,\n",
       "  'DEPTH': 22,\n",
       "  'GROUP_W': 8},\n",
       " 'merciful_elephant': {'ws': [56, 152, 392],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 6, 3],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 6.115550994873047,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 258343,\n",
       "  'params': 1603155,\n",
       "  'acts': 2679,\n",
       "  'WA': 24.0,\n",
       "  'W0': 56,\n",
       "  'WM': 2.6499999999999977,\n",
       "  'DEPTH': 11,\n",
       "  'GROUP_W': 8},\n",
       " 'lurking_okapi': {'ws': [24, 64, 160],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [1, 4, 6],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 1.9640998840332031,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 81105,\n",
       "  'params': 514877,\n",
       "  'acts': 1537,\n",
       "  'WA': 16.0,\n",
       "  'W0': 24,\n",
       "  'WM': 2.599999999999998,\n",
       "  'DEPTH': 11,\n",
       "  'GROUP_W': 8},\n",
       " 'athletic_dogfish': {'ws': [96, 224, 504, 1168],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 4, 8, 9],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 140.59917831420898,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 6826611,\n",
       "  'params': 36857231,\n",
       "  'acts': 19155,\n",
       "  'WA': 56.0,\n",
       "  'W0': 96,\n",
       "  'WM': 2.299999999999999,\n",
       "  'DEPTH': 22,\n",
       "  'GROUP_W': 8},\n",
       " 'zippy_curassow': {'ws': [104, 248, 576],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [1, 4, 9],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 32.201908111572266,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 1524389,\n",
       "  'params': 8441537,\n",
       "  'acts': 7733,\n",
       "  'WA': 56.0,\n",
       "  'W0': 104,\n",
       "  'WM': 2.3499999999999988,\n",
       "  'DEPTH': 14,\n",
       "  'GROUP_W': 8},\n",
       " 'therapeutic_spoonbill': {'ws': [104, 256, 648, 1624],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [2, 4, 11, 4],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 145.96964645385742,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 6833855,\n",
       "  'params': 38265067,\n",
       "  'acts': 18191,\n",
       "  'WA': 56.0,\n",
       "  'W0': 104,\n",
       "  'WM': 2.4999999999999982,\n",
       "  'DEPTH': 21,\n",
       "  'GROUP_W': 8},\n",
       " 'tungsten_nautilus': {'ws': [120, 248, 528],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 5, 1],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 6.058048248291016,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 225205,\n",
       "  'params': 1588081,\n",
       "  'acts': 2405,\n",
       "  'WA': 40.0,\n",
       "  'W0': 120,\n",
       "  'WM': 2.0999999999999996,\n",
       "  'DEPTH': 8,\n",
       "  'GROUP_W': 8},\n",
       " 'athletic_manatee': {'ws': [104, 216],\n",
       "  'bs': [1.0, 1.0],\n",
       "  'gs': [8, 8],\n",
       "  'ds': [2, 6],\n",
       "  'num_stages': 2,\n",
       "  'total_size_mb': 3.2657203674316406,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 140909,\n",
       "  'params': 856089,\n",
       "  'acts': 1853,\n",
       "  'WA': 24.0,\n",
       "  'W0': 104,\n",
       "  'WM': 2.0999999999999996,\n",
       "  'DEPTH': 8,\n",
       "  'GROUP_W': 8},\n",
       " 'talented_cuscus': {'ws': [96, 224, 528],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 5, 4],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 13.711284637451172,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 609103,\n",
       "  'params': 3594331,\n",
       "  'acts': 4175,\n",
       "  'WA': 40.0,\n",
       "  'W0': 96,\n",
       "  'WM': 2.3499999999999988,\n",
       "  'DEPTH': 11,\n",
       "  'GROUP_W': 8},\n",
       " 'denim_hippo': {'ws': [72, 200],\n",
       "  'bs': [1.0, 1.0],\n",
       "  'gs': [8, 8],\n",
       "  'ds': [4, 5],\n",
       "  'num_stages': 2,\n",
       "  'total_size_mb': 2.4128379821777344,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 101571,\n",
       "  'params': 632511,\n",
       "  'acts': 1587,\n",
       "  'WA': 16.0,\n",
       "  'W0': 72,\n",
       "  'WM': 2.799999999999997,\n",
       "  'DEPTH': 9,\n",
       "  'GROUP_W': 8},\n",
       " 'lurking_chamois': {'ws': [104, 272, 728],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [3, 12, 3],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 24.415164947509766,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 1083925,\n",
       "  'params': 6400289,\n",
       "  'acts': 7045,\n",
       "  'WA': 24.0,\n",
       "  'W0': 104,\n",
       "  'WM': 2.6499999999999977,\n",
       "  'DEPTH': 18,\n",
       "  'GROUP_W': 8},\n",
       " 'primitive_harrier': {'ws': [104, 216, 440, 896],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [2, 4, 8, 8],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 80.52289962768555,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 3852599,\n",
       "  'params': 21108595,\n",
       "  'acts': 14503,\n",
       "  'WA': 40.0,\n",
       "  'W0': 104,\n",
       "  'WM': 2.05,\n",
       "  'DEPTH': 22,\n",
       "  'GROUP_W': 8},\n",
       " 'dainty_rattlesnake': {'ws': [96, 208, 440, 952],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [2, 5, 11, 1],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 31.892353057861328,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 1353273,\n",
       "  'params': 8360389,\n",
       "  'acts': 8569,\n",
       "  'WA': 32.0,\n",
       "  'W0': 96,\n",
       "  'WM': 2.1499999999999995,\n",
       "  'DEPTH': 19,\n",
       "  'GROUP_W': 8},\n",
       " 'debonair_bloodhound': {'ws': [72, 208, 608, 1760],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 4, 10, 1],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 60.750308990478516,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 2380869,\n",
       "  'params': 15925329,\n",
       "  'acts': 10517,\n",
       "  'WA': 64.0,\n",
       "  'W0': 72,\n",
       "  'WM': 2.899999999999997,\n",
       "  'DEPTH': 16,\n",
       "  'GROUP_W': 8},\n",
       " 'tall_koel': {'ws': [40, 96, 248, 624],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8],\n",
       "  'ds': [1, 2, 6, 8],\n",
       "  'num_stages': 4,\n",
       "  'total_size_mb': 34.612735748291016,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 1633685,\n",
       "  'params': 9073521,\n",
       "  'acts': 8261,\n",
       "  'WA': 40.0,\n",
       "  'W0': 40,\n",
       "  'WM': 2.4999999999999982,\n",
       "  'DEPTH': 17,\n",
       "  'GROUP_W': 8},\n",
       " 'cyan_tench': {'ws': [96, 280],\n",
       "  'bs': [1.0, 1.0],\n",
       "  'gs': [8, 8],\n",
       "  'ds': [3, 7],\n",
       "  'num_stages': 2,\n",
       "  'total_size_mb': 5.984844207763672,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 267567,\n",
       "  'params': 1568891,\n",
       "  'acts': 2767,\n",
       "  'WA': 24.0,\n",
       "  'W0': 96,\n",
       "  'WM': 2.899999999999997,\n",
       "  'DEPTH': 10,\n",
       "  'GROUP_W': 8},\n",
       " 'finicky_armadillo': {'ws': [80, 232, 672],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 6, 7],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 34.05759811401367,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 1601983,\n",
       "  'params': 8927995,\n",
       "  'acts': 7679,\n",
       "  'WA': 40.0,\n",
       "  'W0': 80,\n",
       "  'WM': 2.899999999999997,\n",
       "  'DEPTH': 15,\n",
       "  'GROUP_W': 8},\n",
       " 'nocturnal_scorpion': {'ws': [48, 128, 328],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 5, 7],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 8.704845428466797,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 391703,\n",
       "  'params': 2281923,\n",
       "  'acts': 3735,\n",
       "  'WA': 24.0,\n",
       "  'W0': 48,\n",
       "  'WM': 2.599999999999998,\n",
       "  'DEPTH': 14,\n",
       "  'GROUP_W': 8},\n",
       " 'uptight_dugong': {'ws': [104, 304, 872],\n",
       "  'bs': [1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8],\n",
       "  'ds': [2, 6, 7],\n",
       "  'num_stages': 3,\n",
       "  'total_size_mb': 56.70949935913086,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 2694827,\n",
       "  'params': 14866055,\n",
       "  'acts': 9979,\n",
       "  'WA': 56.0,\n",
       "  'W0': 104,\n",
       "  'WM': 2.899999999999997,\n",
       "  'DEPTH': 15,\n",
       "  'GROUP_W': 8},\n",
       " 'striped_dalmatian': {'ws': [64, 128, 272, 552, 1128],\n",
       "  'bs': [1.0, 1.0, 1.0, 1.0, 1.0],\n",
       "  'gs': [8, 8, 8, 8, 8],\n",
       "  'ds': [1, 2, 4, 9, 5],\n",
       "  'num_stages': 5,\n",
       "  'total_size_mb': 91.96847915649414,\n",
       "  'h': 1,\n",
       "  'w': 1,\n",
       "  'flops': 4328877,\n",
       "  'params': 24108985,\n",
       "  'acts': 14765,\n",
       "  'WA': 48.0,\n",
       "  'W0': 64,\n",
       "  'WM': 2.05,\n",
       "  'DEPTH': 21,\n",
       "  'GROUP_W': 8}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chromosomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20190f9-6c5e-419b-bc10-e09b9591be9e",
   "metadata": {},
   "source": [
    "# Load teacher (regnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "458804df-d09e-4692-9466-7f3544c64cbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/anki_lab_submission/tests/results/full_training_evonas/augmentations_test/Adaline\n",
      "{'0': {'val_acc': 93.00666809082031, 'train_acc': 100.0, 'epoch_time': 8.457533359527588}, '1': {'val_acc': 79.26667022705078, 'train_acc': 99.99554443359375, 'epoch_time': 16.541600227355957}, '2': {'val_acc': 56.59333419799805, 'train_acc': 99.9065170288086, 'epoch_time': 16.808910846710205}, '3': {'val_acc': 18.80666732788086, 'train_acc': 63.078704833984375, 'epoch_time': 19.368762016296387}, '4': {'val_acc': 19.84000015258789, 'train_acc': 76.0973129272461, 'epoch_time': 20.979628324508667}, '5': {'val_acc': 91.55333709716797, 'train_acc': 99.5615234375, 'epoch_time': 11.082377195358276}, '6': {'val_acc': 92.73999786376953, 'train_acc': 97.33573913574219, 'epoch_time': 14.146133184432983}, '7': {'val_acc': 91.22666931152344, 'train_acc': 93.76780700683594, 'epoch_time': 11.500339031219482}, '8': {'val_acc': 92.16666412353516, 'train_acc': 86.56072235107422, 'epoch_time': 16.458400011062622}}\n",
      "[('0', {'val_acc': 93.00666809082031, 'train_acc': 100.0, 'epoch_time': 8.457533359527588}), ('6', {'val_acc': 92.73999786376953, 'train_acc': 97.33573913574219, 'epoch_time': 14.146133184432983}), ('8', {'val_acc': 92.16666412353516, 'train_acc': 86.56072235107422, 'epoch_time': 16.458400011062622}), ('5', {'val_acc': 91.55333709716797, 'train_acc': 99.5615234375, 'epoch_time': 11.082377195358276}), ('7', {'val_acc': 91.22666931152344, 'train_acc': 93.76780700683594, 'epoch_time': 11.500339031219482}), ('1', {'val_acc': 79.26667022705078, 'train_acc': 99.99554443359375, 'epoch_time': 16.541600227355957}), ('2', {'val_acc': 56.59333419799805, 'train_acc': 99.9065170288086, 'epoch_time': 16.808910846710205}), ('4', {'val_acc': 19.84000015258789, 'train_acc': 76.0973129272461, 'epoch_time': 20.979628324508667}), ('3', {'val_acc': 18.80666732788086, 'train_acc': 63.078704833984375, 'epoch_time': 19.368762016296387})]\n",
      "First best key: 0\n",
      "Second best key: 6\n",
      "The key with the maximum value is \"6\" with a value of {'val_acc': 92.73999786376953, 'train_acc': 97.33573913574219, 'epoch_time': 14.146133184432983}.\n"
     ]
    }
   ],
   "source": [
    "    import torchvision.models as models_torch\n",
    "    from utils.train_cfg import load_checkpoint\n",
    "    # Train models\n",
    "    SUBMISSION_PATH=\"..\"\n",
    "    metadata[\"train_config_path\"]=f'{SUBMISSION_PATH}/configs/train/regnet_distillation_adam_test.yaml'\n",
    "    train_cfg=get_cfg()\n",
    "    train_cfg.merge_from_file(metadata[\"train_config_path\"])\n",
    "    os.makedirs(test_folder, exist_ok=True)\n",
    "    output_file_path = f\"{test_folder}/config.yaml\"#\n",
    "    \n",
    "    with open(output_file_path, \"w\") as f:\n",
    "            f.write(train_cfg.dump()) \n",
    "\n",
    "    models_names=sorted(list(models.keys()))[:] \n",
    "    multi=False\n",
    "    #ic((get_gpu_memory(0) / (1024 ** 3)))\n",
    "    ############################### Load resnet teacher model #################\n",
    "    # save the results to a file\n",
    "    aug_path=f\"/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/anki_lab_submission/tests/results/full_training_evonas/augmentations_test/{metadata['codename']}\"\n",
    "    print(aug_path)\n",
    "    with open(f\"{aug_path}/augmentation_results.json\", 'r') as f:\n",
    "        results = json.load(f)\n",
    "    print(results)\n",
    "    \n",
    "    # Sort the dictionary by value in descending order\n",
    "    # Sorting by 'val_acc'\n",
    "    sorted_items = sorted(results.items(), key=lambda item: item[1]['val_acc'], reverse=True)\n",
    "    print(sorted_items)\n",
    "    print(f\"First best key: {sorted_items[0][0]}\")\n",
    "    print(f\"Second best key: {sorted_items[1][0]}\")\n",
    "    \n",
    "    max_key = sorted_items[0][0] if sorted_items[0][0]!=\"0\" else sorted_items[1][0]\n",
    "    max_value = results[max_key]\n",
    "\n",
    "    print(f'The key with the maximum value is \"{max_key}\" with a value of {max_value}.')\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e09a392d-dffc-4af7-92fb-a3ee149adcd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    weights_file=f\"{aug_path}/aug_{max_key}/student_best\"\n",
    "    teacher = models_torch.resnet18(weights=None)\n",
    "    new_conv1 = torch.nn.Conv2d(in_channels=metadata[\"input_shape\"][1], \n",
    "                            out_channels=teacher.conv1.out_channels, \n",
    "                            kernel_size=teacher.conv1.kernel_size, \n",
    "                            stride=teacher.conv1.stride, \n",
    "                            padding=teacher.conv1.padding, \n",
    "                            bias=teacher.conv1.bias)\n",
    "    # Replace the first convolutional layer\n",
    "    teacher.conv1 = new_conv1\n",
    "    teacher.fc = torch.nn.Linear(512, metadata['num_classes'])\n",
    "    state = load_checkpoint(weights_file)\n",
    "    teacher.load_state_dict(state[\"model\"])\n",
    "    teacher.to(device)\n",
    "    ########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31209b85-393b-4a29-a6a6-2a1f3613b53f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: /home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/Mateo/awesome_dodo/config.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RegNet(\n",
       "  (stem): ResStemCifar(\n",
       "    (conv): Conv2d(3, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (af): ReLU(inplace=True)\n",
       "  )\n",
       "  (s1): AnyStage(\n",
       "    (b1): ResBottleneckBlock(\n",
       "      (f): BottleneckTransform(\n",
       "        (a): Conv2d(28, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (a_bn): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (a_af): ReLU(inplace=True)\n",
       "        (b): Conv2d(120, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=15, bias=False)\n",
       "        (b_bn): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (b_af): ReLU(inplace=True)\n",
       "        (se): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "          (f_ex): Sequential(\n",
       "            (0): Conv2d(120, 7, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(7, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (3): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (c): Conv2d(120, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (c_bn): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (af): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (1): ConvNormAct(\n",
       "          (conv): Conv2d(28, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "    )\n",
       "    (b2): ResBottleneckBlock(\n",
       "      (f): BottleneckTransform(\n",
       "        (a): Conv2d(120, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (a_bn): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (a_af): ReLU(inplace=True)\n",
       "        (b): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=15, bias=False)\n",
       "        (b_bn): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (b_af): ReLU(inplace=True)\n",
       "        (se): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "          (f_ex): Sequential(\n",
       "            (0): Conv2d(120, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(30, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (3): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (c): Conv2d(120, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (c_bn): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (af): ReLU(inplace=True)\n",
       "      (downsample): Identity()\n",
       "      (drop_path): DropPath()\n",
       "    )\n",
       "    (b3): ResBottleneckBlock(\n",
       "      (f): BottleneckTransform(\n",
       "        (a): Conv2d(120, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (a_bn): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (a_af): ReLU(inplace=True)\n",
       "        (b): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=15, bias=False)\n",
       "        (b_bn): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (b_af): ReLU(inplace=True)\n",
       "        (se): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "          (f_ex): Sequential(\n",
       "            (0): Conv2d(120, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(30, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (3): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (c): Conv2d(120, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (c_bn): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (af): ReLU(inplace=True)\n",
       "      (downsample): Identity()\n",
       "      (drop_path): DropPath()\n",
       "    )\n",
       "  )\n",
       "  (s2): AnyStage(\n",
       "    (b1): ResBottleneckBlock(\n",
       "      (f): BottleneckTransform(\n",
       "        (a): Conv2d(120, 344, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (a_bn): BatchNorm2d(344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (a_af): ReLU(inplace=True)\n",
       "        (b): Conv2d(344, 344, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=43, bias=False)\n",
       "        (b_bn): BatchNorm2d(344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (b_af): ReLU(inplace=True)\n",
       "        (se): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "          (f_ex): Sequential(\n",
       "            (0): Conv2d(344, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(30, 344, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (3): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (c): Conv2d(344, 344, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (c_bn): BatchNorm2d(344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (af): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (1): ConvNormAct(\n",
       "          (conv): Conv2d(120, 344, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "    )\n",
       "    (b2): ResBottleneckBlock(\n",
       "      (f): BottleneckTransform(\n",
       "        (a): Conv2d(344, 344, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (a_bn): BatchNorm2d(344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (a_af): ReLU(inplace=True)\n",
       "        (b): Conv2d(344, 344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=43, bias=False)\n",
       "        (b_bn): BatchNorm2d(344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (b_af): ReLU(inplace=True)\n",
       "        (se): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "          (f_ex): Sequential(\n",
       "            (0): Conv2d(344, 86, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(86, 344, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (3): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (c): Conv2d(344, 344, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (c_bn): BatchNorm2d(344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (af): ReLU(inplace=True)\n",
       "      (downsample): Identity()\n",
       "      (drop_path): DropPath()\n",
       "    )\n",
       "    (b3): ResBottleneckBlock(\n",
       "      (f): BottleneckTransform(\n",
       "        (a): Conv2d(344, 344, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (a_bn): BatchNorm2d(344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (a_af): ReLU(inplace=True)\n",
       "        (b): Conv2d(344, 344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=43, bias=False)\n",
       "        (b_bn): BatchNorm2d(344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (b_af): ReLU(inplace=True)\n",
       "        (se): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "          (f_ex): Sequential(\n",
       "            (0): Conv2d(344, 86, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(86, 344, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (3): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (c): Conv2d(344, 344, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (c_bn): BatchNorm2d(344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (af): ReLU(inplace=True)\n",
       "      (downsample): Identity()\n",
       "      (drop_path): DropPath()\n",
       "    )\n",
       "    (b4): ResBottleneckBlock(\n",
       "      (f): BottleneckTransform(\n",
       "        (a): Conv2d(344, 344, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (a_bn): BatchNorm2d(344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (a_af): ReLU(inplace=True)\n",
       "        (b): Conv2d(344, 344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=43, bias=False)\n",
       "        (b_bn): BatchNorm2d(344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (b_af): ReLU(inplace=True)\n",
       "        (se): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "          (f_ex): Sequential(\n",
       "            (0): Conv2d(344, 86, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(86, 344, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (3): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (c): Conv2d(344, 344, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (c_bn): BatchNorm2d(344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (af): ReLU(inplace=True)\n",
       "      (downsample): Identity()\n",
       "      (drop_path): DropPath()\n",
       "    )\n",
       "    (b5): ResBottleneckBlock(\n",
       "      (f): BottleneckTransform(\n",
       "        (a): Conv2d(344, 344, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (a_bn): BatchNorm2d(344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (a_af): ReLU(inplace=True)\n",
       "        (b): Conv2d(344, 344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=43, bias=False)\n",
       "        (b_bn): BatchNorm2d(344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (b_af): ReLU(inplace=True)\n",
       "        (se): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "          (f_ex): Sequential(\n",
       "            (0): Conv2d(344, 86, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(86, 344, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (3): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (c): Conv2d(344, 344, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (c_bn): BatchNorm2d(344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (af): ReLU(inplace=True)\n",
       "      (downsample): Identity()\n",
       "      (drop_path): DropPath()\n",
       "    )\n",
       "    (b6): ResBottleneckBlock(\n",
       "      (f): BottleneckTransform(\n",
       "        (a): Conv2d(344, 344, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (a_bn): BatchNorm2d(344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (a_af): ReLU(inplace=True)\n",
       "        (b): Conv2d(344, 344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=43, bias=False)\n",
       "        (b_bn): BatchNorm2d(344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (b_af): ReLU(inplace=True)\n",
       "        (se): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "          (f_ex): Sequential(\n",
       "            (0): Conv2d(344, 86, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(86, 344, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (3): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (c): Conv2d(344, 344, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (c_bn): BatchNorm2d(344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (af): ReLU(inplace=True)\n",
       "      (downsample): Identity()\n",
       "      (drop_path): DropPath()\n",
       "    )\n",
       "    (b7): ResBottleneckBlock(\n",
       "      (f): BottleneckTransform(\n",
       "        (a): Conv2d(344, 344, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (a_bn): BatchNorm2d(344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (a_af): ReLU(inplace=True)\n",
       "        (b): Conv2d(344, 344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=43, bias=False)\n",
       "        (b_bn): BatchNorm2d(344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (b_af): ReLU(inplace=True)\n",
       "        (se): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "          (f_ex): Sequential(\n",
       "            (0): Conv2d(344, 86, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(86, 344, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (3): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (c): Conv2d(344, 344, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (c_bn): BatchNorm2d(344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (af): ReLU(inplace=True)\n",
       "      (downsample): Identity()\n",
       "      (drop_path): DropPath()\n",
       "    )\n",
       "    (b8): ResBottleneckBlock(\n",
       "      (f): BottleneckTransform(\n",
       "        (a): Conv2d(344, 344, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (a_bn): BatchNorm2d(344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (a_af): ReLU(inplace=True)\n",
       "        (b): Conv2d(344, 344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=43, bias=False)\n",
       "        (b_bn): BatchNorm2d(344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (b_af): ReLU(inplace=True)\n",
       "        (se): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "          (f_ex): Sequential(\n",
       "            (0): Conv2d(344, 86, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(86, 344, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (3): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (c): Conv2d(344, 344, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (c_bn): BatchNorm2d(344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (af): ReLU(inplace=True)\n",
       "      (downsample): Identity()\n",
       "      (drop_path): DropPath()\n",
       "    )\n",
       "    (b9): ResBottleneckBlock(\n",
       "      (f): BottleneckTransform(\n",
       "        (a): Conv2d(344, 344, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (a_bn): BatchNorm2d(344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (a_af): ReLU(inplace=True)\n",
       "        (b): Conv2d(344, 344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=43, bias=False)\n",
       "        (b_bn): BatchNorm2d(344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (b_af): ReLU(inplace=True)\n",
       "        (se): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "          (f_ex): Sequential(\n",
       "            (0): Conv2d(344, 86, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(86, 344, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (3): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (c): Conv2d(344, 344, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (c_bn): BatchNorm2d(344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (af): ReLU(inplace=True)\n",
       "      (downsample): Identity()\n",
       "      (drop_path): DropPath()\n",
       "    )\n",
       "  )\n",
       "  (s3): AnyStage(\n",
       "    (b1): ResBottleneckBlock(\n",
       "      (f): BottleneckTransform(\n",
       "        (a): Conv2d(344, 1008, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (a_bn): BatchNorm2d(1008, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (a_af): ReLU(inplace=True)\n",
       "        (b): Conv2d(1008, 1008, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=126, bias=False)\n",
       "        (b_bn): BatchNorm2d(1008, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (b_af): ReLU(inplace=True)\n",
       "        (se): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "          (f_ex): Sequential(\n",
       "            (0): Conv2d(1008, 86, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(86, 1008, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (3): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (c): Conv2d(1008, 1008, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (c_bn): BatchNorm2d(1008, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (af): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (1): ConvNormAct(\n",
       "          (conv): Conv2d(344, 1008, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            1008, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "    )\n",
       "    (b2): ResBottleneckBlock(\n",
       "      (f): BottleneckTransform(\n",
       "        (a): Conv2d(1008, 1008, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (a_bn): BatchNorm2d(1008, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (a_af): ReLU(inplace=True)\n",
       "        (b): Conv2d(1008, 1008, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=126, bias=False)\n",
       "        (b_bn): BatchNorm2d(1008, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (b_af): ReLU(inplace=True)\n",
       "        (se): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "          (f_ex): Sequential(\n",
       "            (0): Conv2d(1008, 252, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(252, 1008, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (3): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (c): Conv2d(1008, 1008, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (c_bn): BatchNorm2d(1008, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (af): ReLU(inplace=True)\n",
       "      (downsample): Identity()\n",
       "      (drop_path): DropPath()\n",
       "    )\n",
       "    (b3): ResBottleneckBlock(\n",
       "      (f): BottleneckTransform(\n",
       "        (a): Conv2d(1008, 1008, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (a_bn): BatchNorm2d(1008, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (a_af): ReLU(inplace=True)\n",
       "        (b): Conv2d(1008, 1008, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=126, bias=False)\n",
       "        (b_bn): BatchNorm2d(1008, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (b_af): ReLU(inplace=True)\n",
       "        (se): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "          (f_ex): Sequential(\n",
       "            (0): Conv2d(1008, 252, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(252, 1008, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (3): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (c): Conv2d(1008, 1008, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (c_bn): BatchNorm2d(1008, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (af): ReLU(inplace=True)\n",
       "      (downsample): Identity()\n",
       "      (drop_path): DropPath()\n",
       "    )\n",
       "    (b4): ResBottleneckBlock(\n",
       "      (f): BottleneckTransform(\n",
       "        (a): Conv2d(1008, 1008, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (a_bn): BatchNorm2d(1008, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (a_af): ReLU(inplace=True)\n",
       "        (b): Conv2d(1008, 1008, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=126, bias=False)\n",
       "        (b_bn): BatchNorm2d(1008, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (b_af): ReLU(inplace=True)\n",
       "        (se): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "          (f_ex): Sequential(\n",
       "            (0): Conv2d(1008, 252, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(252, 1008, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (3): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (c): Conv2d(1008, 1008, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (c_bn): BatchNorm2d(1008, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (af): ReLU(inplace=True)\n",
       "      (downsample): Identity()\n",
       "      (drop_path): DropPath()\n",
       "    )\n",
       "  )\n",
       "  (head): AnyHead(\n",
       "    (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=1008, out_features=10, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " weights_file=\"/home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/kwnowledge_distillation/vanilla/16_08_2024_11_44/Mateo/awesome_dodo/student_best\"\n",
    "config_file=f\"/home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/{metadata['codename']}/awesome_dodo/config.yaml\"\n",
    "teacher, info_teacher=rg.load_model(config_file=config_file, weights_file=weights_file,  config_updates=None)\n",
    "teacher.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b4131f-d0cf-4471-95f4-229fea5123b0",
   "metadata": {},
   "source": [
    "# Train student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28327bd5-fed2-4066-a9a8-99c30d8773a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_cfg import  validate\n",
    "import gc\n",
    "from distillation.base import Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1dea7bcf-6cee-4de8-853d-5eef26721ccd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name='abiding_markhor'\n",
    "weights_file=f\"/home/woody/iwb3/iwb3021h/NAS_COMPETITION_RESULTS/classifier_train/{metadata[\"codename\"]}/{model_name}/student_best\"\n",
    "state = load_checkpoint(weights_file)\n",
    "models[model_name].load_state_dict(state[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e85c8e33-10e4-4eac-ad00-efa15eaeb096",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clear_output(wait=True)\n",
    "distiller_teach=Vanilla(models['rampant_myna'])\n",
    "#distiller_teach=Vanilla(teacher)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1528bfac-3d48-4913-bee5-428cf12587ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.3133, device='cuda:0'),\n",
       " tensor(5.3133, device='cuda:0'),\n",
       " 3.0034738519032795)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(valid_loader, distiller_teach, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f69df683-1b87-4f53-b386-98e3b2c2d03c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m         gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrampant_myna\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mtrain_mp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43mteacher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m, in \u001b[0;36mtrain_mp\u001b[0;34m(student, student_name, teacher, metadata, test_folder, device, train_loader, valid_loader)\u001b[0m\n\u001b[1;32m      4\u001b[0m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiment_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m trainer\u001b[38;5;241m=\u001b[39mTrainerDistillation(student, device, train_loader, valid_loader,metadata)\u001b[38;5;66;03m#,[teacher]) \u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      8\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m~/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/anki_lab_submission/tests/../trainer.py:214\u001b[0m, in \u001b[0;36mTrainerDistillation.train\u001b[0;34m(self, return_acc)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m##############################################\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m epoch \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mSOLVER\u001b[38;5;241m.\u001b[39mEPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m     epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28mprint\u001b[39m(log_msg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest accuracy:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_acc), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEVAL\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/anki_lab_submission/tests/../trainer.py:246\u001b[0m, in \u001b[0;36mTrainerDistillation.train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistiller\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader):\n\u001b[0;32m--> 246\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_meters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;66;03m#ic(idx)\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m#    pbar.set_description(log_msg(msg, \"TRAIN\"))\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m#    pbar.update()\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mSOLVER\u001b[38;5;241m.\u001b[39mWARMUP:\n",
      "File \u001b[0;32m~/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/anki_lab_submission/tests/../trainer.py:301\u001b[0m, in \u001b[0;36mTrainerDistillation.train_iter\u001b[0;34m(self, data, epoch, train_meters)\u001b[0m\n\u001b[1;32m    298\u001b[0m     target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mcuda(non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m preds, losses_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistiller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# backward\u001b[39;00m\n\u001b[1;32m    303\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([l\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m losses_dict\u001b[38;5;241m.\u001b[39mvalues()])\n",
      "File \u001b[0;32m/apps/jupyterhub/jh3.1.1-py3.11/envs/pytorch-2.2.0/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/apps/jupyterhub/jh3.1.1-py3.11/envs/pytorch-2.2.0/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/anki_lab_submission/tests/../distillation/base.py:66\u001b[0m, in \u001b[0;36mVanilla.forward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m---> 66\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_train\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_test(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/anki_lab_submission/tests/../distillation/base.py:60\u001b[0m, in \u001b[0;36mVanilla.forward_train\u001b[0;34m(self, image, target, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, target, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m#logits_student, _ = self.student(image)\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     logits_student \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstudent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mlogits_student, target\u001b[38;5;241m=\u001b[39mtarget, label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits_student, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mce\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss}\n",
      "File \u001b[0;32m/apps/jupyterhub/jh3.1.1-py3.11/envs/pytorch-2.2.0/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/apps/jupyterhub/jh3.1.1-py3.11/envs/pytorch-2.2.0/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/anki_lab_submission/tests/../search_space/models/anynet_modified.py:420\u001b[0m, in \u001b[0;36mAnyNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 420\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/apps/jupyterhub/jh3.1.1-py3.11/envs/pytorch-2.2.0/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/apps/jupyterhub/jh3.1.1-py3.11/envs/pytorch-2.2.0/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/anki_lab_submission/tests/../search_space/models/anynet_modified.py:369\u001b[0m, in \u001b[0;36mAnyStage.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 369\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/apps/jupyterhub/jh3.1.1-py3.11/envs/pytorch-2.2.0/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/apps/jupyterhub/jh3.1.1-py3.11/envs/pytorch-2.2.0/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/anki_lab_submission/tests/../search_space/models/anynet_modified.py:258\u001b[0m, in \u001b[0;36mResBottleneckBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;66;03m#x_p = self.bn(self.proj(x)) if self.proj else x\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     shortcut\u001b[38;5;241m=\u001b[39mx\n\u001b[0;32m--> 258\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;66;03m# NOTE stuck with downsample as the attr name due to weight compatibility\u001b[39;00m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;66;03m# now represents the shortcut, no shortcut if None, and non-downsample shortcut == nn.Identity()\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(shortcut)\n",
      "File \u001b[0;32m/apps/jupyterhub/jh3.1.1-py3.11/envs/pytorch-2.2.0/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/apps/jupyterhub/jh3.1.1-py3.11/envs/pytorch-2.2.0/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/anki_lab_submission/tests/../search_space/models/anynet_modified.py:213\u001b[0m, in \u001b[0;36mBottleneckTransform.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 213\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/apps/jupyterhub/jh3.1.1-py3.11/envs/pytorch-2.2.0/lib/python3.12/site-packages/torch/nn/modules/module.py:2283\u001b[0m, in \u001b[0;36mModule.children\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchildren\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return an iterator over immediate children modules.\u001b[39;00m\n\u001b[1;32m   2279\u001b[0m \n\u001b[1;32m   2280\u001b[0m \u001b[38;5;124;03m    Yields:\u001b[39;00m\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;124;03m        Module: a child module\u001b[39;00m\n\u001b[1;32m   2282\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_children\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2284\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\n",
      "File \u001b[0;32m/apps/jupyterhub/jh3.1.1-py3.11/envs/pytorch-2.2.0/lib/python3.12/site-packages/torch/nn/modules/module.py:2303\u001b[0m, in \u001b[0;36mModule.named_children\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2301\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m module \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m memo:\n\u001b[0;32m-> 2303\u001b[0m         \u001b[43mmemo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2304\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m name, module\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_mp(student,student_name,teacher,  metadata, test_folder, device, train_loader,valid_loader):\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        metadata[\"experiment_name\"]=f\"{test_folder}/{student_name}\"\n",
    "        trainer=TrainerDistillation(student, device, train_loader, valid_loader,metadata)#,[teacher]) \n",
    "        trainer.train()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "for name in [\"rampant_myna\"]:\n",
    "    train_mp(models[name],name,teacher, metadata, test_folder, device, train_loader,valid_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "205fa673-018d-4897-b0f8-e0220ee9e341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amaranth_ringtail', 'athletic_dogfish']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_names[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e3cdb9-fec7-4fcc-bbae-b684b3304311",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch-2.2.0]",
   "language": "python",
   "name": "conda-env-pytorch-2.2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
