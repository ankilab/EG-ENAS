### Starting TaskPrologue of job 915936 on tg096 at Mon 21 Oct 2024 02:14:01 PM CEST
Running on cores 96-127 with governor ondemand
Mon Oct 21 14:14:01 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.28.03              Driver Version: 560.28.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:C1:00.0 Off |                    0 |
| N/A   35C    P0             55W /  400W |       1MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

Loading python/pytorch-1.13py3.10
  Loading requirement: cuda/11.6.1
rm -Rf /home/woody/iwb3/iwb3021h/THESIS_RESULTS/scoring0
rm -Rf /home/woody/iwb3/iwb3021h/THESIS_RESULTS/package0
rm -Rf /home/woody/iwb3/iwb3021h/THESIS_RESULTS/package0
mkdir /home/woody/iwb3/iwb3021h/THESIS_RESULTS/package0
mkdir /home/woody/iwb3/iwb3021h/THESIS_RESULTS/package0/predictions
mkdir /home/woody/iwb3/iwb3021h/THESIS_RESULTS/package0/datasets
rsync -ar --exclude='**/test_y.npy' datasets/* /home/woody/iwb3/iwb3021h/THESIS_RESULTS/package0/datasets/
cp -R evaluation/main.py /home/woody/iwb3/iwb3021h/THESIS_RESULTS/package0/main.py
cp -R anki_lab_submission/* /home/woody/iwb3/iwb3021h/THESIS_RESULTS/package0
cd /home/woody/iwb3/iwb3021h/THESIS_RESULTS/package0; python3 main.py --mode T0 --select_augment Proxy
ic| self.select_augment: 'Proxy'
ic| self.x.shape: torch.Size([148700, 3, 16, 16])
ic| unique_values: array([0.02745098, 0.05098039, 0.05490196, 0.05882353, 0.0627451 ,
                          0.07058824, 0.07450981, 0.07843138, 0.08627451, 0.09019608,
                          0.09411765, 0.09803922, 0.10196079, 0.10980392, 0.11372549,
                          0.11764706, 0.12156863, 0.1254902 , 0.12941177, 0.13333334,
                          0.13725491, 0.14117648, 0.14509805, 0.14901961, 0.15294118,
                          0.15686275, 0.16078432, 0.16470589, 0.16862746, 0.17254902,
                          0.1764706 , 0.18039216, 0.18431373, 0.1882353 , 0.19215687,
                          0.19607843, 0.2       , 0.20392157, 0.21176471, 0.21568628,
                          0.21960784, 0.22352941, 0.22745098, 0.23137255, 0.23529412,
                          0.23921569, 0.24313726, 0.24705882, 0.2509804 , 0.25490198,
                          0.25882354, 0.2627451 , 0.26666668, 0.27058825, 0.27450982,
                          0.2784314 , 0.2901961 , 0.29411766, 0.29803923, 0.3019608 ,
                          0.3137255 , 0.31764707, 0.3254902 , 0.32941177, 0.33333334,
                          0.3372549 , 0.34117648, 0.34509805, 0.34901962, 0.3529412 ,
                          0.35686275, 0.36078432, 0.3647059 , 0.36862746, 0.37254903,
                          0.3764706 , 0.38039216, 0.38431373, 0.3882353 , 0.39215687,
                          0.39607844, 0.40392157, 0.40784314, 0.4117647 , 0.41568628,
                          0.41960785, 0.42352942, 0.43137255, 0.43529412, 0.4392157 ,
                          0.44313726, 0.44705883, 0.4509804 , 0.45490196, 0.45882353,
                          0.4627451 , 0.46666667, 0.47058824, 0.4745098 , 0.48235294,
                          0.4862745 , 0.49019608, 0.49411765, 0.49803922, 0.5019608 ,
                          0.5058824 , 0.50980395, 0.5137255 , 0.5176471 , 0.52156866,
                          0.5254902 , 0.5294118 , 0.53333336, 0.5372549 , 0.5411765 ,
                          0.54509807, 0.54901963, 0.5529412 , 0.5568628 , 0.56078434,
                          0.5647059 , 0.5686275 , 0.57254905, 0.5764706 , 0.5803922 ,
                          0.58431375, 0.5882353 , 0.5921569 , 0.59607846, 0.6       ,
                          0.6039216 , 0.60784316, 0.6117647 , 0.6156863 , 0.61960787,
                          0.62352943, 0.627451  , 0.6313726 , 0.63529414, 0.6392157 ,
                          0.6431373 , 0.64705884, 0.6509804 , 0.654902  , 0.65882355,
                          0.6627451 , 0.6666667 , 0.67058825, 0.6745098 , 0.6784314 ,
                          0.68235296, 0.6862745 , 0.6901961 , 0.69411767, 0.69803923,
                          0.7019608 , 0.7058824 , 0.70980394, 0.7137255 , 0.7176471 ,
                          0.72156864, 0.7254902 , 0.7294118 , 0.73333335, 0.7372549 ,
                          0.7411765 , 0.74509805, 0.7490196 , 0.7529412 , 0.75686276,
                          0.7607843 , 0.7647059 , 0.76862746, 0.77254903, 0.7764706 ,
                          0.78039217, 0.78431374, 0.7882353 , 0.7921569 , 0.79607844,
                          0.8       , 0.8039216 , 0.80784315, 0.8117647 , 0.8156863 ,
                          0.8235294 , 0.83137256, 0.8352941 , 0.8392157 , 0.84313726,
                          0.85882354, 0.8666667 , 0.87058824, 0.8745098 , 0.8784314 ,
                          0.88235295, 0.8980392 , 0.90588236, 0.9098039 , 0.9137255 ,
                          0.91764706, 0.92156863, 0.9254902 , 0.92941177, 0.93333334,
                          0.9372549 , 0.9411765 , 0.94509804, 0.95686275, 0.9607843 ],
                         dtype=float32)
ic| C: 3
ic| H: 16
ic| PH: 2
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.
  warnings.warn(
ic| poss_augs: [[],
                [RandAugment(interpolation=InterpolationMode.NEAREST, num_ops=2, magnitude=9, num_magnitude_bins=31)],
                [RandAugment(interpolation=InterpolationMode.NEAREST, num_ops=2, magnitude=5, num_magnitude_bins=31)],
                [RandAugment(interpolation=InterpolationMode.NEAREST, num_ops=2, magnitude=1, num_magnitude_bins=31)],
                [TrivialAugmentWide(interpolation=InterpolationMode.NEAREST, num_magnitude_bins=31)],
                [TrivialAugmentWide(interpolation=InterpolationMode.NEAREST, num_magnitude_bins=15)],
                [AugMix(interpolation=InterpolationMode.BILINEAR, severity=3, mixture_width=3, chain_depth=-1, alpha=1.0, all_ops=True)],
                [AugMix(interpolation=InterpolationMode.BILINEAR, severity=1, mixture_width=3, chain_depth=-1, alpha=1.0, all_ops=True)],
                [RandomHorizontalFlip(p=0.5), RandomVerticalFlip(p=0.5)],
                [RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                 RandomHorizontalFlip(p=0.5),
                 RandomVerticalFlip(p=0.5)],
                [RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False)],
                [RandomErasing(p=0.2, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                 RandomCrop(size=(16, 16), padding=[2, 2, 2, 2], pad_if_needed=False, fill=0, padding_mode=constant)],
                [RandomCrop(size=(16, 16), padding=[2, 2, 2, 2], pad_if_needed=False, fill=0, padding_mode=constant)],
                [RandomCrop(size=(16, 16), padding=[2, 2, 2, 2], pad_if_needed=False, fill=0, padding_mode=constant),
                 RandomHorizontalFlip(p=0.5),
                 RandomVerticalFlip(p=0.5)],
                [RandomErasing(p=0.2, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                 RandomCrop(size=(16, 16), padding=[2, 2, 2, 2], pad_if_needed=False, fill=0, padding_mode=constant),
                 RandomHorizontalFlip(p=0.5)],
                [<data_processor.RandomPixelChange object at 0x7f77968a3fa0>, ToTensor()],
                [<data_processor.RandomPixelChange object at 0x7f77968a0f40>, ToTensor()],
                [<data_processor.RandomPixelChange object at 0x7f77968a1030>, ToTensor()],
                [<data_processor.RandomPixelChange object at 0x7f77968a0f70>,
                 ToTensor(),
                 RandomHorizontalFlip(p=0.5),
                 RandomVerticalFlip(p=0.5)],
                [<data_processor.RandomPixelChange object at 0x7f77968a1150>,
                 ToTensor(),
                 RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False)],
                [<data_processor.RandomPixelChange object at 0x7f77968a1870>,
                 ToTensor(),
                 RandomCrop(size=(16, 16), padding=[2, 2, 2, 2], pad_if_needed=False, fill=0, padding_mode=constant)],
                [<data_processor.RandomPixelChange object at 0x7f77968a1330>,
                 ToTensor(),
                 RandomHorizontalFlip(p=0.5),
                 RandomVerticalFlip(p=0.5),
                 RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False)]]
ic| individuals: ['daring_crayfish',
                  'dandelion_turkey',
                  'tidy_newt',
                  'spectral_dolphin',
                  'resourceful_okapi',
                  'meticulous_panda',
                  'archetypal_vicugna',
                  'adept_magpie',
                  'fortunate_oryx',
                  'agile_labrador']
ic| params: [48.0, 120, 2.1499999999999995, 12, 8]
ic| params: [64.0, 72, 2.799999999999997, 18, 8]
ic| params: [16.0, 72, 2.05, 22, 8]
ic| params: [48.0, 112, 2.05, 19, 8]
ic| params: [56.0, 112, 2.3999999999999986, 17, 8]
ic| params: [16.0, 16, 2.05, 18, 8]
ic| params: [56.0, 120, 2.6999999999999975, 19, 8]
ic| params: [64.0, 72, 2.799999999999997, 16, 8]
ic| params: [24.0, 96, 2.3999999999999986, 20, 8]
ic| params: [16.0, 64, 2.899999999999997, 22, 8]
ic| individuals: ['daring_crayfish',
                  'dandelion_turkey',
                  'tidy_newt',
                  'spectral_dolphin',
                  'resourceful_okapi',
                  'meticulous_panda',
                  'archetypal_vicugna',
                  'adept_magpie',
                  'fortunate_oryx',
                  'agile_labrador']
ic| params_dict: {'adept_magpie': [64.0, 72, 2.799999999999997, 16, 8],
                  'agile_labrador': [16.0, 64, 2.899999999999997, 22, 8],
                  'archetypal_vicugna': [56.0, 120, 2.6999999999999975, 19, 8],
                  'dandelion_turkey': [64.0, 72, 2.799999999999997, 18, 8],
                  'daring_crayfish': [48.0, 120, 2.1499999999999995, 12, 8],
                  'fortunate_oryx': [24.0, 96, 2.3999999999999986, 20, 8],
                  'meticulous_panda': [16.0, 16, 2.05, 18, 8],
                  'resourceful_okapi': [56.0, 112, 2.3999999999999986, 17, 8],
                  'spectral_dolphin': [48.0, 112, 2.05, 19, 8],
                  'tidy_newt': [16.0, 72, 2.05, 22, 8]}
ic| '#############'
ic| aug: 0
ic| train_loader.dataset.transform.transforms: [Normalize(mean=tensor([0.4712, 0.4543, 0.3944]), std=tensor([0.2319, 0.2252, 0.2321]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 1
ic| train_loader.dataset.transform.transforms: [RandAugment(interpolation=InterpolationMode.NEAREST, num_ops=2, magnitude=9, num_magnitude_bins=31),
                                                Normalize(mean=tensor([0.4712, 0.4543, 0.3944]), std=tensor([0.2319, 0.2252, 0.2321]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 2
ic| train_loader.dataset.transform.transforms: [RandAugment(interpolation=InterpolationMode.NEAREST, num_ops=2, magnitude=5, num_magnitude_bins=31),
                                                Normalize(mean=tensor([0.4712, 0.4543, 0.3944]), std=tensor([0.2319, 0.2252, 0.2321]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 3
ic| train_loader.dataset.transform.transforms: [RandAugment(interpolation=InterpolationMode.NEAREST, num_ops=2, magnitude=1, num_magnitude_bins=31),
                                                Normalize(mean=tensor([0.4712, 0.4543, 0.3944]), std=tensor([0.2319, 0.2252, 0.2321]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 4
ic| train_loader.dataset.transform.transforms: [TrivialAugmentWide(interpolation=InterpolationMode.NEAREST, num_magnitude_bins=31),
                                                Normalize(mean=tensor([0.4712, 0.4543, 0.3944]), std=tensor([0.2319, 0.2252, 0.2321]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 5
ic| train_loader.dataset.transform.transforms: [TrivialAugmentWide(interpolation=InterpolationMode.NEAREST, num_magnitude_bins=15),
                                                Normalize(mean=tensor([0.4712, 0.4543, 0.3944]), std=tensor([0.2319, 0.2252, 0.2321]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 6
ic| train_loader.dataset.transform.transforms: [AugMix(interpolation=InterpolationMode.BILINEAR, severity=3, mixture_width=3, chain_depth=-1, alpha=1.0, all_ops=True),
                                                Normalize(mean=tensor([0.4712, 0.4543, 0.3944]), std=tensor([0.2319, 0.2252, 0.2321]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 7
ic| train_loader.dataset.transform.transforms: [AugMix(interpolation=InterpolationMode.BILINEAR, severity=1, mixture_width=3, chain_depth=-1, alpha=1.0, all_ops=True),
                                                Normalize(mean=tensor([0.4712, 0.4543, 0.3944]), std=tensor([0.2319, 0.2252, 0.2321]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 8
ic| train_loader.dataset.transform.transforms: [RandomHorizontalFlip(p=0.5),
                                                RandomVerticalFlip(p=0.5),
                                                Normalize(mean=tensor([0.4712, 0.4543, 0.3944]), std=tensor([0.2319, 0.2252, 0.2321]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 9
ic| train_loader.dataset.transform.transforms: [RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                                                RandomHorizontalFlip(p=0.5),
                                                RandomVerticalFlip(p=0.5),
                                                Normalize(mean=tensor([0.4712, 0.4543, 0.3944]), std=tensor([0.2319, 0.2252, 0.2321]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 10
ic| train_loader.dataset.transform.transforms: [RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                                                Normalize(mean=tensor([0.4712, 0.4543, 0.3944]), std=tensor([0.2319, 0.2252, 0.2321]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 11
ic| train_loader.dataset.transform.transforms: [RandomErasing(p=0.2, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                                                RandomCrop(size=(16, 16), padding=[2, 2, 2, 2], pad_if_needed=False, fill=0, padding_mode=constant),
                                                Normalize(mean=tensor([0.4712, 0.4543, 0.3944]), std=tensor([0.2319, 0.2252, 0.2321]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 12
ic| train_loader.dataset.transform.transforms: [RandomCrop(size=(16, 16), padding=[2, 2, 2, 2], pad_if_needed=False, fill=0, padding_mode=constant),
                                                Normalize(mean=tensor([0.4712, 0.4543, 0.3944]), std=tensor([0.2319, 0.2252, 0.2321]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 13
ic| train_loader.dataset.transform.transforms: [RandomCrop(size=(16, 16), padding=[2, 2, 2, 2], pad_if_needed=False, fill=0, padding_mode=constant),
                                                RandomHorizontalFlip(p=0.5),
                                                RandomVerticalFlip(p=0.5),
                                                Normalize(mean=tensor([0.4712, 0.4543, 0.3944]), std=tensor([0.2319, 0.2252, 0.2321]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 14
ic| train_loader.dataset.transform.transforms: [RandomErasing(p=0.2, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                                                RandomCrop(size=(16, 16), padding=[2, 2, 2, 2], pad_if_needed=False, fill=0, padding_mode=constant),
                                                RandomHorizontalFlip(p=0.5),
                                                Normalize(mean=tensor([0.4712, 0.4543, 0.3944]), std=tensor([0.2319, 0.2252, 0.2321]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 15
ic| train_loader.dataset.transform.transforms: [<data_processor.RandomPixelChange object at 0x7f77968a3fa0>,
                                                ToTensor(),
                                                Normalize(mean=tensor([0.4712, 0.4543, 0.3944]), std=tensor([0.2319, 0.2252, 0.2321]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 16
ic| train_loader.dataset.transform.transforms: [<data_processor.RandomPixelChange object at 0x7f77968a0f40>,
                                                ToTensor(),
                                                Normalize(mean=tensor([0.4712, 0.4543, 0.3944]), std=tensor([0.2319, 0.2252, 0.2321]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 17
ic| train_loader.dataset.transform.transforms: [<data_processor.RandomPixelChange object at 0x7f77968a1030>,
                                                ToTensor(),
                                                Normalize(mean=tensor([0.4712, 0.4543, 0.3944]), std=tensor([0.2319, 0.2252, 0.2321]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 18
ic| train_loader.dataset.transform.transforms: [<data_processor.RandomPixelChange object at 0x7f77968a0f70>,
                                                ToTensor(),
                                                RandomHorizontalFlip(p=0.5),
                                                RandomVerticalFlip(p=0.5),
                                                Normalize(mean=tensor([0.4712, 0.4543, 0.3944]), std=tensor([0.2319, 0.2252, 0.2321]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 19
ic| train_loader.dataset.transform.transforms: [<data_processor.RandomPixelChange object at 0x7f77968a1150>,
                                                ToTensor(),
                                                RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                                                Normalize(mean=tensor([0.4712, 0.4543, 0.3944]), std=tensor([0.2319, 0.2252, 0.2321]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 20
ic| train_loader.dataset.transform.transforms: [<data_processor.RandomPixelChange object at 0x7f77968a1870>,
                                                ToTensor(),
                                                RandomCrop(size=(16, 16), padding=[2, 2, 2, 2], pad_if_needed=False, fill=0, padding_mode=constant),
                                                Normalize(mean=tensor([0.4712, 0.4543, 0.3944]), std=tensor([0.2319, 0.2252, 0.2321]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 21
ic| train_loader.dataset.transform.transforms: [<data_processor.RandomPixelChange object at 0x7f77968a1330>,
                                                ToTensor(),
                                                RandomHorizontalFlip(p=0.5),
                                                RandomVerticalFlip(p=0.5),
                                                RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                                                Normalize(mean=tensor([0.4712, 0.4543, 0.3944]), std=tensor([0.2319, 0.2252, 0.2321]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/home/woody/iwb3/iwb3021h/THESIS_RESULTS/package0/data_processor.py:590: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  subset_df_no_outliers["fisher"] = scaler.fit_transform(subset_df_no_outliers[["fisher"]])
/home/woody/iwb3/iwb3021h/THESIS_RESULTS/package0/data_processor.py:590: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  subset_df_no_outliers["fisher"] = scaler.fit_transform(subset_df_no_outliers[["fisher"]])
/home/woody/iwb3/iwb3021h/THESIS_RESULTS/package0/data_processor.py:608: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  subset_df_no_outliers["jacob_cov"] = scaler.fit_transform(subset_df_no_outliers[["jacob_cov"]])
/home/woody/iwb3/iwb3021h/THESIS_RESULTS/package0/data_processor.py:608: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  subset_df_no_outliers["jacob_cov"] = scaler.fit_transform(subset_df_no_outliers[["jacob_cov"]])
/home/woody/iwb3/iwb3021h/THESIS_RESULTS/package0/data_processor.py:608: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  subset_df_no_outliers["jacob_cov"] = scaler.fit_transform(subset_df_no_outliers[["jacob_cov"]])
/home/woody/iwb3/iwb3021h/THESIS_RESULTS/package0/data_processor.py:608: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  subset_df_no_outliers["jacob_cov"] = scaler.fit_transform(subset_df_no_outliers[["jacob_cov"]])
ic| f"best_augmentation: {best_aug}": 'best_augmentation: 4'
ic| f"selected transform {train_transform}": ('selected transform '
                                              '[TrivialAugmentWide(interpolation=InterpolationMode.NEAREST, '
                                              'num_magnitude_bins=31)]')
ic| self.x.shape: torch.Size([148700, 3, 16, 16])
ic| self.x.shape: torch.Size([3000, 3, 16, 16])
ic| self.x.shape: torch.Size([3000, 3, 16, 16])
ic| mode: 'T0'
ic| f"Mode {mode}": 'Mode T0'
ic| get_gpu_memory(0): 41553100800
ic| self.total_generations: 3
ic| 'Time remaining:'
ic| metadata['time_remaining']: 107640.40751862526
ic| self.zcost_nas: True
ic| self.cfg: CfgNode({'MODEL': CfgNode({'TYPE': 'regnet', 'NUM_CLASSES': 120, 'ACTIVATION_FUN': 'relu', 'ACTIVATION_INPLACE': True, 'SCALING_TYPE': '', 'SCALING_FACTOR': 1.0}), 'REGNET': CfgNode({'STEM_TYPE': 'res_stem_cifar', 'INPUT_CHANNELS': 3, 'STEM_W': 16, 'BLOCK_TYPE': 'res_bottleneck_block', 'STRIDE': 2, 'SE_ON': True, 'SE_R': 0.25, 'DEPTH': 20, 'W0': 232, 'WA': 115.89, 'WM': 2.53, 'GROUP_W': 8, 'BOT_MUL': 1.0, 'HEAD_W': 0, 'DOWNSAMPLE': 'avg', 'DROP_RATE': 0.01, 'DROPOUT': 0.2}), 'BN': CfgNode({'EPS': 1e-05, 'MOM': 0.1, 'ZERO_INIT_FINAL_GAMMA': False}), 'LN': CfgNode({'EPS': 1e-05}), 'DESC': ''})
ic| samples: 120
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator SGDRegressor from version 1.4.2 when using version 1.5.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:
https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations
  warnings.warn(
/home/woody/iwb3/iwb3021h/THESIS_RESULTS/package0/search_space/RegNet.py:299: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  ranking_test_df[pred_column]=sgd_regressor.predict(X_test)
ic| ranking_prediction_df:                     score
                           adorable_oryx  166.847189
ic| best_models.keys(): dict_keys(['adorable_oryx'])
ic| self.initial_population_size: 120
ic| metadata: {'benchmark': 46.38,
               'codename': 'in16',
               'input_shape': [148700, 3, 16, 16],
               'mode': 'NAS',
               'num_classes': 120,
               'test_type': 'T0_Proxy/seed_1',
               'time_remaining': 107623.23432660103,
               'train_config_path': 'configs/train/finetuning_generation_adam.yaml'}
ic| cfg_path: 'configs/train/finetuning_generation_adam.yaml'
ic| self.cfg.SOLVER.LR: 0.001
ic| self.cfg.SOLVER.EPOCHS-self.cfg.SOLVER.SWA_START: 10
ic| self.cfg.SOLVER.EPOCHS: 100
/home/woody/iwb3/iwb3021h/THESIS_RESULTS/package0/trainer.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(checkpoint_path, map_location=device)
ic| self.select_augment: 'Proxy'
ic| self.x.shape: torch.Size([50000, 20, 20, 20])
ic| unique_values: array([0., 1.], dtype=float32)
ic| C: 20
ic| H: 20
ic| PH: 2
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.
  warnings.warn(
ic| poss_augs: [[],
                [RandAugment(interpolation=InterpolationMode.NEAREST, num_ops=2, magnitude=9, num_magnitude_bins=31)],
                [RandAugment(interpolation=InterpolationMode.NEAREST, num_ops=2, magnitude=5, num_magnitude_bins=31)],
                [RandAugment(interpolation=InterpolationMode.NEAREST, num_ops=2, magnitude=1, num_magnitude_bins=31)],
                [TrivialAugmentWide(interpolation=InterpolationMode.NEAREST, num_magnitude_bins=31)],
                [TrivialAugmentWide(interpolation=InterpolationMode.NEAREST, num_magnitude_bins=15)],
                [AugMix(interpolation=InterpolationMode.BILINEAR, severity=3, mixture_width=3, chain_depth=-1, alpha=1.0, all_ops=True)],
                [AugMix(interpolation=InterpolationMode.BILINEAR, severity=1, mixture_width=3, chain_depth=-1, alpha=1.0, all_ops=True)],
                [RandomHorizontalFlip(p=0.5), RandomVerticalFlip(p=0.5)],
                [RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                 RandomHorizontalFlip(p=0.5),
                 RandomVerticalFlip(p=0.5)],
                [RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False)],
                [RandomErasing(p=0.2, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                 RandomCrop(size=(20, 20), padding=[2, 2, 2, 2], pad_if_needed=False, fill=0, padding_mode=constant)],
                [RandomCrop(size=(20, 20), padding=[2, 2, 2, 2], pad_if_needed=False, fill=0, padding_mode=constant)],
                [RandomCrop(size=(20, 20), padding=[2, 2, 2, 2], pad_if_needed=False, fill=0, padding_mode=constant),
                 RandomHorizontalFlip(p=0.5),
                 RandomVerticalFlip(p=0.5)],
                [RandomErasing(p=0.2, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                 RandomCrop(size=(20, 20), padding=[2, 2, 2, 2], pad_if_needed=False, fill=0, padding_mode=constant),
                 RandomHorizontalFlip(p=0.5)],
                [<data_processor.RandomPixelChange object at 0x7f7796896c80>, ToTensor()],
                [<data_processor.RandomPixelChange object at 0x7f7796895720>, ToTensor()],
                [<data_processor.RandomPixelChange object at 0x7f7796895ab0>, ToTensor()],
                [<data_processor.RandomPixelChange object at 0x7f7643fff9d0>,
                 ToTensor(),
                 RandomHorizontalFlip(p=0.5),
                 RandomVerticalFlip(p=0.5)],
                [<data_processor.RandomPixelChange object at 0x7f7643ffe980>,
                 ToTensor(),
                 RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False)],
                [<data_processor.RandomPixelChange object at 0x7f764234b6d0>,
                 ToTensor(),
                 RandomCrop(size=(20, 20), padding=[2, 2, 2, 2], pad_if_needed=False, fill=0, padding_mode=constant)],
                [<data_processor.RandomPixelChange object at 0x7f764234ada0>,
                 ToTensor(),
                 RandomHorizontalFlip(p=0.5),
                 RandomVerticalFlip(p=0.5),
                 RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False)]]
ic| individuals: ['holistic_axolotl',
                  'prudent_gopher',
                  'zircon_kudu',
                  'determined_kakapo',
                  'expert_binturong',
                  'wooden_gopher',
                  'nifty_chachalaca',
                  'vengeful_bustard',
                  'sloppy_mastiff',
                  'hallowed_cricket']
ic| params: [32.0, 64, 2.3999999999999986, 20, 8]
ic| params: [16.0, 96, 2.299999999999999, 21, 8]
ic| params: [56.0, 72, 2.4499999999999984, 15, 8]
ic| params: [56.0, 72, 2.599999999999998, 12, 8]
ic| params: [16.0, 104, 2.3499999999999988, 19, 8]
ic| params: [16.0, 40, 2.6499999999999977, 13, 8]
ic| params: [24.0, 120, 2.0999999999999996, 20, 8]
ic| params: [56.0, 96, 2.799999999999997, 19, 8]
ic| params: [16.0, 88, 2.299999999999999, 9, 8]
ic| params: [24.0, 72, 2.7499999999999973, 14, 8]
ic| individuals: ['holistic_axolotl',
                  'prudent_gopher',
                  'zircon_kudu',
                  'determined_kakapo',
                  'expert_binturong',
                  'wooden_gopher',
                  'nifty_chachalaca',
                  'vengeful_bustard',
                  'sloppy_mastiff',
                  'hallowed_cricket']
ic| params_dict: {'determined_kakapo': [56.0, 72, 2.599999999999998, 12, 8],
                  'expert_binturong': [16.0, 104, 2.3499999999999988, 19, 8],
                  'hallowed_cricket': [24.0, 72, 2.7499999999999973, 14, 8],
                  'holistic_axolotl': [32.0, 64, 2.3999999999999986, 20, 8],
                  'nifty_chachalaca': [24.0, 120, 2.0999999999999996, 20, 8],
                  'prudent_gopher': [16.0, 96, 2.299999999999999, 21, 8],
                  'sloppy_mastiff': [16.0, 88, 2.299999999999999, 9, 8],
                  'vengeful_bustard': [56.0, 96, 2.799999999999997, 19, 8],
                  'wooden_gopher': [16.0, 40, 2.6499999999999977, 13, 8],
                  'zircon_kudu': [56.0, 72, 2.4499999999999984, 15, 8]}
ic| '#############'
ic| aug: 0
ic| train_loader.dataset.transform.transforms: [Normalize(mean=tensor([0.0724, 0.0832, 0.0932, 0.1020, 0.1097, 0.1162, 0.1213, 0.1250, 0.1275,
                                                       0.1288, 0.1289, 0.1274, 0.1250, 0.1211, 0.1157, 0.1091, 0.1012, 0.0924,
                                                       0.0824, 0.0716]), std=tensor([0.2592, 0.2762, 0.2907, 0.3026, 0.3125, 0.3204, 0.3264, 0.3307, 0.3336,
                                                       0.3350, 0.3351, 0.3334, 0.3308, 0.3263, 0.3198, 0.3117, 0.3016, 0.2896,
                                                       0.2750, 0.2578]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 1
ic| train_loader.dataset.transform.transforms: [RandAugment(interpolation=InterpolationMode.NEAREST, num_ops=2, magnitude=9, num_magnitude_bins=31),
                                                Normalize(mean=tensor([0.0724, 0.0832, 0.0932, 0.1020, 0.1097, 0.1162, 0.1213, 0.1250, 0.1275,
                                                       0.1288, 0.1289, 0.1274, 0.1250, 0.1211, 0.1157, 0.1091, 0.1012, 0.0924,
                                                       0.0824, 0.0716]), std=tensor([0.2592, 0.2762, 0.2907, 0.3026, 0.3125, 0.3204, 0.3264, 0.3307, 0.3336,
                                                       0.3350, 0.3351, 0.3334, 0.3308, 0.3263, 0.3198, 0.3117, 0.3016, 0.2896,
                                                       0.2750, 0.2578]))]
ic| '#############'
ic| aug: 2
ic| train_loader.dataset.transform.transforms: [RandAugment(interpolation=InterpolationMode.NEAREST, num_ops=2, magnitude=5, num_magnitude_bins=31),
                                                Normalize(mean=tensor([0.0724, 0.0832, 0.0932, 0.1020, 0.1097, 0.1162, 0.1213, 0.1250, 0.1275,
                                                       0.1288, 0.1289, 0.1274, 0.1250, 0.1211, 0.1157, 0.1091, 0.1012, 0.0924,
                                                       0.0824, 0.0716]), std=tensor([0.2592, 0.2762, 0.2907, 0.3026, 0.3125, 0.3204, 0.3264, 0.3307, 0.3336,
                                                       0.3350, 0.3351, 0.3334, 0.3308, 0.3263, 0.3198, 0.3117, 0.3016, 0.2896,
                                                       0.2750, 0.2578]))]
ic| '#############'
ic| aug: 3
ic| train_loader.dataset.transform.transforms: [RandAugment(interpolation=InterpolationMode.NEAREST, num_ops=2, magnitude=1, num_magnitude_bins=31),
                                                Normalize(mean=tensor([0.0724, 0.0832, 0.0932, 0.1020, 0.1097, 0.1162, 0.1213, 0.1250, 0.1275,
                                                       0.1288, 0.1289, 0.1274, 0.1250, 0.1211, 0.1157, 0.1091, 0.1012, 0.0924,
                                                       0.0824, 0.0716]), std=tensor([0.2592, 0.2762, 0.2907, 0.3026, 0.3125, 0.3204, 0.3264, 0.3307, 0.3336,
                                                       0.3350, 0.3351, 0.3334, 0.3308, 0.3263, 0.3198, 0.3117, 0.3016, 0.2896,
                                                       0.2750, 0.2578]))]
ic| '#############'
ic| aug: 4
ic| train_loader.dataset.transform.transforms: [TrivialAugmentWide(interpolation=InterpolationMode.NEAREST, num_magnitude_bins=31),
                                                Normalize(mean=tensor([0.0724, 0.0832, 0.0932, 0.1020, 0.1097, 0.1162, 0.1213, 0.1250, 0.1275,
                                                       0.1288, 0.1289, 0.1274, 0.1250, 0.1211, 0.1157, 0.1091, 0.1012, 0.0924,
                                                       0.0824, 0.0716]), std=tensor([0.2592, 0.2762, 0.2907, 0.3026, 0.3125, 0.3204, 0.3264, 0.3307, 0.3336,
                                                       0.3350, 0.3351, 0.3334, 0.3308, 0.3263, 0.3198, 0.3117, 0.3016, 0.2896,
                                                       0.2750, 0.2578]))]
ic| '#############'
ic| aug: 5
ic| train_loader.dataset.transform.transforms: [TrivialAugmentWide(interpolation=InterpolationMode.NEAREST, num_magnitude_bins=15),
                                                Normalize(mean=tensor([0.0724, 0.0832, 0.0932, 0.1020, 0.1097, 0.1162, 0.1213, 0.1250, 0.1275,
                                                       0.1288, 0.1289, 0.1274, 0.1250, 0.1211, 0.1157, 0.1091, 0.1012, 0.0924,
                                                       0.0824, 0.0716]), std=tensor([0.2592, 0.2762, 0.2907, 0.3026, 0.3125, 0.3204, 0.3264, 0.3307, 0.3336,
                                                       0.3350, 0.3351, 0.3334, 0.3308, 0.3263, 0.3198, 0.3117, 0.3016, 0.2896,
                                                       0.2750, 0.2578]))]
ic| '#############'
ic| aug: 6
ic| train_loader.dataset.transform.transforms: [AugMix(interpolation=InterpolationMode.BILINEAR, severity=3, mixture_width=3, chain_depth=-1, alpha=1.0, all_ops=True),
                                                Normalize(mean=tensor([0.0724, 0.0832, 0.0932, 0.1020, 0.1097, 0.1162, 0.1213, 0.1250, 0.1275,
                                                       0.1288, 0.1289, 0.1274, 0.1250, 0.1211, 0.1157, 0.1091, 0.1012, 0.0924,
                                                       0.0824, 0.0716]), std=tensor([0.2592, 0.2762, 0.2907, 0.3026, 0.3125, 0.3204, 0.3264, 0.3307, 0.3336,
                                                       0.3350, 0.3351, 0.3334, 0.3308, 0.3263, 0.3198, 0.3117, 0.3016, 0.2896,
                                                       0.2750, 0.2578]))]
ic| '#############'
ic| aug: 7
ic| train_loader.dataset.transform.transforms: [AugMix(interpolation=InterpolationMode.BILINEAR, severity=1, mixture_width=3, chain_depth=-1, alpha=1.0, all_ops=True),
                                                Normalize(mean=tensor([0.0724, 0.0832, 0.0932, 0.1020, 0.1097, 0.1162, 0.1213, 0.1250, 0.1275,
                                                       0.1288, 0.1289, 0.1274, 0.1250, 0.1211, 0.1157, 0.1091, 0.1012, 0.0924,
                                                       0.0824, 0.0716]), std=tensor([0.2592, 0.2762, 0.2907, 0.3026, 0.3125, 0.3204, 0.3264, 0.3307, 0.3336,
                                                       0.3350, 0.3351, 0.3334, 0.3308, 0.3263, 0.3198, 0.3117, 0.3016, 0.2896,
                                                       0.2750, 0.2578]))]
ic| '#############'
ic| aug: 8
ic| train_loader.dataset.transform.transforms: [RandomHorizontalFlip(p=0.5),
                                                RandomVerticalFlip(p=0.5),
                                                Normalize(mean=tensor([0.0724, 0.0832, 0.0932, 0.1020, 0.1097, 0.1162, 0.1213, 0.1250, 0.1275,
                                                       0.1288, 0.1289, 0.1274, 0.1250, 0.1211, 0.1157, 0.1091, 0.1012, 0.0924,
                                                       0.0824, 0.0716]), std=tensor([0.2592, 0.2762, 0.2907, 0.3026, 0.3125, 0.3204, 0.3264, 0.3307, 0.3336,
                                                       0.3350, 0.3351, 0.3334, 0.3308, 0.3263, 0.3198, 0.3117, 0.3016, 0.2896,
                                                       0.2750, 0.2578]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 9
ic| train_loader.dataset.transform.transforms: [RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                                                RandomHorizontalFlip(p=0.5),
                                                RandomVerticalFlip(p=0.5),
                                                Normalize(mean=tensor([0.0724, 0.0832, 0.0932, 0.1020, 0.1097, 0.1162, 0.1213, 0.1250, 0.1275,
                                                       0.1288, 0.1289, 0.1274, 0.1250, 0.1211, 0.1157, 0.1091, 0.1012, 0.0924,
                                                       0.0824, 0.0716]), std=tensor([0.2592, 0.2762, 0.2907, 0.3026, 0.3125, 0.3204, 0.3264, 0.3307, 0.3336,
                                                       0.3350, 0.3351, 0.3334, 0.3308, 0.3263, 0.3198, 0.3117, 0.3016, 0.2896,
                                                       0.2750, 0.2578]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 10
ic| train_loader.dataset.transform.transforms: [RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                                                Normalize(mean=tensor([0.0724, 0.0832, 0.0932, 0.1020, 0.1097, 0.1162, 0.1213, 0.1250, 0.1275,
                                                       0.1288, 0.1289, 0.1274, 0.1250, 0.1211, 0.1157, 0.1091, 0.1012, 0.0924,
                                                       0.0824, 0.0716]), std=tensor([0.2592, 0.2762, 0.2907, 0.3026, 0.3125, 0.3204, 0.3264, 0.3307, 0.3336,
                                                       0.3350, 0.3351, 0.3334, 0.3308, 0.3263, 0.3198, 0.3117, 0.3016, 0.2896,
                                                       0.2750, 0.2578]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 11
ic| train_loader.dataset.transform.transforms: [RandomErasing(p=0.2, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                                                RandomCrop(size=(20, 20), padding=[2, 2, 2, 2], pad_if_needed=False, fill=0, padding_mode=constant),
                                                Normalize(mean=tensor([0.0724, 0.0832, 0.0932, 0.1020, 0.1097, 0.1162, 0.1213, 0.1250, 0.1275,
                                                       0.1288, 0.1289, 0.1274, 0.1250, 0.1211, 0.1157, 0.1091, 0.1012, 0.0924,
                                                       0.0824, 0.0716]), std=tensor([0.2592, 0.2762, 0.2907, 0.3026, 0.3125, 0.3204, 0.3264, 0.3307, 0.3336,
                                                       0.3350, 0.3351, 0.3334, 0.3308, 0.3263, 0.3198, 0.3117, 0.3016, 0.2896,
                                                       0.2750, 0.2578]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 12
ic| train_loader.dataset.transform.transforms: [RandomCrop(size=(20, 20), padding=[2, 2, 2, 2], pad_if_needed=False, fill=0, padding_mode=constant),
                                                Normalize(mean=tensor([0.0724, 0.0832, 0.0932, 0.1020, 0.1097, 0.1162, 0.1213, 0.1250, 0.1275,
                                                       0.1288, 0.1289, 0.1274, 0.1250, 0.1211, 0.1157, 0.1091, 0.1012, 0.0924,
                                                       0.0824, 0.0716]), std=tensor([0.2592, 0.2762, 0.2907, 0.3026, 0.3125, 0.3204, 0.3264, 0.3307, 0.3336,
                                                       0.3350, 0.3351, 0.3334, 0.3308, 0.3263, 0.3198, 0.3117, 0.3016, 0.2896,
                                                       0.2750, 0.2578]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 13
ic| train_loader.dataset.transform.transforms: [RandomCrop(size=(20, 20), padding=[2, 2, 2, 2], pad_if_needed=False, fill=0, padding_mode=constant),
                                                RandomHorizontalFlip(p=0.5),
                                                RandomVerticalFlip(p=0.5),
                                                Normalize(mean=tensor([0.0724, 0.0832, 0.0932, 0.1020, 0.1097, 0.1162, 0.1213, 0.1250, 0.1275,
                                                       0.1288, 0.1289, 0.1274, 0.1250, 0.1211, 0.1157, 0.1091, 0.1012, 0.0924,
                                                       0.0824, 0.0716]), std=tensor([0.2592, 0.2762, 0.2907, 0.3026, 0.3125, 0.3204, 0.3264, 0.3307, 0.3336,
                                                       0.3350, 0.3351, 0.3334, 0.3308, 0.3263, 0.3198, 0.3117, 0.3016, 0.2896,
                                                       0.2750, 0.2578]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 14
ic| train_loader.dataset.transform.transforms: [RandomErasing(p=0.2, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                                                RandomCrop(size=(20, 20), padding=[2, 2, 2, 2], pad_if_needed=False, fill=0, padding_mode=constant),
                                                RandomHorizontalFlip(p=0.5),
                                                Normalize(mean=tensor([0.0724, 0.0832, 0.0932, 0.1020, 0.1097, 0.1162, 0.1213, 0.1250, 0.1275,
                                                       0.1288, 0.1289, 0.1274, 0.1250, 0.1211, 0.1157, 0.1091, 0.1012, 0.0924,
                                                       0.0824, 0.0716]), std=tensor([0.2592, 0.2762, 0.2907, 0.3026, 0.3125, 0.3204, 0.3264, 0.3307, 0.3336,
                                                       0.3350, 0.3351, 0.3334, 0.3308, 0.3263, 0.3198, 0.3117, 0.3016, 0.2896,
                                                       0.2750, 0.2578]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 15
ic| train_loader.dataset.transform.transforms: [<data_processor.RandomPixelChange object at 0x7f7796896c80>,
                                                ToTensor(),
                                                Normalize(mean=tensor([0.0724, 0.0832, 0.0932, 0.1020, 0.1097, 0.1162, 0.1213, 0.1250, 0.1275,
                                                       0.1288, 0.1289, 0.1274, 0.1250, 0.1211, 0.1157, 0.1091, 0.1012, 0.0924,
                                                       0.0824, 0.0716]), std=tensor([0.2592, 0.2762, 0.2907, 0.3026, 0.3125, 0.3204, 0.3264, 0.3307, 0.3336,
                                                       0.3350, 0.3351, 0.3334, 0.3308, 0.3263, 0.3198, 0.3117, 0.3016, 0.2896,
                                                       0.2750, 0.2578]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 16
ic| train_loader.dataset.transform.transforms: [<data_processor.RandomPixelChange object at 0x7f7796895720>,
                                                ToTensor(),
                                                Normalize(mean=tensor([0.0724, 0.0832, 0.0932, 0.1020, 0.1097, 0.1162, 0.1213, 0.1250, 0.1275,
                                                       0.1288, 0.1289, 0.1274, 0.1250, 0.1211, 0.1157, 0.1091, 0.1012, 0.0924,
                                                       0.0824, 0.0716]), std=tensor([0.2592, 0.2762, 0.2907, 0.3026, 0.3125, 0.3204, 0.3264, 0.3307, 0.3336,
                                                       0.3350, 0.3351, 0.3334, 0.3308, 0.3263, 0.3198, 0.3117, 0.3016, 0.2896,
                                                       0.2750, 0.2578]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 17
ic| train_loader.dataset.transform.transforms: [<data_processor.RandomPixelChange object at 0x7f7796895ab0>,
                                                ToTensor(),
                                                Normalize(mean=tensor([0.0724, 0.0832, 0.0932, 0.1020, 0.1097, 0.1162, 0.1213, 0.1250, 0.1275,
                                                       0.1288, 0.1289, 0.1274, 0.1250, 0.1211, 0.1157, 0.1091, 0.1012, 0.0924,
                                                       0.0824, 0.0716]), std=tensor([0.2592, 0.2762, 0.2907, 0.3026, 0.3125, 0.3204, 0.3264, 0.3307, 0.3336,
                                                       0.3350, 0.3351, 0.3334, 0.3308, 0.3263, 0.3198, 0.3117, 0.3016, 0.2896,
                                                       0.2750, 0.2578]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 18
ic| train_loader.dataset.transform.transforms: [<data_processor.RandomPixelChange object at 0x7f7643fff9d0>,
                                                ToTensor(),
                                                RandomHorizontalFlip(p=0.5),
                                                RandomVerticalFlip(p=0.5),
                                                Normalize(mean=tensor([0.0724, 0.0832, 0.0932, 0.1020, 0.1097, 0.1162, 0.1213, 0.1250, 0.1275,
                                                       0.1288, 0.1289, 0.1274, 0.1250, 0.1211, 0.1157, 0.1091, 0.1012, 0.0924,
                                                       0.0824, 0.0716]), std=tensor([0.2592, 0.2762, 0.2907, 0.3026, 0.3125, 0.3204, 0.3264, 0.3307, 0.3336,
                                                       0.3350, 0.3351, 0.3334, 0.3308, 0.3263, 0.3198, 0.3117, 0.3016, 0.2896,
                                                       0.2750, 0.2578]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 19
ic| train_loader.dataset.transform.transforms: [<data_processor.RandomPixelChange object at 0x7f7643ffe980>,
                                                ToTensor(),
                                                RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                                                Normalize(mean=tensor([0.0724, 0.0832, 0.0932, 0.1020, 0.1097, 0.1162, 0.1213, 0.1250, 0.1275,
                                                       0.1288, 0.1289, 0.1274, 0.1250, 0.1211, 0.1157, 0.1091, 0.1012, 0.0924,
                                                       0.0824, 0.0716]), std=tensor([0.2592, 0.2762, 0.2907, 0.3026, 0.3125, 0.3204, 0.3264, 0.3307, 0.3336,
                                                       0.3350, 0.3351, 0.3334, 0.3308, 0.3263, 0.3198, 0.3117, 0.3016, 0.2896,
                                                       0.2750, 0.2578]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 20
ic| train_loader.dataset.transform.transforms: [<data_processor.RandomPixelChange object at 0x7f764234b6d0>,
                                                ToTensor(),
                                                RandomCrop(size=(20, 20), padding=[2, 2, 2, 2], pad_if_needed=False, fill=0, padding_mode=constant),
                                                Normalize(mean=tensor([0.0724, 0.0832, 0.0932, 0.1020, 0.1097, 0.1162, 0.1213, 0.1250, 0.1275,
                                                       0.1288, 0.1289, 0.1274, 0.1250, 0.1211, 0.1157, 0.1091, 0.1012, 0.0924,
                                                       0.0824, 0.0716]), std=tensor([0.2592, 0.2762, 0.2907, 0.3026, 0.3125, 0.3204, 0.3264, 0.3307, 0.3336,
                                                       0.3350, 0.3351, 0.3334, 0.3308, 0.3263, 0.3198, 0.3117, 0.3016, 0.2896,
                                                       0.2750, 0.2578]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 21
ic| train_loader.dataset.transform.transforms: [<data_processor.RandomPixelChange object at 0x7f764234ada0>,
                                                ToTensor(),
                                                RandomHorizontalFlip(p=0.5),
                                                RandomVerticalFlip(p=0.5),
                                                RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                                                Normalize(mean=tensor([0.0724, 0.0832, 0.0932, 0.1020, 0.1097, 0.1162, 0.1213, 0.1250, 0.1275,
                                                       0.1288, 0.1289, 0.1274, 0.1250, 0.1211, 0.1157, 0.1091, 0.1012, 0.0924,
                                                       0.0824, 0.0716]), std=tensor([0.2592, 0.2762, 0.2907, 0.3026, 0.3125, 0.3204, 0.3264, 0.3307, 0.3336,
                                                       0.3350, 0.3351, 0.3334, 0.3308, 0.3263, 0.3198, 0.3117, 0.3016, 0.2896,
                                                       0.2750, 0.2578]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| 'best aug is 0. Taking next best'
ic| f"best_augmentation: {best_aug}": 'best_augmentation: 12'
ic| f"selected transform {train_transform}": ('selected transform [RandomCrop(size=(20, 20), padding=[2, 2, 2, 2], '
                                              'pad_if_needed=False, fill=0, padding_mode=constant)]')
ic| self.x.shape: torch.Size([50000, 20, 20, 20])
ic| self.x.shape: torch.Size([10000, 20, 20, 20])
ic| self.x.shape: torch.Size([10000, 20, 20, 20])
ic| mode: 'T0'
ic| f"Mode {mode}": 'Mode T0'
ic| get_gpu_memory(0): 41521643520
ic| self.total_generations: 3
ic| 'Time remaining:'
ic| metadata['time_remaining']: 104142.11726927757
ic| self.zcost_nas: True
ic| self.cfg: CfgNode({'MODEL': CfgNode({'TYPE': 'regnet', 'NUM_CLASSES': 7, 'ACTIVATION_FUN': 'relu', 'ACTIVATION_INPLACE': True, 'SCALING_TYPE': '', 'SCALING_FACTOR': 1.0}), 'REGNET': CfgNode({'STEM_TYPE': 'res_stem_cifar', 'INPUT_CHANNELS': 20, 'STEM_W': 20, 'BLOCK_TYPE': 'res_bottleneck_block', 'STRIDE': 2, 'SE_ON': True, 'SE_R': 0.25, 'DEPTH': 20, 'W0': 232, 'WA': 115.89, 'WM': 2.53, 'GROUP_W': 8, 'BOT_MUL': 1.0, 'HEAD_W': 0, 'DOWNSAMPLE': 'avg', 'DROP_RATE': 0.01, 'DROPOUT': 0.2}), 'BN': CfgNode({'EPS': 1e-05, 'MOM': 0.1, 'ZERO_INIT_FINAL_GAMMA': False}), 'LN': CfgNode({'EPS': 1e-05}), 'DESC': ''})
ic| samples: 120
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator SGDRegressor from version 1.4.2 when using version 1.5.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:
https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations
  warnings.warn(
/home/woody/iwb3/iwb3021h/THESIS_RESULTS/package0/search_space/RegNet.py:299: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  ranking_test_df[pred_column]=sgd_regressor.predict(X_test)
ic| ranking_prediction_df:                       score
                           alluring_trogon  180.041096
ic| best_models.keys(): dict_keys(['alluring_trogon'])
ic| self.initial_population_size: 120
ic| metadata: {'benchmark': 71.35,
               'codename': 'Volga',
               'input_shape': [50000, 20, 20, 20],
               'mode': 'NAS',
               'num_classes': 7,
               'test_type': 'T0_Proxy/seed_1',
               'time_remaining': 104125.73527765274,
               'train_config_path': 'configs/train/finetuning_generation_adam.yaml'}
ic| cfg_path: 'configs/train/finetuning_generation_adam.yaml'
ic| self.cfg.SOLVER.LR: 0.001
ic| self.cfg.SOLVER.EPOCHS-self.cfg.SOLVER.SWA_START: 10
ic| self.cfg.SOLVER.EPOCHS: 100
/home/woody/iwb3/iwb3021h/THESIS_RESULTS/package0/trainer.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(checkpoint_path, map_location=device)
ic| self.select_augment: 'Proxy'
ic| self.x.shape: torch.Size([50000, 1, 9, 9])
ic| unique_values: array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ],
                         dtype=float32)
ic| C: 1
ic| H: 9
ic| PH: 1
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.
  warnings.warn(
ic| poss_augs: [[],
                [RandAugment(interpolation=InterpolationMode.NEAREST, num_ops=2, magnitude=9, num_magnitude_bins=31)],
                [RandAugment(interpolation=InterpolationMode.NEAREST, num_ops=2, magnitude=5, num_magnitude_bins=31)],
                [RandAugment(interpolation=InterpolationMode.NEAREST, num_ops=2, magnitude=1, num_magnitude_bins=31)],
                [TrivialAugmentWide(interpolation=InterpolationMode.NEAREST, num_magnitude_bins=31)],
                [TrivialAugmentWide(interpolation=InterpolationMode.NEAREST, num_magnitude_bins=15)],
                [AugMix(interpolation=InterpolationMode.BILINEAR, severity=3, mixture_width=3, chain_depth=-1, alpha=1.0, all_ops=True)],
                [AugMix(interpolation=InterpolationMode.BILINEAR, severity=1, mixture_width=3, chain_depth=-1, alpha=1.0, all_ops=True)],
                [RandomHorizontalFlip(p=0.5), RandomVerticalFlip(p=0.5)],
                [RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                 RandomHorizontalFlip(p=0.5),
                 RandomVerticalFlip(p=0.5)],
                [RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False)],
                [RandomErasing(p=0.2, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                 RandomCrop(size=(9, 9), padding=[1, 1, 1, 1], pad_if_needed=False, fill=0, padding_mode=constant)],
                [RandomCrop(size=(9, 9), padding=[1, 1, 1, 1], pad_if_needed=False, fill=0, padding_mode=constant)],
                [RandomCrop(size=(9, 9), padding=[1, 1, 1, 1], pad_if_needed=False, fill=0, padding_mode=constant),
                 RandomHorizontalFlip(p=0.5),
                 RandomVerticalFlip(p=0.5)],
                [RandomErasing(p=0.2, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                 RandomCrop(size=(9, 9), padding=[1, 1, 1, 1], pad_if_needed=False, fill=0, padding_mode=constant),
                 RandomHorizontalFlip(p=0.5)],
                [<data_processor.RandomPixelChange object at 0x7f7652ef9b10>, ToTensor()],
                [<data_processor.RandomPixelChange object at 0x7f7652efaaa0>, ToTensor()],
                [<data_processor.RandomPixelChange object at 0x7f7652efada0>, ToTensor()],
                [<data_processor.RandomPixelChange object at 0x7f7652efa050>,
                 ToTensor(),
                 RandomHorizontalFlip(p=0.5),
                 RandomVerticalFlip(p=0.5)],
                [<data_processor.RandomPixelChange object at 0x7f7780031ed0>,
                 ToTensor(),
                 RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False)],
                [<data_processor.RandomPixelChange object at 0x7f7780032350>,
                 ToTensor(),
                 RandomCrop(size=(9, 9), padding=[1, 1, 1, 1], pad_if_needed=False, fill=0, padding_mode=constant)],
                [<data_processor.RandomPixelChange object at 0x7f7652ef84c0>,
                 ToTensor(),
                 RandomHorizontalFlip(p=0.5),
                 RandomVerticalFlip(p=0.5),
                 RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False)]]
ic| individuals: ['spiked_caracara',
                  'yellow_bat',
                  'aspiring_rat',
                  'loose_rabbit',
                  'accomplished_stallion',
                  'almond_cheetah',
                  'gainful_cicada',
                  'faithful_oxpecker',
                  'charming_kittiwake',
                  'fine_cicada']
ic| params: [32.0, 96, 2.1999999999999993, 22, 8]
ic| params: [56.0, 96, 2.799999999999997, 22, 8]
ic| params: [48.0, 96, 2.799999999999997, 22, 8]
ic| params: [40.0, 104, 2.3999999999999986, 9, 8]
ic| params: [32.0, 96, 2.3999999999999986, 20, 8]
ic| params: [32.0, 72, 2.3499999999999988, 14, 8]
ic| params: [48.0, 64, 2.799999999999997, 22, 8]
ic| params: [16.0, 64, 2.6999999999999975, 9, 8]
ic| params: [24.0, 72, 2.6499999999999977, 18, 8]
ic| params: [24.0, 80, 2.1999999999999993, 12, 8]
ic| individuals: ['spiked_caracara',
                  'yellow_bat',
                  'aspiring_rat',
                  'loose_rabbit',
                  'accomplished_stallion',
                  'almond_cheetah',
                  'gainful_cicada',
                  'faithful_oxpecker',
                  'charming_kittiwake',
                  'fine_cicada']
ic| params_dict: {'accomplished_stallion': [32.0, 96, 2.3999999999999986, 20, 8],
                  'almond_cheetah': [32.0, 72, 2.3499999999999988, 14, 8],
                  'aspiring_rat': [48.0, 96, 2.799999999999997, 22, 8],
                  'charming_kittiwake': [24.0, 72, 2.6499999999999977, 18, 8],
                  'faithful_oxpecker': [16.0, 64, 2.6999999999999975, 9, 8],
                  'fine_cicada': [24.0, 80, 2.1999999999999993, 12, 8],
                  'gainful_cicada': [48.0, 64, 2.799999999999997, 22, 8],
                  'loose_rabbit': [40.0, 104, 2.3999999999999986, 9, 8],
                  'spiked_caracara': [32.0, 96, 2.1999999999999993, 22, 8],
                  'yellow_bat': [56.0, 96, 2.799999999999997, 22, 8]}
ic| '#############'
ic| aug: 0
ic| train_loader.dataset.transform.transforms: [Normalize(mean=tensor([0.4198]), std=tensor([0.3068]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 1
ic| train_loader.dataset.transform.transforms: [RandAugment(interpolation=InterpolationMode.NEAREST, num_ops=2, magnitude=9, num_magnitude_bins=31),
                                                Normalize(mean=tensor([0.4198]), std=tensor([0.3068]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 2
ic| train_loader.dataset.transform.transforms: [RandAugment(interpolation=InterpolationMode.NEAREST, num_ops=2, magnitude=5, num_magnitude_bins=31),
                                                Normalize(mean=tensor([0.4198]), std=tensor([0.3068]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 3
ic| train_loader.dataset.transform.transforms: [RandAugment(interpolation=InterpolationMode.NEAREST, num_ops=2, magnitude=1, num_magnitude_bins=31),
                                                Normalize(mean=tensor([0.4198]), std=tensor([0.3068]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 4
ic| train_loader.dataset.transform.transforms: [TrivialAugmentWide(interpolation=InterpolationMode.NEAREST, num_magnitude_bins=31),
                                                Normalize(mean=tensor([0.4198]), std=tensor([0.3068]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 5
ic| train_loader.dataset.transform.transforms: [TrivialAugmentWide(interpolation=InterpolationMode.NEAREST, num_magnitude_bins=15),
                                                Normalize(mean=tensor([0.4198]), std=tensor([0.3068]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 6
ic| train_loader.dataset.transform.transforms: [AugMix(interpolation=InterpolationMode.BILINEAR, severity=3, mixture_width=3, chain_depth=-1, alpha=1.0, all_ops=True),
                                                Normalize(mean=tensor([0.4198]), std=tensor([0.3068]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 7
ic| train_loader.dataset.transform.transforms: [AugMix(interpolation=InterpolationMode.BILINEAR, severity=1, mixture_width=3, chain_depth=-1, alpha=1.0, all_ops=True),
                                                Normalize(mean=tensor([0.4198]), std=tensor([0.3068]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 8
ic| train_loader.dataset.transform.transforms: [RandomHorizontalFlip(p=0.5),
                                                RandomVerticalFlip(p=0.5),
                                                Normalize(mean=tensor([0.4198]), std=tensor([0.3068]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 9
ic| train_loader.dataset.transform.transforms: [RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                                                RandomHorizontalFlip(p=0.5),
                                                RandomVerticalFlip(p=0.5),
                                                Normalize(mean=tensor([0.4198]), std=tensor([0.3068]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 10
ic| train_loader.dataset.transform.transforms: [RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                                                Normalize(mean=tensor([0.4198]), std=tensor([0.3068]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 11
ic| train_loader.dataset.transform.transforms: [RandomErasing(p=0.2, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                                                RandomCrop(size=(9, 9), padding=[1, 1, 1, 1], pad_if_needed=False, fill=0, padding_mode=constant),
                                                Normalize(mean=tensor([0.4198]), std=tensor([0.3068]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 12
ic| train_loader.dataset.transform.transforms: [RandomCrop(size=(9, 9), padding=[1, 1, 1, 1], pad_if_needed=False, fill=0, padding_mode=constant),
                                                Normalize(mean=tensor([0.4198]), std=tensor([0.3068]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 13
ic| train_loader.dataset.transform.transforms: [RandomCrop(size=(9, 9), padding=[1, 1, 1, 1], pad_if_needed=False, fill=0, padding_mode=constant),
                                                RandomHorizontalFlip(p=0.5),
                                                RandomVerticalFlip(p=0.5),
                                                Normalize(mean=tensor([0.4198]), std=tensor([0.3068]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 14
ic| train_loader.dataset.transform.transforms: [RandomErasing(p=0.2, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                                                RandomCrop(size=(9, 9), padding=[1, 1, 1, 1], pad_if_needed=False, fill=0, padding_mode=constant),
                                                RandomHorizontalFlip(p=0.5),
                                                Normalize(mean=tensor([0.4198]), std=tensor([0.3068]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 15
ic| train_loader.dataset.transform.transforms: [<data_processor.RandomPixelChange object at 0x7f7652ef9b10>,
                                                ToTensor(),
                                                Normalize(mean=tensor([0.4198]), std=tensor([0.3068]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 16
ic| train_loader.dataset.transform.transforms: [<data_processor.RandomPixelChange object at 0x7f7652efaaa0>,
                                                ToTensor(),
                                                Normalize(mean=tensor([0.4198]), std=tensor([0.3068]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 17
ic| train_loader.dataset.transform.transforms: [<data_processor.RandomPixelChange object at 0x7f7652efada0>,
                                                ToTensor(),
                                                Normalize(mean=tensor([0.4198]), std=tensor([0.3068]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 18
ic| train_loader.dataset.transform.transforms: [<data_processor.RandomPixelChange object at 0x7f7652efa050>,
                                                ToTensor(),
                                                RandomHorizontalFlip(p=0.5),
                                                RandomVerticalFlip(p=0.5),
                                                Normalize(mean=tensor([0.4198]), std=tensor([0.3068]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 19
ic| train_loader.dataset.transform.transforms: [<data_processor.RandomPixelChange object at 0x7f7780031ed0>,
                                                ToTensor(),
                                                RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                                                Normalize(mean=tensor([0.4198]), std=tensor([0.3068]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 20
ic| train_loader.dataset.transform.transforms: [<data_processor.RandomPixelChange object at 0x7f7780032350>,
                                                ToTensor(),
                                                RandomCrop(size=(9, 9), padding=[1, 1, 1, 1], pad_if_needed=False, fill=0, padding_mode=constant),
                                                Normalize(mean=tensor([0.4198]), std=tensor([0.3068]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 21
ic| train_loader.dataset.transform.transforms: [<data_processor.RandomPixelChange object at 0x7f7652ef84c0>,
                                                ToTensor(),
                                                RandomHorizontalFlip(p=0.5),
                                                RandomVerticalFlip(p=0.5),
                                                RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                                                Normalize(mean=tensor([0.4198]), std=tensor([0.3068]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/pandas/core/dtypes/astype.py:133: ComplexWarning: Casting complex values to real discards the imaginary part
  return arr.astype(dtype, copy=True)
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/pandas/core/dtypes/astype.py:133: ComplexWarning: Casting complex values to real discards the imaginary part
  return arr.astype(dtype, copy=True)
ic| f"best_augmentation: {best_aug}": 'best_augmentation: 1'
ic| f"selected transform {train_transform}": ('selected transform [RandAugment(interpolation=InterpolationMode.NEAREST, '
                                              'num_ops=2, magnitude=9, num_magnitude_bins=31)]')
ic| self.x.shape: torch.Size([50000, 1, 9, 9])
ic| self.x.shape: torch.Size([10000, 1, 9, 9])
ic| self.x.shape: torch.Size([10000, 1, 9, 9])
ic| mode: 'T0'
ic| f"Mode {mode}": 'Mode T0'
ic| get_gpu_memory(0): 41739747328
ic| self.total_generations: 3
ic| 'Time remaining:'
ic| metadata['time_remaining']: 102070.97502112389
ic| self.zcost_nas: True
ic| self.cfg: CfgNode({'MODEL': CfgNode({'TYPE': 'regnet', 'NUM_CLASSES': 9, 'ACTIVATION_FUN': 'relu', 'ACTIVATION_INPLACE': True, 'SCALING_TYPE': '', 'SCALING_FACTOR': 1.0}), 'REGNET': CfgNode({'STEM_TYPE': 'res_stem_cifar', 'INPUT_CHANNELS': 1, 'STEM_W': 9, 'BLOCK_TYPE': 'res_bottleneck_block', 'STRIDE': 2, 'SE_ON': True, 'SE_R': 0.25, 'DEPTH': 20, 'W0': 232, 'WA': 115.89, 'WM': 2.53, 'GROUP_W': 8, 'BOT_MUL': 1.0, 'HEAD_W': 0, 'DOWNSAMPLE': 'avg', 'DROP_RATE': 0.01, 'DROPOUT': 0.2}), 'BN': CfgNode({'EPS': 1e-05, 'MOM': 0.1, 'ZERO_INIT_FINAL_GAMMA': False}), 'LN': CfgNode({'EPS': 1e-05}), 'DESC': ''})
ic| samples: 120
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator SGDRegressor from version 1.4.2 when using version 1.5.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:
https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations
  warnings.warn(
/home/woody/iwb3/iwb3021h/THESIS_RESULTS/package0/search_space/RegNet.py:299: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  ranking_test_df[pred_column]=sgd_regressor.predict(X_test)
ic| ranking_prediction_df:                         score
                           delectable_dragon  167.036822
ic| best_models.keys(): dict_keys(['delectable_dragon'])
ic| self.initial_population_size: 120
ic| metadata: {'benchmark': 0.0,
               'codename': 'Sokoto',
               'input_shape': [50000, 1, 9, 9],
               'mode': 'NAS',
               'num_classes': 9,
               'test_type': 'T0_Proxy/seed_1',
               'time_remaining': 102055.7351474762,
               'train_config_path': 'configs/train/finetuning_generation_adam.yaml'}
ic| cfg_path: 'configs/train/finetuning_generation_adam.yaml'
ic| self.cfg.SOLVER.LR: 0.001
ic| self.cfg.SOLVER.EPOCHS-self.cfg.SOLVER.SWA_START: 10
ic| self.cfg.SOLVER.EPOCHS: 100
/home/woody/iwb3/iwb3021h/THESIS_RESULTS/package0/trainer.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(checkpoint_path, map_location=device)
ic| self.select_augment: 'Proxy'
ic| self.x.shape: torch.Size([40000, 3, 32, 32])
ic| unique_values: array([0.09411765, 0.09803922, 0.10196079, 0.10588235, 0.10980392,
                          0.11372549, 0.11764706, 0.12156863, 0.1254902 , 0.12941177,
                          0.13333334, 0.13725491, 0.14117648, 0.14509805, 0.14901961,
                          0.15294118, 0.15686275, 0.16078432, 0.16470589, 0.16862746,
                          0.17254902, 0.1764706 , 0.18039216, 0.18431373, 0.1882353 ,
                          0.19215687, 0.19607843, 0.2       , 0.20392157, 0.20784314,
                          0.21176471, 0.21568628, 0.21960784, 0.22352941, 0.22745098,
                          0.23137255, 0.23529412, 0.23921569, 0.24313726, 0.24705882,
                          0.2509804 , 0.25490198, 0.25882354, 0.2627451 , 0.26666668,
                          0.27058825, 0.27450982, 0.2784314 , 0.28235295, 0.28627452,
                          0.2901961 , 0.29411766, 0.29803923, 0.3019608 , 0.30588236,
                          0.30980393, 0.3137255 , 0.31764707, 0.32156864, 0.3254902 ,
                          0.32941177, 0.33333334, 0.3372549 , 0.34117648, 0.34509805,
                          0.34901962, 0.3529412 , 0.35686275, 0.36078432, 0.3647059 ,
                          0.36862746, 0.37254903, 0.3764706 , 0.38039216, 0.38431373,
                          0.3882353 , 0.39215687, 0.39607844, 0.4       , 0.40392157,
                          0.40784314, 0.4117647 , 0.41568628, 0.41960785, 0.42352942,
                          0.42745098, 0.43137255, 0.43529412, 0.4392157 , 0.44313726,
                          0.44705883, 0.4509804 , 0.45490196, 0.45882353, 0.4627451 ,
                          0.46666667, 0.47058824, 0.4745098 , 0.47843137, 0.48235294,
                          0.4862745 , 0.49019608, 0.49411765, 0.49803922, 0.5019608 ,
                          0.5058824 , 0.50980395, 0.5137255 , 0.5176471 , 0.52156866,
                          0.5254902 , 0.5294118 , 0.53333336, 0.5372549 , 0.5411765 ,
                          0.54509807, 0.54901963, 0.5529412 , 0.5568628 , 0.56078434,
                          0.5647059 , 0.5686275 , 0.57254905, 0.5764706 , 0.5803922 ,
                          0.58431375, 0.5882353 , 0.5921569 , 0.59607846, 0.6       ,
                          0.6039216 , 0.60784316, 0.6117647 , 0.6156863 , 0.61960787,
                          0.62352943, 0.627451  , 0.6313726 , 0.63529414, 0.6392157 ,
                          0.6431373 , 0.64705884, 0.6509804 , 0.654902  , 0.65882355,
                          0.6627451 , 0.6666667 , 0.67058825, 0.6745098 , 0.6784314 ,
                          0.68235296, 0.6862745 , 0.6901961 , 0.69411767, 0.69803923,
                          0.7019608 , 0.7058824 , 0.70980394, 0.7137255 , 0.7176471 ,
                          0.72156864, 0.7254902 , 0.7294118 , 0.73333335, 0.7411765 ,
                          0.74509805, 0.7490196 , 0.7529412 , 0.75686276, 0.7647059 ,
                          0.76862746, 0.77254903, 0.7764706 , 0.78431374, 0.7882353 ,
                          0.7921569 , 0.79607844, 0.8       , 0.8039216 , 0.8156863 ,
                          0.81960785, 0.8235294 , 0.827451  , 0.83137256, 0.8392157 ,
                          0.8509804 , 0.85882354, 0.8627451 , 0.8666667 , 0.87058824,
                          0.8784314 , 0.88235295, 0.8901961 , 0.89411765, 0.90588236,
                          0.9098039 , 0.9137255 , 0.92156863, 0.92941177, 0.93333334,
                          0.9411765 , 0.9490196 , 0.9529412 , 0.972549  ], dtype=float32)
ic| C: 3
ic| H: 32
ic| PH: 4
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.
  warnings.warn(
ic| poss_augs: [[],
                [RandAugment(interpolation=InterpolationMode.NEAREST, num_ops=2, magnitude=9, num_magnitude_bins=31)],
                [RandAugment(interpolation=InterpolationMode.NEAREST, num_ops=2, magnitude=5, num_magnitude_bins=31)],
                [RandAugment(interpolation=InterpolationMode.NEAREST, num_ops=2, magnitude=1, num_magnitude_bins=31)],
                [TrivialAugmentWide(interpolation=InterpolationMode.NEAREST, num_magnitude_bins=31)],
                [TrivialAugmentWide(interpolation=InterpolationMode.NEAREST, num_magnitude_bins=15)],
                [AugMix(interpolation=InterpolationMode.BILINEAR, severity=3, mixture_width=3, chain_depth=-1, alpha=1.0, all_ops=True)],
                [AugMix(interpolation=InterpolationMode.BILINEAR, severity=1, mixture_width=3, chain_depth=-1, alpha=1.0, all_ops=True)],
                [RandomHorizontalFlip(p=0.5), RandomVerticalFlip(p=0.5)],
                [RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                 RandomHorizontalFlip(p=0.5),
                 RandomVerticalFlip(p=0.5)],
                [RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False)],
                [RandomErasing(p=0.2, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                 RandomCrop(size=(32, 32), padding=[4, 4, 4, 4], pad_if_needed=False, fill=0, padding_mode=constant)],
                [RandomCrop(size=(32, 32), padding=[4, 4, 4, 4], pad_if_needed=False, fill=0, padding_mode=constant)],
                [RandomCrop(size=(32, 32), padding=[4, 4, 4, 4], pad_if_needed=False, fill=0, padding_mode=constant),
                 RandomHorizontalFlip(p=0.5),
                 RandomVerticalFlip(p=0.5)],
                [RandomErasing(p=0.2, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                 RandomCrop(size=(32, 32), padding=[4, 4, 4, 4], pad_if_needed=False, fill=0, padding_mode=constant),
                 RandomHorizontalFlip(p=0.5)],
                [<data_processor.RandomPixelChange object at 0x7f7779488460>, ToTensor()],
                [<data_processor.RandomPixelChange object at 0x7f77794882e0>, ToTensor()],
                [<data_processor.RandomPixelChange object at 0x7f7779488910>, ToTensor()],
                [<data_processor.RandomPixelChange object at 0x7f777948bf10>,
                 ToTensor(),
                 RandomHorizontalFlip(p=0.5),
                 RandomVerticalFlip(p=0.5)],
                [<data_processor.RandomPixelChange object at 0x7f7789862620>,
                 ToTensor(),
                 RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False)],
                [<data_processor.RandomPixelChange object at 0x7f7789862b00>,
                 ToTensor(),
                 RandomCrop(size=(32, 32), padding=[4, 4, 4, 4], pad_if_needed=False, fill=0, padding_mode=constant)],
                [<data_processor.RandomPixelChange object at 0x7f777948b160>,
                 ToTensor(),
                 RandomHorizontalFlip(p=0.5),
                 RandomVerticalFlip(p=0.5),
                 RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False)]]
ic| individuals: ['cuddly_kagu',
                  'rich_earwig',
                  'psychedelic_malamute',
                  'spiffy_oryx',
                  'ultramarine_koala',
                  'spectacular_cassowary',
                  'exuberant_cuttlefish',
                  'tidy_capybara',
                  'idealistic_dolphin',
                  'opal_mouse']
ic| params: [64.0, 88, 2.4499999999999984, 11, 8]
ic| params: [24.0, 48, 2.799999999999997, 12, 8]
ic| params: [24.0, 80, 2.599999999999998, 22, 8]
ic| params: [64.0, 120, 2.549999999999998, 9, 8]
ic| params: [16.0, 16, 2.3499999999999988, 18, 8]
ic| params: [48.0, 120, 2.599999999999998, 21, 8]
ic| params: [56.0, 88, 2.299999999999999, 14, 8]
ic| params: [16.0, 40, 2.4999999999999982, 13, 8]
ic| params: [32.0, 112, 2.0999999999999996, 11, 8]
ic| params: [16.0, 40, 2.1999999999999993, 10, 8]
ic| individuals: ['cuddly_kagu',
                  'rich_earwig',
                  'psychedelic_malamute',
                  'spiffy_oryx',
                  'ultramarine_koala',
                  'spectacular_cassowary',
                  'exuberant_cuttlefish',
                  'tidy_capybara',
                  'idealistic_dolphin',
                  'opal_mouse']
ic| params_dict: {'cuddly_kagu': [64.0, 88, 2.4499999999999984, 11, 8],
                  'exuberant_cuttlefish': [56.0, 88, 2.299999999999999, 14, 8],
                  'idealistic_dolphin': [32.0, 112, 2.0999999999999996, 11, 8],
                  'opal_mouse': [16.0, 40, 2.1999999999999993, 10, 8],
                  'psychedelic_malamute': [24.0, 80, 2.599999999999998, 22, 8],
                  'rich_earwig': [24.0, 48, 2.799999999999997, 12, 8],
                  'spectacular_cassowary': [48.0, 120, 2.599999999999998, 21, 8],
                  'spiffy_oryx': [64.0, 120, 2.549999999999998, 9, 8],
                  'tidy_capybara': [16.0, 40, 2.4999999999999982, 13, 8],
                  'ultramarine_koala': [16.0, 16, 2.3499999999999988, 18, 8]}
ic| '#############'
ic| aug: 0
ic| train_loader.dataset.transform.transforms: [Normalize(mean=tensor([0.4914, 0.4823, 0.4466]), std=tensor([0.2468, 0.2431, 0.2613]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 1
ic| train_loader.dataset.transform.transforms: [RandAugment(interpolation=InterpolationMode.NEAREST, num_ops=2, magnitude=9, num_magnitude_bins=31),
                                                Normalize(mean=tensor([0.4914, 0.4823, 0.4466]), std=tensor([0.2468, 0.2431, 0.2613]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 2
ic| train_loader.dataset.transform.transforms: [RandAugment(interpolation=InterpolationMode.NEAREST, num_ops=2, magnitude=5, num_magnitude_bins=31),
                                                Normalize(mean=tensor([0.4914, 0.4823, 0.4466]), std=tensor([0.2468, 0.2431, 0.2613]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 3
ic| train_loader.dataset.transform.transforms: [RandAugment(interpolation=InterpolationMode.NEAREST, num_ops=2, magnitude=1, num_magnitude_bins=31),
                                                Normalize(mean=tensor([0.4914, 0.4823, 0.4466]), std=tensor([0.2468, 0.2431, 0.2613]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 4
ic| train_loader.dataset.transform.transforms: [TrivialAugmentWide(interpolation=InterpolationMode.NEAREST, num_magnitude_bins=31),
                                                Normalize(mean=tensor([0.4914, 0.4823, 0.4466]), std=tensor([0.2468, 0.2431, 0.2613]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 5
ic| train_loader.dataset.transform.transforms: [TrivialAugmentWide(interpolation=InterpolationMode.NEAREST, num_magnitude_bins=15),
                                                Normalize(mean=tensor([0.4914, 0.4823, 0.4466]), std=tensor([0.2468, 0.2431, 0.2613]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 6
ic| train_loader.dataset.transform.transforms: [AugMix(interpolation=InterpolationMode.BILINEAR, severity=3, mixture_width=3, chain_depth=-1, alpha=1.0, all_ops=True),
                                                Normalize(mean=tensor([0.4914, 0.4823, 0.4466]), std=tensor([0.2468, 0.2431, 0.2613]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 7
ic| train_loader.dataset.transform.transforms: [AugMix(interpolation=InterpolationMode.BILINEAR, severity=1, mixture_width=3, chain_depth=-1, alpha=1.0, all_ops=True),
                                                Normalize(mean=tensor([0.4914, 0.4823, 0.4466]), std=tensor([0.2468, 0.2431, 0.2613]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 8
ic| train_loader.dataset.transform.transforms: [RandomHorizontalFlip(p=0.5),
                                                RandomVerticalFlip(p=0.5),
                                                Normalize(mean=tensor([0.4914, 0.4823, 0.4466]), std=tensor([0.2468, 0.2431, 0.2613]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 9
ic| train_loader.dataset.transform.transforms: [RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                                                RandomHorizontalFlip(p=0.5),
                                                RandomVerticalFlip(p=0.5),
                                                Normalize(mean=tensor([0.4914, 0.4823, 0.4466]), std=tensor([0.2468, 0.2431, 0.2613]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 10
ic| train_loader.dataset.transform.transforms: [RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                                                Normalize(mean=tensor([0.4914, 0.4823, 0.4466]), std=tensor([0.2468, 0.2431, 0.2613]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 11
ic| train_loader.dataset.transform.transforms: [RandomErasing(p=0.2, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                                                RandomCrop(size=(32, 32), padding=[4, 4, 4, 4], pad_if_needed=False, fill=0, padding_mode=constant),
                                                Normalize(mean=tensor([0.4914, 0.4823, 0.4466]), std=tensor([0.2468, 0.2431, 0.2613]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 12
ic| train_loader.dataset.transform.transforms: [RandomCrop(size=(32, 32), padding=[4, 4, 4, 4], pad_if_needed=False, fill=0, padding_mode=constant),
                                                Normalize(mean=tensor([0.4914, 0.4823, 0.4466]), std=tensor([0.2468, 0.2431, 0.2613]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 13
ic| train_loader.dataset.transform.transforms: [RandomCrop(size=(32, 32), padding=[4, 4, 4, 4], pad_if_needed=False, fill=0, padding_mode=constant),
                                                RandomHorizontalFlip(p=0.5),
                                                RandomVerticalFlip(p=0.5),
                                                Normalize(mean=tensor([0.4914, 0.4823, 0.4466]), std=tensor([0.2468, 0.2431, 0.2613]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 14
ic| train_loader.dataset.transform.transforms: [RandomErasing(p=0.2, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                                                RandomCrop(size=(32, 32), padding=[4, 4, 4, 4], pad_if_needed=False, fill=0, padding_mode=constant),
                                                RandomHorizontalFlip(p=0.5),
                                                Normalize(mean=tensor([0.4914, 0.4823, 0.4466]), std=tensor([0.2468, 0.2431, 0.2613]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 15
ic| train_loader.dataset.transform.transforms: [<data_processor.RandomPixelChange object at 0x7f7779488460>,
                                                ToTensor(),
                                                Normalize(mean=tensor([0.4914, 0.4823, 0.4466]), std=tensor([0.2468, 0.2431, 0.2613]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 16
ic| train_loader.dataset.transform.transforms: [<data_processor.RandomPixelChange object at 0x7f77794882e0>,
                                                ToTensor(),
                                                Normalize(mean=tensor([0.4914, 0.4823, 0.4466]), std=tensor([0.2468, 0.2431, 0.2613]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 17
ic| train_loader.dataset.transform.transforms: [<data_processor.RandomPixelChange object at 0x7f7779488910>,
                                                ToTensor(),
                                                Normalize(mean=tensor([0.4914, 0.4823, 0.4466]), std=tensor([0.2468, 0.2431, 0.2613]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 18
ic| train_loader.dataset.transform.transforms: [<data_processor.RandomPixelChange object at 0x7f777948bf10>,
                                                ToTensor(),
                                                RandomHorizontalFlip(p=0.5),
                                                RandomVerticalFlip(p=0.5),
                                                Normalize(mean=tensor([0.4914, 0.4823, 0.4466]), std=tensor([0.2468, 0.2431, 0.2613]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 19
ic| train_loader.dataset.transform.transforms: [<data_processor.RandomPixelChange object at 0x7f7789862620>,
                                                ToTensor(),
                                                RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                                                Normalize(mean=tensor([0.4914, 0.4823, 0.4466]), std=tensor([0.2468, 0.2431, 0.2613]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 20
ic| train_loader.dataset.transform.transforms: [<data_processor.RandomPixelChange object at 0x7f7789862b00>,
                                                ToTensor(),
                                                RandomCrop(size=(32, 32), padding=[4, 4, 4, 4], pad_if_needed=False, fill=0, padding_mode=constant),
                                                Normalize(mean=tensor([0.4914, 0.4823, 0.4466]), std=tensor([0.2468, 0.2431, 0.2613]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
ic| '#############'
ic| aug: 21
ic| train_loader.dataset.transform.transforms: [<data_processor.RandomPixelChange object at 0x7f777948b160>,
                                                ToTensor(),
                                                RandomHorizontalFlip(p=0.5),
                                                RandomVerticalFlip(p=0.5),
                                                RandomErasing(p=0.2, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=[0.0], inplace=False),
                                                Normalize(mean=tensor([0.4914, 0.4823, 0.4466]), std=tensor([0.2468, 0.2431, 0.2613]))]
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
/home/woody/iwb3/iwb3021h/THESIS_RESULTS/package0/data_processor.py:608: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  subset_df_no_outliers["jacob_cov"] = scaler.fit_transform(subset_df_no_outliers[["jacob_cov"]])
ic| f"best_augmentation: {best_aug}": 'best_augmentation: 13'
ic| f"selected transform {train_transform}": ('selected transform [RandomCrop(size=(32, 32), padding=[4, 4, 4, 4], '
                                              'pad_if_needed=False, fill=0, padding_mode=constant), '
                                              'RandomHorizontalFlip(p=0.5), RandomVerticalFlip(p=0.5)]')
ic| self.x.shape: torch.Size([40000, 3, 32, 32])
ic| self.x.shape: torch.Size([10000, 3, 32, 32])
ic| self.x.shape: torch.Size([10000, 3, 32, 32])
ic| mode: 'T0'
ic| f"Mode {mode}": 'Mode T0'
ic| get_gpu_memory(0): 41720872960
ic| self.total_generations: 3
ic| 'Time remaining:'
ic| metadata['time_remaining']: 99401.8970541954
ic| self.zcost_nas: True
ic| self.cfg: CfgNode({'MODEL': CfgNode({'TYPE': 'regnet', 'NUM_CLASSES': 10, 'ACTIVATION_FUN': 'relu', 'ACTIVATION_INPLACE': True, 'SCALING_TYPE': '', 'SCALING_FACTOR': 1.0}), 'REGNET': CfgNode({'STEM_TYPE': 'res_stem_cifar', 'INPUT_CHANNELS': 3, 'STEM_W': 32, 'BLOCK_TYPE': 'res_bottleneck_block', 'STRIDE': 2, 'SE_ON': True, 'SE_R': 0.25, 'DEPTH': 20, 'W0': 232, 'WA': 115.89, 'WM': 2.53, 'GROUP_W': 8, 'BOT_MUL': 1.0, 'HEAD_W': 0, 'DOWNSAMPLE': 'avg', 'DROP_RATE': 0.01, 'DROPOUT': 0.2}), 'BN': CfgNode({'EPS': 1e-05, 'MOM': 0.1, 'ZERO_INIT_FINAL_GAMMA': False}), 'LN': CfgNode({'EPS': 1e-05}), 'DESC': ''})
ic| samples: 120
/home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/.testvenv/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator SGDRegressor from version 1.4.2 when using version 1.5.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:
https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations
  warnings.warn(
/home/woody/iwb3/iwb3021h/THESIS_RESULTS/package0/search_space/RegNet.py:299: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  ranking_test_df[pred_column]=sgd_regressor.predict(X_test)
ic| ranking_prediction_df:                   score
                           quiet_cobra  176.491622
ic| best_models.keys(): dict_keys(['quiet_cobra'])
ic| self.initial_population_size: 120
ic| metadata: {'benchmark': 90.65,
               'codename': 'CIFAR10',
               'input_shape': [50000, 3, 32, 32],
               'mode': 'NAS',
               'num_classes': 10,
               'test_type': 'T0_Proxy/seed_1',
               'time_remaining': 99385.40171933174,
               'train_config_path': 'configs/train/finetuning_generation_adam.yaml'}
ic| cfg_path: 'configs/train/finetuning_generation_adam.yaml'
ic| self.cfg.SOLVER.LR: 0.001
ic| self.cfg.SOLVER.EPOCHS-self.cfg.SOLVER.SWA_START: 10
ic| self.cfg.SOLVER.EPOCHS: 100
/home/woody/iwb3/iwb3021h/THESIS_RESULTS/package0/trainer.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(checkpoint_path, map_location=device)
Mode: T0
===========================================================================
=============    Your Unseen Data 2024 Submission is running     =============
===========================================================================
========== Dataset    in16    =============================================
  Metadata:
   - input_shape         : [148700, 3, 16, 16]
   - codename            : in16
   - benchmark           : 46.38
   - num_classes         : 120
   - time_remaining      : 107999.4884622097

=== Processing Data ===
  Allotted compute time remaining: ~29h,59m,59s

=== Performing NAS ===
  Allotted compute time remaining: ~29h,54m,0s
None

=== Training ===
  Allotted compute time remaining: ~29h,53m,43s
Early stopping at epoch 44
[31m[EVAL] Best accuracy:34.733333587646484[0m

=== Predicting ===
  Allotted compute time remaining: ~28h,59m,15s

========== Dataset   Volga    =============================================
  Metadata:
   - input_shape         : [50000, 20, 20, 20]
   - codename            : Volga
   - benchmark           : 71.35
   - num_classes         : 7
   - time_remaining      : 104350.2504966259

=== Processing Data ===
  Allotted compute time remaining: ~28h,59m,10s

=== Performing NAS ===
  Allotted compute time remaining: ~28h,55m,42s
spawn

=== Training ===
  Allotted compute time remaining: ~28h,55m,25s
Early stopping at epoch 92
[31m[EVAL] Best accuracy:82.3699951171875[0m

=== Predicting ===
  Allotted compute time remaining: ~28h,27m,0s

========== Dataset   Sokoto   =============================================
  Metadata:
   - input_shape         : [50000, 1, 9, 9]
   - codename            : Sokoto
   - benchmark           : 0.0
   - num_classes         : 9
   - time_remaining      : 102419.27170801163

=== Processing Data ===
  Allotted compute time remaining: ~28h,26m,59s

=== Performing NAS ===
  Allotted compute time remaining: ~28h,21m,10s
spawn

=== Training ===
  Allotted compute time remaining: ~28h,20m,55s
Early stopping at epoch 92
[31m[EVAL] Best accuracy:50.28999710083008[0m

=== Predicting ===
  Allotted compute time remaining: ~27h,41m,28s

========== Dataset  CIFAR10   =============================================
  Metadata:
   - input_shape         : [50000, 3, 32, 32]
   - codename            : CIFAR10
   - benchmark           : 90.65
   - num_classes         : 10
   - time_remaining      : 99685.67261862755

=== Processing Data ===
  Allotted compute time remaining: ~27h,41m,25s

=== Performing NAS ===
  Allotted compute time remaining: ~27h,36m,41s
spawn

=== Training ===
  Allotted compute time remaining: ~27h,36m,25s
Early stopping at epoch 97
[31m[EVAL] Best accuracy:90.5999984741211[0m

=== Predicting ===
  Allotted compute time remaining: ~27h,7m,42s

rm -Rf /home/woody/iwb3/iwb3021h/THESIS_RESULTS/scoring0
mkdir /home/woody/iwb3/iwb3021h/THESIS_RESULTS/scoring0
mkdir /home/woody/iwb3/iwb3021h/THESIS_RESULTS/scoring0/labels
mkdir /home/woody/iwb3/iwb3021h/THESIS_RESULTS/scoring0/predictions
rsync -avr --exclude='**/*x.npy' --exclude='**/train*.npy' --exclude='**/valid*.npy'   --include='**/test_y.npy' datasets/* /home/woody/iwb3/iwb3021h/THESIS_RESULTS/scoring0/labels/
sending incremental file list
CIFAR10/
CIFAR10/cifar-10-python.tar.gz
CIFAR10/metadata
CIFAR10/test_y.npy
ImageNet16-120/
ImageNet16-120/metadata
ImageNet16-120/test_y.npy
Sudoku/
Sudoku/Example Image with Corresponding Sudoku Grid.png
Sudoku/README
Sudoku/metadata
Sudoku/test_y.npy
Sudoku/Sokoto/
Sudoku/Sokoto/augmentation_results.json
Sudoku/Sokoto/aug_0/
Sudoku/Sokoto/aug_0/student_best
Sudoku/Sokoto/aug_0/worklog.txt
Sudoku/Sokoto/aug_0/.ipynb_checkpoints/
Sudoku/Sokoto/aug_0/.ipynb_checkpoints/worklog-checkpoint.txt
Sudoku/Sokoto/aug_1/
Sudoku/Sokoto/aug_1/student_best
Sudoku/Sokoto/aug_1/worklog.txt
Sudoku/Sokoto/aug_1/.ipynb_checkpoints/
Sudoku/Sokoto/aug_1/.ipynb_checkpoints/worklog-checkpoint.txt
Sudoku/Sokoto/aug_2/
Sudoku/Sokoto/aug_2/student_best
Sudoku/Sokoto/aug_2/worklog.txt
Sudoku/Sokoto/aug_2/.ipynb_checkpoints/
Sudoku/Sokoto/aug_2/.ipynb_checkpoints/worklog-checkpoint.txt
Sudoku/Sokoto/aug_3/
Sudoku/Sokoto/aug_3/student_best
Sudoku/Sokoto/aug_3/worklog.txt
Sudoku/Sokoto/aug_3/.ipynb_checkpoints/
Sudoku/Sokoto/aug_3/.ipynb_checkpoints/worklog-checkpoint.txt
Sudoku/Sokoto/aug_4/
Sudoku/Sokoto/aug_4/student_best
Sudoku/Sokoto/aug_4/worklog.txt
Sudoku/Sokoto/aug_5/
Sudoku/Sokoto/aug_5/student_best
Sudoku/Sokoto/aug_5/worklog.txt
Sudoku/Sokoto/aug_5/.ipynb_checkpoints/
Sudoku/Sokoto/aug_5/.ipynb_checkpoints/worklog-checkpoint.txt
Sudoku/Sokoto/aug_6/
Sudoku/Sokoto/aug_6/student_best
Sudoku/Sokoto/aug_6/worklog.txt
Sudoku/Sokoto/aug_6/.ipynb_checkpoints/
Sudoku/Sokoto/aug_6/.ipynb_checkpoints/worklog-checkpoint.txt
Sudoku/Sokoto/aug_7/
Sudoku/Sokoto/aug_7/student_best
Sudoku/Sokoto/aug_7/worklog.txt
Sudoku/Sokoto/aug_8/
Sudoku/Sokoto/aug_8/student_best
Sudoku/Sokoto/aug_8/worklog.txt
Sudoku/Sokoto/aug_8/.ipynb_checkpoints/
Sudoku/Sokoto/aug_8/.ipynb_checkpoints/worklog-checkpoint.txt
Sudoku/Sokoto/aug_9/
Sudoku/Sokoto/aug_9/student_best
Sudoku/Sokoto/aug_9/worklog.txt
Sudoku/Sokoto/aug_9/.ipynb_checkpoints/
Sudoku/Sokoto/aug_9/.ipynb_checkpoints/worklog-checkpoint.txt
Voxel/
Voxel/Rendered Examples.png
Voxel/metadata
Voxel/test_y.npy

sent 620,487,017 bytes  received 951 bytes  137,886,215.11 bytes/sec
total size is 620,331,646  speedup is 1.00
cp -R /home/woody/iwb3/iwb3021h/THESIS_RESULTS/package0/predictions /home/woody/iwb3/iwb3021h/THESIS_RESULTS/scoring0
cp evaluation/score.py /home/woody/iwb3/iwb3021h/THESIS_RESULTS/scoring0/score.py
cd /home/woody/iwb3/iwb3021h/THESIS_RESULTS/scoring0; python3 score.py
===========================================================================
=============    Your Unseen Data 2024 Submission is scoring     =============
===========================================================================
== Scoring Voxel ==
Raw Score:    82.540
Adj Score:    3.906
Model Params: 2,175,784
Runtime:      1,930.8s
== Scoring ImageNet16-120 ==
Raw Score:    35.533
Adj Score:    -2.023
Model Params: 3,299,190
Runtime:      3,644.3s
== Scoring Sudoku ==
Raw Score:    51.300
Adj Score:    5.130
Model Params: 2,309,028
Runtime:      2,731.7s
== Scoring CIFAR10 ==
Raw Score:    90.430
Adj Score:    -0.235
Model Params: 2,640,270
Runtime:      2,024.1s
===========================
Final Score: 6.778
=== JOB_STATISTICS ===
=== current date     : Mon 21 Oct 2024 05:08:39 PM CEST
= Job-ID             : 915936 on tinygpu
= Job-Name           : /home/woody/iwb3/iwb3021h/THESIS_RESULTS/hpcruns/evonas
= Job-Command        : /home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024/evonas_job_full_main_0.sh
= Initial workdir    : /home/hpc/iwb3/iwb3021h/NAS_CHALLENGE/NAS_Challenge_AutoML_2024
= Queue/Partition    : a100
= Slurm account      : iwb3 with QOS=normal
= Requested resources:  for 23:59:00
= Elapsed runtime    : 02:54:40
= Total RAM usage    : 5.9 GiB of requested  GiB (%)   
= Node list          : tg096
= Subm/Elig/Start/End: 2024-10-21T14:06:27 / 2024-10-21T14:06:27 / 2024-10-21T14:13:59 / 2024-10-21T17:08:39
======================
=== Quota infos ======
    Path              Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc          102.9G   104.9G   209.7G        N/A     196K     500K   1,000K        N/A    
    /home/vault        984.8G  1048.6G  2097.2G        N/A     180K     200K     400K        N/A    
    /home/woody        812.2G  1000.0G  1500.0G        N/A     250K   5,000K   7,500K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA A100-SXM4-40GB, 00000000:C1:00.0, 3070313, 24 %, 5 %, 9670 MiB, 10344340 ms
